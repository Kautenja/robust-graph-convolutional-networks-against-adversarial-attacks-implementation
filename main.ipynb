{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the path of execution\n",
    "EXE_PATH = os.path.abspath(os.path.curdir)\n",
    "# the path of the vendor files\n",
    "VENDOR_PATH = os.path.join(EXE_PATH, 'vendor')\n",
    "# the vendors to include in the system path\n",
    "VENDORS = ['keras-gcn']\n",
    "# create the absolute paths for all vendors\n",
    "VENDORS = list(map(lambda x: os.path.join(VENDOR_PATH, x), VENDORS))\n",
    "# update the Python path to include necessary vendor module\n",
    "sys.path += VENDORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kegra.layers.graph import GraphConvolution\n",
    "from kegra.utils import load_data, preprocess_adj, get_splits, evaluate_preds\n",
    "from ggcn import GaussianGraphConvolution, kl_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "Dataset has 2708 nodes, 5429 edges, 1433 features.\n"
     ]
    }
   ],
   "source": [
    "X, A, y = load_data('data/cora/', dataset='cora')\n",
    "X /= X.sum(1).reshape(-1, 1)\n",
    "A = preprocess_adj(A)\n",
    "y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask = get_splits(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2708x2708 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 13264 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1110 18:31:20.687325 140649269012288 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1110 18:31:20.695386 140649269012288 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:515: The name tf.sparse_placeholder is deprecated. Please use tf.compat.v1.sparse_placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "support = 1\n",
    "graph = [X, A]\n",
    "G = [Input(shape=(None, None), batch_shape=(None, None), sparse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 18:31:20.700241 140649269012288 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_in = Input(shape=(X.shape[1],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs=200, patience=10):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    wait = 0\n",
    "    preds = None\n",
    "    best_val_loss = 99999\n",
    "    # Fit\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        # Log wall-clock time\n",
    "        t = time.time()\n",
    "\n",
    "        # Single training iteration (we mask nodes without labels for loss calculation)\n",
    "        model.fit(graph, y_train, sample_weight=train_mask,\n",
    "                  batch_size=A.shape[0], epochs=1, shuffle=False, verbose=0)\n",
    "\n",
    "        # Predict on full dataset\n",
    "        preds = model.predict(graph, batch_size=A.shape[0])\n",
    "\n",
    "        # Train / validation scores\n",
    "        train_val_loss, train_val_acc = evaluate_preds(preds, [y_train, y_val],\n",
    "                                                       [idx_train, idx_val])\n",
    "        print(\"Epoch: {:04d}\".format(epoch),\n",
    "              \"train_loss= {:.4f}\".format(train_val_loss[0]),\n",
    "              \"train_acc= {:.4f}\".format(train_val_acc[0]),\n",
    "              \"val_loss= {:.4f}\".format(train_val_loss[1]),\n",
    "              \"val_acc= {:.4f}\".format(train_val_acc[1]),\n",
    "              \"time= {:.4f}\".format(time.time() - t))\n",
    "\n",
    "        # Early stopping\n",
    "        if train_val_loss[1] < best_val_loss:\n",
    "            best_val_loss = train_val_loss[1]\n",
    "            wait = 0\n",
    "        else:\n",
    "            if wait >= patience:\n",
    "                print('Epoch {}: early stopping'.format(epoch))\n",
    "                break\n",
    "            wait += 1\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 18:31:20.715382 140649269012288 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1110 18:31:20.719588 140649269012288 deprecation.py:506] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1110 18:31:20.727949 140649269012288 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define model architecture\n",
    "# NOTE: We pass arguments for graph convolutional layers as a list of tensors.\n",
    "# This is somewhat hacky, more elegant options would require rewriting the Layer base class.\n",
    "H = Dropout(0.5)(X_in)\n",
    "H = GraphConvolution(32, support, activation='relu', kernel_regularizer=l2(5e-4))([H]+G)\n",
    "H = Dropout(0.5)(H)\n",
    "Y = GraphConvolution(y.shape[1], support, activation='softmax')([H]+G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 18:31:20.769330 140649269012288 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[X_in]+G, outputs=Y)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 1433)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1433)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_1 (GraphConvo (None, 32)           45888       dropout_1[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32)           0           graph_convolution_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_2 (GraphConvo (None, 7)            231         dropout_2[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 46,119\n",
      "Trainable params: 46,119\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 18:31:20.846734 140649269012288 deprecation.py:323] From /home/bitcommander/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.9315 train_acc= 0.3000 val_loss= 1.9319 val_acc= 0.3500 time= 1.1493\n",
      "Epoch: 0002 train_loss= 1.9150 train_acc= 0.3000 val_loss= 1.9176 val_acc= 0.3500 time= 0.0173\n",
      "Epoch: 0003 train_loss= 1.8965 train_acc= 0.3429 val_loss= 1.9023 val_acc= 0.3633 time= 0.0168\n",
      "Epoch: 0004 train_loss= 1.8769 train_acc= 0.3786 val_loss= 1.8860 val_acc= 0.3633 time= 0.0164\n",
      "Epoch: 0005 train_loss= 1.8566 train_acc= 0.3857 val_loss= 1.8691 val_acc= 0.3633 time= 0.0165\n",
      "Epoch: 0006 train_loss= 1.8355 train_acc= 0.3857 val_loss= 1.8518 val_acc= 0.3667 time= 0.0164\n",
      "Epoch: 0007 train_loss= 1.8147 train_acc= 0.3929 val_loss= 1.8351 val_acc= 0.3667 time= 0.0175\n",
      "Epoch: 0008 train_loss= 1.7948 train_acc= 0.4071 val_loss= 1.8193 val_acc= 0.3667 time= 0.0177\n",
      "Epoch: 0009 train_loss= 1.7758 train_acc= 0.4071 val_loss= 1.8046 val_acc= 0.3667 time= 0.0170\n",
      "Epoch: 0010 train_loss= 1.7575 train_acc= 0.4000 val_loss= 1.7905 val_acc= 0.3667 time= 0.0172\n",
      "Epoch: 0011 train_loss= 1.7405 train_acc= 0.4071 val_loss= 1.7780 val_acc= 0.3667 time= 0.0172\n",
      "Epoch: 0012 train_loss= 1.7252 train_acc= 0.4143 val_loss= 1.7671 val_acc= 0.3667 time= 0.0174\n",
      "Epoch: 0013 train_loss= 1.7111 train_acc= 0.4143 val_loss= 1.7575 val_acc= 0.3700 time= 0.0176\n",
      "Epoch: 0014 train_loss= 1.6976 train_acc= 0.4214 val_loss= 1.7489 val_acc= 0.3733 time= 0.0171\n",
      "Epoch: 0015 train_loss= 1.6844 train_acc= 0.4214 val_loss= 1.7407 val_acc= 0.3767 time= 0.0171\n",
      "Epoch: 0016 train_loss= 1.6711 train_acc= 0.4286 val_loss= 1.7324 val_acc= 0.3933 time= 0.0168\n",
      "Epoch: 0017 train_loss= 1.6573 train_acc= 0.4357 val_loss= 1.7237 val_acc= 0.4067 time= 0.0168\n",
      "Epoch: 0018 train_loss= 1.6427 train_acc= 0.4429 val_loss= 1.7144 val_acc= 0.4133 time= 0.0167\n",
      "Epoch: 0019 train_loss= 1.6277 train_acc= 0.4643 val_loss= 1.7050 val_acc= 0.4333 time= 0.0170\n",
      "Epoch: 0020 train_loss= 1.6123 train_acc= 0.4643 val_loss= 1.6950 val_acc= 0.4567 time= 0.0165\n",
      "Epoch: 0021 train_loss= 1.5969 train_acc= 0.4643 val_loss= 1.6852 val_acc= 0.4633 time= 0.0166\n",
      "Epoch: 0022 train_loss= 1.5816 train_acc= 0.4714 val_loss= 1.6753 val_acc= 0.4767 time= 0.0168\n",
      "Epoch: 0023 train_loss= 1.5667 train_acc= 0.4714 val_loss= 1.6654 val_acc= 0.4733 time= 0.0169\n",
      "Epoch: 0024 train_loss= 1.5518 train_acc= 0.4714 val_loss= 1.6555 val_acc= 0.4733 time= 0.0178\n",
      "Epoch: 0025 train_loss= 1.5363 train_acc= 0.4714 val_loss= 1.6449 val_acc= 0.4733 time= 0.0178\n",
      "Epoch: 0026 train_loss= 1.5204 train_acc= 0.4786 val_loss= 1.6337 val_acc= 0.4700 time= 0.0170\n",
      "Epoch: 0027 train_loss= 1.5043 train_acc= 0.4857 val_loss= 1.6222 val_acc= 0.4733 time= 0.0177\n",
      "Epoch: 0028 train_loss= 1.4880 train_acc= 0.4857 val_loss= 1.6103 val_acc= 0.4700 time= 0.0184\n",
      "Epoch: 0029 train_loss= 1.4717 train_acc= 0.5000 val_loss= 1.5984 val_acc= 0.4767 time= 0.0169\n",
      "Epoch: 0030 train_loss= 1.4554 train_acc= 0.5071 val_loss= 1.5867 val_acc= 0.4800 time= 0.0161\n",
      "Epoch: 0031 train_loss= 1.4392 train_acc= 0.5143 val_loss= 1.5753 val_acc= 0.4867 time= 0.0168\n",
      "Epoch: 0032 train_loss= 1.4229 train_acc= 0.5143 val_loss= 1.5640 val_acc= 0.4867 time= 0.0169\n",
      "Epoch: 0033 train_loss= 1.4066 train_acc= 0.5143 val_loss= 1.5529 val_acc= 0.4933 time= 0.0166\n",
      "Epoch: 0034 train_loss= 1.3904 train_acc= 0.5286 val_loss= 1.5416 val_acc= 0.4967 time= 0.0165\n",
      "Epoch: 0035 train_loss= 1.3741 train_acc= 0.5286 val_loss= 1.5305 val_acc= 0.5000 time= 0.0169\n",
      "Epoch: 0036 train_loss= 1.3573 train_acc= 0.5429 val_loss= 1.5190 val_acc= 0.4967 time= 0.0165\n",
      "Epoch: 0037 train_loss= 1.3404 train_acc= 0.5500 val_loss= 1.5072 val_acc= 0.5100 time= 0.0164\n",
      "Epoch: 0038 train_loss= 1.3233 train_acc= 0.5643 val_loss= 1.4948 val_acc= 0.5133 time= 0.0165\n",
      "Epoch: 0039 train_loss= 1.3062 train_acc= 0.6000 val_loss= 1.4823 val_acc= 0.5333 time= 0.0176\n",
      "Epoch: 0040 train_loss= 1.2892 train_acc= 0.6429 val_loss= 1.4700 val_acc= 0.5367 time= 0.0165\n",
      "Epoch: 0041 train_loss= 1.2721 train_acc= 0.6571 val_loss= 1.4575 val_acc= 0.5533 time= 0.0170\n",
      "Epoch: 0042 train_loss= 1.2550 train_acc= 0.6786 val_loss= 1.4445 val_acc= 0.5633 time= 0.0166\n",
      "Epoch: 0043 train_loss= 1.2378 train_acc= 0.7071 val_loss= 1.4316 val_acc= 0.5667 time= 0.0171\n",
      "Epoch: 0044 train_loss= 1.2209 train_acc= 0.7214 val_loss= 1.4189 val_acc= 0.5767 time= 0.0169\n",
      "Epoch: 0045 train_loss= 1.2041 train_acc= 0.7357 val_loss= 1.4062 val_acc= 0.5900 time= 0.0171\n",
      "Epoch: 0046 train_loss= 1.1874 train_acc= 0.7429 val_loss= 1.3934 val_acc= 0.6000 time= 0.0171\n",
      "Epoch: 0047 train_loss= 1.1708 train_acc= 0.7429 val_loss= 1.3810 val_acc= 0.6033 time= 0.0173\n",
      "Epoch: 0048 train_loss= 1.1542 train_acc= 0.7500 val_loss= 1.3684 val_acc= 0.6167 time= 0.0168\n",
      "Epoch: 0049 train_loss= 1.1377 train_acc= 0.7571 val_loss= 1.3562 val_acc= 0.6267 time= 0.0184\n",
      "Epoch: 0050 train_loss= 1.1214 train_acc= 0.7571 val_loss= 1.3443 val_acc= 0.6267 time= 0.0173\n",
      "Epoch: 0051 train_loss= 1.1050 train_acc= 0.7786 val_loss= 1.3325 val_acc= 0.6300 time= 0.0173\n",
      "Epoch: 0052 train_loss= 1.0888 train_acc= 0.8071 val_loss= 1.3208 val_acc= 0.6467 time= 0.0165\n",
      "Epoch: 0053 train_loss= 1.0731 train_acc= 0.8071 val_loss= 1.3093 val_acc= 0.6733 time= 0.0170\n",
      "Epoch: 0054 train_loss= 1.0578 train_acc= 0.8071 val_loss= 1.2974 val_acc= 0.6900 time= 0.0166\n",
      "Epoch: 0055 train_loss= 1.0425 train_acc= 0.8143 val_loss= 1.2852 val_acc= 0.6933 time= 0.0170\n",
      "Epoch: 0056 train_loss= 1.0273 train_acc= 0.8286 val_loss= 1.2728 val_acc= 0.7033 time= 0.0165\n",
      "Epoch: 0057 train_loss= 1.0124 train_acc= 0.8357 val_loss= 1.2602 val_acc= 0.7100 time= 0.0164\n",
      "Epoch: 0058 train_loss= 0.9979 train_acc= 0.8357 val_loss= 1.2477 val_acc= 0.7000 time= 0.0166\n",
      "Epoch: 0059 train_loss= 0.9837 train_acc= 0.8429 val_loss= 1.2355 val_acc= 0.7067 time= 0.0163\n",
      "Epoch: 0060 train_loss= 0.9695 train_acc= 0.8429 val_loss= 1.2239 val_acc= 0.7100 time= 0.0174\n",
      "Epoch: 0061 train_loss= 0.9556 train_acc= 0.8429 val_loss= 1.2125 val_acc= 0.7233 time= 0.0175\n",
      "Epoch: 0062 train_loss= 0.9417 train_acc= 0.8500 val_loss= 1.2019 val_acc= 0.7267 time= 0.0190\n",
      "Epoch: 0063 train_loss= 0.9282 train_acc= 0.8571 val_loss= 1.1913 val_acc= 0.7300 time= 0.0160\n",
      "Epoch: 0064 train_loss= 0.9146 train_acc= 0.8786 val_loss= 1.1816 val_acc= 0.7433 time= 0.0162\n",
      "Epoch: 0065 train_loss= 0.9014 train_acc= 0.8786 val_loss= 1.1722 val_acc= 0.7467 time= 0.0160\n",
      "Epoch: 0066 train_loss= 0.8885 train_acc= 0.8786 val_loss= 1.1628 val_acc= 0.7500 time= 0.0189\n",
      "Epoch: 0067 train_loss= 0.8760 train_acc= 0.8786 val_loss= 1.1533 val_acc= 0.7533 time= 0.0169\n",
      "Epoch: 0068 train_loss= 0.8638 train_acc= 0.8714 val_loss= 1.1435 val_acc= 0.7567 time= 0.0165\n",
      "Epoch: 0069 train_loss= 0.8520 train_acc= 0.8714 val_loss= 1.1339 val_acc= 0.7633 time= 0.0167\n",
      "Epoch: 0070 train_loss= 0.8404 train_acc= 0.8714 val_loss= 1.1244 val_acc= 0.7700 time= 0.0169\n",
      "Epoch: 0071 train_loss= 0.8289 train_acc= 0.8786 val_loss= 1.1146 val_acc= 0.7667 time= 0.0166\n",
      "Epoch: 0072 train_loss= 0.8176 train_acc= 0.8786 val_loss= 1.1057 val_acc= 0.7667 time= 0.0168\n",
      "Epoch: 0073 train_loss= 0.8068 train_acc= 0.8786 val_loss= 1.0971 val_acc= 0.7667 time= 0.0167\n",
      "Epoch: 0074 train_loss= 0.7965 train_acc= 0.8786 val_loss= 1.0883 val_acc= 0.7667 time= 0.0172\n",
      "Epoch: 0075 train_loss= 0.7864 train_acc= 0.8786 val_loss= 1.0802 val_acc= 0.7667 time= 0.0180\n",
      "Epoch: 0076 train_loss= 0.7767 train_acc= 0.8857 val_loss= 1.0724 val_acc= 0.7667 time= 0.0170\n",
      "Epoch: 0077 train_loss= 0.7669 train_acc= 0.9000 val_loss= 1.0641 val_acc= 0.7700 time= 0.0166\n",
      "Epoch: 0078 train_loss= 0.7575 train_acc= 0.9000 val_loss= 1.0564 val_acc= 0.7700 time= 0.0168\n",
      "Epoch: 0079 train_loss= 0.7483 train_acc= 0.9071 val_loss= 1.0485 val_acc= 0.7767 time= 0.0166\n",
      "Epoch: 0080 train_loss= 0.7401 train_acc= 0.9071 val_loss= 1.0408 val_acc= 0.7733 time= 0.0168\n",
      "Epoch: 0081 train_loss= 0.7322 train_acc= 0.9143 val_loss= 1.0338 val_acc= 0.7700 time= 0.0166\n",
      "Epoch: 0082 train_loss= 0.7234 train_acc= 0.9143 val_loss= 1.0268 val_acc= 0.7700 time= 0.0170\n",
      "Epoch: 0083 train_loss= 0.7142 train_acc= 0.9143 val_loss= 1.0193 val_acc= 0.7767 time= 0.0167\n",
      "Epoch: 0084 train_loss= 0.7051 train_acc= 0.9286 val_loss= 1.0114 val_acc= 0.7833 time= 0.0169\n",
      "Epoch: 0085 train_loss= 0.6964 train_acc= 0.9357 val_loss= 1.0048 val_acc= 0.7867 time= 0.0171\n",
      "Epoch: 0086 train_loss= 0.6892 train_acc= 0.9357 val_loss= 0.9996 val_acc= 0.7867 time= 0.0202\n",
      "Epoch: 0087 train_loss= 0.6826 train_acc= 0.9429 val_loss= 0.9947 val_acc= 0.7833 time= 0.0168\n",
      "Epoch: 0088 train_loss= 0.6758 train_acc= 0.9429 val_loss= 0.9892 val_acc= 0.7900 time= 0.0166\n",
      "Epoch: 0089 train_loss= 0.6671 train_acc= 0.9429 val_loss= 0.9816 val_acc= 0.7867 time= 0.0165\n",
      "Epoch: 0090 train_loss= 0.6592 train_acc= 0.9357 val_loss= 0.9755 val_acc= 0.7933 time= 0.0161\n",
      "Epoch: 0091 train_loss= 0.6523 train_acc= 0.9286 val_loss= 0.9698 val_acc= 0.7900 time= 0.0214\n",
      "Epoch: 0092 train_loss= 0.6462 train_acc= 0.9214 val_loss= 0.9647 val_acc= 0.7867 time= 0.0166\n",
      "Epoch: 0093 train_loss= 0.6400 train_acc= 0.9214 val_loss= 0.9595 val_acc= 0.7933 time= 0.0167\n",
      "Epoch: 0094 train_loss= 0.6336 train_acc= 0.9214 val_loss= 0.9541 val_acc= 0.7933 time= 0.0165\n",
      "Epoch: 0095 train_loss= 0.6267 train_acc= 0.9286 val_loss= 0.9483 val_acc= 0.7900 time= 0.0160\n",
      "Epoch: 0096 train_loss= 0.6199 train_acc= 0.9286 val_loss= 0.9425 val_acc= 0.7900 time= 0.0165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0097 train_loss= 0.6137 train_acc= 0.9357 val_loss= 0.9365 val_acc= 0.7900 time= 0.0165\n",
      "Epoch: 0098 train_loss= 0.6074 train_acc= 0.9500 val_loss= 0.9322 val_acc= 0.7900 time= 0.0167\n",
      "Epoch: 0099 train_loss= 0.6016 train_acc= 0.9500 val_loss= 0.9278 val_acc= 0.7900 time= 0.0166\n",
      "Epoch: 0100 train_loss= 0.5960 train_acc= 0.9500 val_loss= 0.9233 val_acc= 0.7900 time= 0.0163\n",
      "Epoch: 0101 train_loss= 0.5901 train_acc= 0.9500 val_loss= 0.9184 val_acc= 0.7967 time= 0.0166\n",
      "Epoch: 0102 train_loss= 0.5845 train_acc= 0.9500 val_loss= 0.9132 val_acc= 0.8033 time= 0.0162\n",
      "Epoch: 0103 train_loss= 0.5796 train_acc= 0.9500 val_loss= 0.9084 val_acc= 0.8000 time= 0.0168\n",
      "Epoch: 0104 train_loss= 0.5753 train_acc= 0.9500 val_loss= 0.9040 val_acc= 0.7933 time= 0.0167\n",
      "Epoch: 0105 train_loss= 0.5706 train_acc= 0.9500 val_loss= 0.9005 val_acc= 0.7933 time= 0.0163\n",
      "Epoch: 0106 train_loss= 0.5659 train_acc= 0.9571 val_loss= 0.8973 val_acc= 0.7933 time= 0.0164\n",
      "Epoch: 0107 train_loss= 0.5608 train_acc= 0.9571 val_loss= 0.8942 val_acc= 0.8000 time= 0.0162\n",
      "Epoch: 0108 train_loss= 0.5556 train_acc= 0.9571 val_loss= 0.8920 val_acc= 0.8067 time= 0.0164\n",
      "Epoch: 0109 train_loss= 0.5511 train_acc= 0.9571 val_loss= 0.8905 val_acc= 0.8100 time= 0.0164\n",
      "Epoch: 0110 train_loss= 0.5470 train_acc= 0.9571 val_loss= 0.8889 val_acc= 0.8067 time= 0.0165\n",
      "Epoch: 0111 train_loss= 0.5428 train_acc= 0.9571 val_loss= 0.8863 val_acc= 0.8100 time= 0.0204\n",
      "Epoch: 0112 train_loss= 0.5376 train_acc= 0.9571 val_loss= 0.8823 val_acc= 0.8067 time= 0.0179\n",
      "Epoch: 0113 train_loss= 0.5326 train_acc= 0.9571 val_loss= 0.8775 val_acc= 0.8000 time= 0.0170\n",
      "Epoch: 0114 train_loss= 0.5281 train_acc= 0.9571 val_loss= 0.8732 val_acc= 0.8000 time= 0.0172\n",
      "Epoch: 0115 train_loss= 0.5246 train_acc= 0.9571 val_loss= 0.8707 val_acc= 0.8000 time= 0.0162\n",
      "Epoch: 0116 train_loss= 0.5220 train_acc= 0.9500 val_loss= 0.8694 val_acc= 0.7867 time= 0.0167\n",
      "Epoch: 0117 train_loss= 0.5187 train_acc= 0.9500 val_loss= 0.8682 val_acc= 0.7900 time= 0.0167\n",
      "Epoch: 0118 train_loss= 0.5145 train_acc= 0.9500 val_loss= 0.8677 val_acc= 0.7900 time= 0.0164\n",
      "Epoch: 0119 train_loss= 0.5103 train_acc= 0.9500 val_loss= 0.8663 val_acc= 0.7967 time= 0.0168\n",
      "Epoch: 0120 train_loss= 0.5067 train_acc= 0.9571 val_loss= 0.8647 val_acc= 0.7933 time= 0.0161\n",
      "Epoch: 0121 train_loss= 0.5032 train_acc= 0.9643 val_loss= 0.8624 val_acc= 0.8033 time= 0.0164\n",
      "Epoch: 0122 train_loss= 0.4996 train_acc= 0.9643 val_loss= 0.8585 val_acc= 0.8000 time= 0.0162\n",
      "Epoch: 0123 train_loss= 0.4958 train_acc= 0.9714 val_loss= 0.8526 val_acc= 0.8067 time= 0.0166\n",
      "Epoch: 0124 train_loss= 0.4926 train_acc= 0.9714 val_loss= 0.8481 val_acc= 0.8033 time= 0.0163\n",
      "Epoch: 0125 train_loss= 0.4904 train_acc= 0.9714 val_loss= 0.8442 val_acc= 0.7967 time= 0.0174\n",
      "Epoch: 0126 train_loss= 0.4876 train_acc= 0.9714 val_loss= 0.8410 val_acc= 0.8000 time= 0.0166\n",
      "Epoch: 0127 train_loss= 0.4834 train_acc= 0.9714 val_loss= 0.8378 val_acc= 0.8000 time= 0.0159\n",
      "Epoch: 0128 train_loss= 0.4792 train_acc= 0.9714 val_loss= 0.8357 val_acc= 0.8033 time= 0.0173\n",
      "Epoch: 0129 train_loss= 0.4755 train_acc= 0.9714 val_loss= 0.8345 val_acc= 0.8100 time= 0.0181\n",
      "Epoch: 0130 train_loss= 0.4722 train_acc= 0.9714 val_loss= 0.8336 val_acc= 0.8100 time= 0.0167\n",
      "Epoch: 0131 train_loss= 0.4693 train_acc= 0.9786 val_loss= 0.8326 val_acc= 0.8100 time= 0.0166\n",
      "Epoch: 0132 train_loss= 0.4666 train_acc= 0.9786 val_loss= 0.8315 val_acc= 0.8100 time= 0.0163\n",
      "Epoch: 0133 train_loss= 0.4630 train_acc= 0.9786 val_loss= 0.8293 val_acc= 0.8133 time= 0.0174\n",
      "Epoch: 0134 train_loss= 0.4595 train_acc= 0.9786 val_loss= 0.8274 val_acc= 0.8233 time= 0.0169\n",
      "Epoch: 0135 train_loss= 0.4557 train_acc= 0.9714 val_loss= 0.8222 val_acc= 0.8167 time= 0.0169\n",
      "Epoch: 0136 train_loss= 0.4525 train_acc= 0.9714 val_loss= 0.8175 val_acc= 0.8167 time= 0.0169\n",
      "Epoch: 0137 train_loss= 0.4504 train_acc= 0.9714 val_loss= 0.8135 val_acc= 0.8133 time= 0.0170\n",
      "Epoch: 0138 train_loss= 0.4479 train_acc= 0.9714 val_loss= 0.8109 val_acc= 0.8167 time= 0.0180\n",
      "Epoch: 0139 train_loss= 0.4449 train_acc= 0.9714 val_loss= 0.8090 val_acc= 0.8133 time= 0.0166\n",
      "Epoch: 0140 train_loss= 0.4424 train_acc= 0.9714 val_loss= 0.8091 val_acc= 0.8100 time= 0.0167\n",
      "Epoch: 0141 train_loss= 0.4412 train_acc= 0.9714 val_loss= 0.8107 val_acc= 0.8067 time= 0.0167\n",
      "Epoch: 0142 train_loss= 0.4396 train_acc= 0.9714 val_loss= 0.8110 val_acc= 0.8000 time= 0.0171\n",
      "Epoch: 0143 train_loss= 0.4359 train_acc= 0.9714 val_loss= 0.8091 val_acc= 0.8067 time= 0.0168\n",
      "Epoch: 0144 train_loss= 0.4316 train_acc= 0.9714 val_loss= 0.8042 val_acc= 0.8100 time= 0.0180\n",
      "Epoch: 0145 train_loss= 0.4284 train_acc= 0.9714 val_loss= 0.8005 val_acc= 0.8033 time= 0.0165\n",
      "Epoch: 0146 train_loss= 0.4260 train_acc= 0.9714 val_loss= 0.7977 val_acc= 0.7967 time= 0.0168\n",
      "Epoch: 0147 train_loss= 0.4240 train_acc= 0.9714 val_loss= 0.7961 val_acc= 0.7967 time= 0.0175\n",
      "Epoch: 0148 train_loss= 0.4215 train_acc= 0.9714 val_loss= 0.7946 val_acc= 0.7967 time= 0.0167\n",
      "Epoch: 0149 train_loss= 0.4186 train_acc= 0.9714 val_loss= 0.7931 val_acc= 0.8067 time= 0.0163\n",
      "Epoch: 0150 train_loss= 0.4152 train_acc= 0.9786 val_loss= 0.7920 val_acc= 0.8100 time= 0.0172\n",
      "Epoch: 0151 train_loss= 0.4119 train_acc= 0.9786 val_loss= 0.7913 val_acc= 0.8167 time= 0.0164\n",
      "Epoch: 0152 train_loss= 0.4087 train_acc= 0.9786 val_loss= 0.7906 val_acc= 0.8267 time= 0.0166\n",
      "Epoch: 0153 train_loss= 0.4056 train_acc= 0.9714 val_loss= 0.7881 val_acc= 0.8300 time= 0.0166\n",
      "Epoch: 0154 train_loss= 0.4031 train_acc= 0.9714 val_loss= 0.7851 val_acc= 0.8233 time= 0.0167\n",
      "Epoch: 0155 train_loss= 0.4014 train_acc= 0.9714 val_loss= 0.7839 val_acc= 0.8200 time= 0.0170\n",
      "Epoch: 0156 train_loss= 0.3995 train_acc= 0.9714 val_loss= 0.7827 val_acc= 0.8200 time= 0.0164\n",
      "Epoch: 0157 train_loss= 0.3972 train_acc= 0.9786 val_loss= 0.7818 val_acc= 0.8200 time= 0.0165\n",
      "Epoch: 0158 train_loss= 0.3949 train_acc= 0.9786 val_loss= 0.7822 val_acc= 0.8233 time= 0.0166\n",
      "Epoch: 0159 train_loss= 0.3931 train_acc= 0.9786 val_loss= 0.7820 val_acc= 0.8233 time= 0.0166\n",
      "Epoch: 0160 train_loss= 0.3922 train_acc= 0.9786 val_loss= 0.7831 val_acc= 0.8267 time= 0.0168\n",
      "Epoch: 0161 train_loss= 0.3917 train_acc= 0.9786 val_loss= 0.7842 val_acc= 0.8167 time= 0.0180\n",
      "Epoch: 0162 train_loss= 0.3903 train_acc= 0.9857 val_loss= 0.7841 val_acc= 0.8200 time= 0.0178\n",
      "Epoch: 0163 train_loss= 0.3881 train_acc= 0.9786 val_loss= 0.7807 val_acc= 0.8233 time= 0.0172\n",
      "Epoch: 0164 train_loss= 0.3861 train_acc= 0.9786 val_loss= 0.7755 val_acc= 0.8200 time= 0.0201\n",
      "Epoch: 0165 train_loss= 0.3844 train_acc= 0.9786 val_loss= 0.7720 val_acc= 0.8233 time= 0.0167\n",
      "Epoch: 0166 train_loss= 0.3822 train_acc= 0.9786 val_loss= 0.7694 val_acc= 0.8200 time= 0.0165\n",
      "Epoch: 0167 train_loss= 0.3799 train_acc= 0.9786 val_loss= 0.7668 val_acc= 0.8233 time= 0.0168\n",
      "Epoch: 0168 train_loss= 0.3773 train_acc= 0.9786 val_loss= 0.7641 val_acc= 0.8233 time= 0.0168\n",
      "Epoch: 0169 train_loss= 0.3755 train_acc= 0.9786 val_loss= 0.7613 val_acc= 0.8200 time= 0.0170\n",
      "Epoch: 0170 train_loss= 0.3741 train_acc= 0.9786 val_loss= 0.7593 val_acc= 0.8200 time= 0.0171\n",
      "Epoch: 0171 train_loss= 0.3728 train_acc= 0.9786 val_loss= 0.7577 val_acc= 0.8200 time= 0.0171\n",
      "Epoch: 0172 train_loss= 0.3713 train_acc= 0.9786 val_loss= 0.7567 val_acc= 0.8133 time= 0.0176\n",
      "Epoch: 0173 train_loss= 0.3697 train_acc= 0.9786 val_loss= 0.7562 val_acc= 0.8167 time= 0.0171\n",
      "Epoch: 0174 train_loss= 0.3679 train_acc= 0.9786 val_loss= 0.7552 val_acc= 0.8200 time= 0.0171\n",
      "Epoch: 0175 train_loss= 0.3662 train_acc= 0.9786 val_loss= 0.7542 val_acc= 0.8200 time= 0.0165\n",
      "Epoch: 0176 train_loss= 0.3643 train_acc= 0.9786 val_loss= 0.7530 val_acc= 0.8233 time= 0.0165\n",
      "Epoch: 0177 train_loss= 0.3617 train_acc= 0.9786 val_loss= 0.7524 val_acc= 0.8200 time= 0.0166\n",
      "Epoch: 0178 train_loss= 0.3601 train_acc= 0.9857 val_loss= 0.7531 val_acc= 0.8200 time= 0.0167\n",
      "Epoch: 0179 train_loss= 0.3589 train_acc= 0.9857 val_loss= 0.7556 val_acc= 0.8200 time= 0.0168\n",
      "Epoch: 0180 train_loss= 0.3581 train_acc= 0.9857 val_loss= 0.7576 val_acc= 0.8167 time= 0.0163\n",
      "Epoch: 0181 train_loss= 0.3568 train_acc= 0.9857 val_loss= 0.7578 val_acc= 0.8200 time= 0.0163\n",
      "Epoch: 0182 train_loss= 0.3541 train_acc= 0.9857 val_loss= 0.7552 val_acc= 0.8200 time= 0.0163\n",
      "Epoch: 0183 train_loss= 0.3512 train_acc= 0.9857 val_loss= 0.7510 val_acc= 0.8300 time= 0.0174\n",
      "Epoch: 0184 train_loss= 0.3498 train_acc= 0.9786 val_loss= 0.7475 val_acc= 0.8233 time= 0.0173\n",
      "Epoch: 0185 train_loss= 0.3492 train_acc= 0.9786 val_loss= 0.7456 val_acc= 0.8133 time= 0.0162\n",
      "Epoch: 0186 train_loss= 0.3481 train_acc= 0.9786 val_loss= 0.7441 val_acc= 0.8167 time= 0.0169\n",
      "Epoch: 0187 train_loss= 0.3459 train_acc= 0.9786 val_loss= 0.7426 val_acc= 0.8200 time= 0.0164\n",
      "Epoch: 0188 train_loss= 0.3431 train_acc= 0.9786 val_loss= 0.7417 val_acc= 0.8200 time= 0.0167\n",
      "Epoch: 0189 train_loss= 0.3415 train_acc= 0.9857 val_loss= 0.7430 val_acc= 0.8167 time= 0.0168\n",
      "Epoch: 0190 train_loss= 0.3412 train_acc= 0.9857 val_loss= 0.7445 val_acc= 0.8167 time= 0.0171\n",
      "Epoch: 0191 train_loss= 0.3402 train_acc= 0.9857 val_loss= 0.7438 val_acc= 0.8167 time= 0.0164\n",
      "Epoch: 0192 train_loss= 0.3387 train_acc= 0.9857 val_loss= 0.7414 val_acc= 0.8167 time= 0.0187\n",
      "Epoch: 0193 train_loss= 0.3371 train_acc= 0.9857 val_loss= 0.7375 val_acc= 0.8233 time= 0.0180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0194 train_loss= 0.3355 train_acc= 0.9857 val_loss= 0.7340 val_acc= 0.8267 time= 0.0179\n",
      "Epoch: 0195 train_loss= 0.3342 train_acc= 0.9714 val_loss= 0.7309 val_acc= 0.8333 time= 0.0169\n",
      "Epoch: 0196 train_loss= 0.3342 train_acc= 0.9786 val_loss= 0.7295 val_acc= 0.8233 time= 0.0175\n",
      "Epoch: 0197 train_loss= 0.3339 train_acc= 0.9786 val_loss= 0.7301 val_acc= 0.8233 time= 0.0171\n",
      "Epoch: 0198 train_loss= 0.3312 train_acc= 0.9786 val_loss= 0.7306 val_acc= 0.8233 time= 0.0176\n",
      "Epoch: 0199 train_loss= 0.3285 train_acc= 0.9786 val_loss= 0.7326 val_acc= 0.8267 time= 0.0167\n",
      "Epoch: 0200 train_loss= 0.3292 train_acc= 0.9857 val_loss= 0.7403 val_acc= 0.8200 time= 0.0166\n"
     ]
    }
   ],
   "source": [
    "preds = train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.7685\n",
      "accuracy = 0.8220\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate_preds(preds, [y_test], [idx_test])\n",
    "print(f\"\"\"\n",
    "loss = {test_loss[0]:.4f}\n",
    "accuracy = {test_acc[0]:.4f}\n",
    "\"\"\".strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Graph Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "B1 = 5e-4\n",
    "B2 = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 18:31:25.422359 140649269012288 deprecation.py:323] From /home/bitcommander/Desktop/robust-graph-convolutional-networks-against-adversarial-attacks-implementation/ggcn/ggcl.py:174: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "W1110 18:31:25.423832 140649269012288 deprecation.py:323] From /home/bitcommander/.local/lib/python3.7/site-packages/tensorflow/python/ops/distributions/normal.py:160: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    }
   ],
   "source": [
    "H = Dropout(0.5)(X_in)\n",
    "H1 = GaussianGraphConvolution(16,\n",
    "    is_first=True,\n",
    "    activation='relu',\n",
    "    mean_regularizer=l2(B1),\n",
    "    variance_regularizer=l2(B1),\n",
    "#     dropout=0.5\n",
    ")([H]+G)\n",
    "Y = GaussianGraphConvolution(y.shape[1],\n",
    "    is_last=True,\n",
    "    activation='softmax',\n",
    ")(H1+G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 18:31:25.458773 140649269012288 deprecation.py:323] From /home/bitcommander/Desktop/robust-graph-convolutional-networks-against-adversarial-attacks-implementation/ggcn/losses.py:28: kl_divergence (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[X_in]+G, outputs=Y)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01))\n",
    "model.add_loss(kl_reg(*H1, B2), H1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 1433)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1433)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_graph_convolution_1 (G [(None, 16), (None,  45856       dropout_3[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_graph_convolution_2 (G (None, 7)            224         gaussian_graph_convolution_1[0][0\n",
      "                                                                 gaussian_graph_convolution_1[0][1\n",
      "                                                                 input_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 46,080\n",
      "Trainable params: 46,080\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.9409 train_acc= 0.4357 val_loss= 1.9434 val_acc= 0.3167 time= 0.4739\n",
      "Epoch: 0002 train_loss= 1.9366 train_acc= 0.4929 val_loss= 1.9409 val_acc= 0.4033 time= 0.0170\n",
      "Epoch: 0003 train_loss= 1.9303 train_acc= 0.6071 val_loss= 1.9358 val_acc= 0.4900 time= 0.0163\n",
      "Epoch: 0004 train_loss= 1.9223 train_acc= 0.6571 val_loss= 1.9319 val_acc= 0.4967 time= 0.0163\n",
      "Epoch: 0005 train_loss= 1.9165 train_acc= 0.6143 val_loss= 1.9257 val_acc= 0.5167 time= 0.0163\n",
      "Epoch: 0006 train_loss= 1.9082 train_acc= 0.5714 val_loss= 1.9200 val_acc= 0.5200 time= 0.0164\n",
      "Epoch: 0007 train_loss= 1.8970 train_acc= 0.6357 val_loss= 1.9139 val_acc= 0.5233 time= 0.0165\n",
      "Epoch: 0008 train_loss= 1.8870 train_acc= 0.6357 val_loss= 1.9059 val_acc= 0.5333 time= 0.0165\n",
      "Epoch: 0009 train_loss= 1.8774 train_acc= 0.6143 val_loss= 1.9018 val_acc= 0.5100 time= 0.0162\n",
      "Epoch: 0010 train_loss= 1.8666 train_acc= 0.6429 val_loss= 1.8931 val_acc= 0.5500 time= 0.0163\n",
      "Epoch: 0011 train_loss= 1.8556 train_acc= 0.6500 val_loss= 1.8847 val_acc= 0.5467 time= 0.0165\n",
      "Epoch: 0012 train_loss= 1.8465 train_acc= 0.6500 val_loss= 1.8788 val_acc= 0.5267 time= 0.0169\n",
      "Epoch: 0013 train_loss= 1.8319 train_acc= 0.6571 val_loss= 1.8696 val_acc= 0.5467 time= 0.0175\n",
      "Epoch: 0014 train_loss= 1.8204 train_acc= 0.6571 val_loss= 1.8584 val_acc= 0.5633 time= 0.0182\n",
      "Epoch: 0015 train_loss= 1.8104 train_acc= 0.6500 val_loss= 1.8502 val_acc= 0.5367 time= 0.0184\n",
      "Epoch: 0016 train_loss= 1.7978 train_acc= 0.6571 val_loss= 1.8407 val_acc= 0.5633 time= 0.0198\n",
      "Epoch: 0017 train_loss= 1.7825 train_acc= 0.6714 val_loss= 1.8296 val_acc= 0.5633 time= 0.0181\n",
      "Epoch: 0018 train_loss= 1.7675 train_acc= 0.6857 val_loss= 1.8223 val_acc= 0.5633 time= 0.0176\n",
      "Epoch: 0019 train_loss= 1.7540 train_acc= 0.6786 val_loss= 1.8094 val_acc= 0.5533 time= 0.0180\n",
      "Epoch: 0020 train_loss= 1.7385 train_acc= 0.6643 val_loss= 1.7989 val_acc= 0.5633 time= 0.0212\n",
      "Epoch: 0021 train_loss= 1.7241 train_acc= 0.6714 val_loss= 1.7890 val_acc= 0.5500 time= 0.0179\n",
      "Epoch: 0022 train_loss= 1.7064 train_acc= 0.6714 val_loss= 1.7760 val_acc= 0.5500 time= 0.0176\n",
      "Epoch: 0023 train_loss= 1.6905 train_acc= 0.6786 val_loss= 1.7647 val_acc= 0.5500 time= 0.0177\n",
      "Epoch: 0024 train_loss= 1.6744 train_acc= 0.6786 val_loss= 1.7525 val_acc= 0.5467 time= 0.0177\n",
      "Epoch: 0025 train_loss= 1.6580 train_acc= 0.6786 val_loss= 1.7408 val_acc= 0.5400 time= 0.0188\n",
      "Epoch: 0026 train_loss= 1.6416 train_acc= 0.6786 val_loss= 1.7284 val_acc= 0.5400 time= 0.0193\n",
      "Epoch: 0027 train_loss= 1.6248 train_acc= 0.6857 val_loss= 1.7158 val_acc= 0.5400 time= 0.0175\n",
      "Epoch: 0028 train_loss= 1.6074 train_acc= 0.6786 val_loss= 1.7033 val_acc= 0.5333 time= 0.0169\n",
      "Epoch: 0029 train_loss= 1.5907 train_acc= 0.6857 val_loss= 1.6903 val_acc= 0.5367 time= 0.0171\n",
      "Epoch: 0030 train_loss= 1.5731 train_acc= 0.6786 val_loss= 1.6775 val_acc= 0.5367 time= 0.0170\n",
      "Epoch: 0031 train_loss= 1.5559 train_acc= 0.6786 val_loss= 1.6647 val_acc= 0.5267 time= 0.0169\n",
      "Epoch: 0032 train_loss= 1.5379 train_acc= 0.6786 val_loss= 1.6514 val_acc= 0.5233 time= 0.0169\n",
      "Epoch: 0033 train_loss= 1.5202 train_acc= 0.6786 val_loss= 1.6389 val_acc= 0.5267 time= 0.0171\n",
      "Epoch: 0034 train_loss= 1.5025 train_acc= 0.6786 val_loss= 1.6255 val_acc= 0.5300 time= 0.0170\n",
      "Epoch: 0035 train_loss= 1.4852 train_acc= 0.6786 val_loss= 1.6117 val_acc= 0.5333 time= 0.0171\n",
      "Epoch: 0036 train_loss= 1.4677 train_acc= 0.6786 val_loss= 1.5989 val_acc= 0.5400 time= 0.0178\n",
      "Epoch: 0037 train_loss= 1.4496 train_acc= 0.6786 val_loss= 1.5854 val_acc= 0.5533 time= 0.0172\n",
      "Epoch: 0038 train_loss= 1.4324 train_acc= 0.6857 val_loss= 1.5726 val_acc= 0.5700 time= 0.0190\n",
      "Epoch: 0039 train_loss= 1.4147 train_acc= 0.6929 val_loss= 1.5595 val_acc= 0.5733 time= 0.0176\n",
      "Epoch: 0040 train_loss= 1.3977 train_acc= 0.6929 val_loss= 1.5464 val_acc= 0.5800 time= 0.0171\n",
      "Epoch: 0041 train_loss= 1.3796 train_acc= 0.6929 val_loss= 1.5336 val_acc= 0.5867 time= 0.0170\n",
      "Epoch: 0042 train_loss= 1.3637 train_acc= 0.7071 val_loss= 1.5208 val_acc= 0.5933 time= 0.0170\n",
      "Epoch: 0043 train_loss= 1.3466 train_acc= 0.7071 val_loss= 1.5080 val_acc= 0.5967 time= 0.0171\n",
      "Epoch: 0044 train_loss= 1.3302 train_acc= 0.7143 val_loss= 1.4956 val_acc= 0.6100 time= 0.0176\n",
      "Epoch: 0045 train_loss= 1.3135 train_acc= 0.7286 val_loss= 1.4831 val_acc= 0.6200 time= 0.0180\n",
      "Epoch: 0046 train_loss= 1.2975 train_acc= 0.7357 val_loss= 1.4705 val_acc= 0.6400 time= 0.0170\n",
      "Epoch: 0047 train_loss= 1.2817 train_acc= 0.7357 val_loss= 1.4577 val_acc= 0.6433 time= 0.0179\n",
      "Epoch: 0048 train_loss= 1.2661 train_acc= 0.7571 val_loss= 1.4459 val_acc= 0.6467 time= 0.0180\n",
      "Epoch: 0049 train_loss= 1.2500 train_acc= 0.7571 val_loss= 1.4334 val_acc= 0.6533 time= 0.0243\n",
      "Epoch: 0050 train_loss= 1.2348 train_acc= 0.7643 val_loss= 1.4215 val_acc= 0.6667 time= 0.0178\n",
      "Epoch: 0051 train_loss= 1.2202 train_acc= 0.7786 val_loss= 1.4096 val_acc= 0.6700 time= 0.0177\n",
      "Epoch: 0052 train_loss= 1.2048 train_acc= 0.7857 val_loss= 1.3982 val_acc= 0.6700 time= 0.0169\n",
      "Epoch: 0053 train_loss= 1.1908 train_acc= 0.8000 val_loss= 1.3864 val_acc= 0.6867 time= 0.0170\n",
      "Epoch: 0054 train_loss= 1.1765 train_acc= 0.8143 val_loss= 1.3751 val_acc= 0.6933 time= 0.0164\n",
      "Epoch: 0055 train_loss= 1.1624 train_acc= 0.8214 val_loss= 1.3639 val_acc= 0.6933 time= 0.0168\n",
      "Epoch: 0056 train_loss= 1.1487 train_acc= 0.8429 val_loss= 1.3532 val_acc= 0.6933 time= 0.0167\n",
      "Epoch: 0057 train_loss= 1.1349 train_acc= 0.8500 val_loss= 1.3423 val_acc= 0.7133 time= 0.0167\n",
      "Epoch: 0058 train_loss= 1.1219 train_acc= 0.8571 val_loss= 1.3323 val_acc= 0.7133 time= 0.0168\n",
      "Epoch: 0059 train_loss= 1.1093 train_acc= 0.8643 val_loss= 1.3226 val_acc= 0.7300 time= 0.0166\n",
      "Epoch: 0060 train_loss= 1.0961 train_acc= 0.8786 val_loss= 1.3123 val_acc= 0.7333 time= 0.0165\n",
      "Epoch: 0061 train_loss= 1.0835 train_acc= 0.8786 val_loss= 1.3027 val_acc= 0.7400 time= 0.0166\n",
      "Epoch: 0062 train_loss= 1.0711 train_acc= 0.8929 val_loss= 1.2930 val_acc= 0.7433 time= 0.0192\n",
      "Epoch: 0063 train_loss= 1.0592 train_acc= 0.8929 val_loss= 1.2835 val_acc= 0.7467 time= 0.0166\n",
      "Epoch: 0064 train_loss= 1.0473 train_acc= 0.8929 val_loss= 1.2744 val_acc= 0.7467 time= 0.0167\n",
      "Epoch: 0065 train_loss= 1.0355 train_acc= 0.8929 val_loss= 1.2653 val_acc= 0.7567 time= 0.0166\n",
      "Epoch: 0066 train_loss= 1.0232 train_acc= 0.8929 val_loss= 1.2562 val_acc= 0.7533 time= 0.0167\n",
      "Epoch: 0067 train_loss= 1.0132 train_acc= 0.8929 val_loss= 1.2476 val_acc= 0.7633 time= 0.0164\n",
      "Epoch: 0068 train_loss= 1.0011 train_acc= 0.8929 val_loss= 1.2388 val_acc= 0.7667 time= 0.0167\n",
      "Epoch: 0069 train_loss= 0.9900 train_acc= 0.8929 val_loss= 1.2303 val_acc= 0.7600 time= 0.0164\n",
      "Epoch: 0070 train_loss= 0.9800 train_acc= 0.8929 val_loss= 1.2213 val_acc= 0.7633 time= 0.0165\n",
      "Epoch: 0071 train_loss= 0.9691 train_acc= 0.8929 val_loss= 1.2127 val_acc= 0.7700 time= 0.0165\n",
      "Epoch: 0072 train_loss= 0.9582 train_acc= 0.8929 val_loss= 1.2045 val_acc= 0.7700 time= 0.0164\n",
      "Epoch: 0073 train_loss= 0.9485 train_acc= 0.8929 val_loss= 1.1961 val_acc= 0.7700 time= 0.0167\n",
      "Epoch: 0074 train_loss= 0.9383 train_acc= 0.8929 val_loss= 1.1880 val_acc= 0.7800 time= 0.0206\n",
      "Epoch: 0075 train_loss= 0.9289 train_acc= 0.8929 val_loss= 1.1802 val_acc= 0.7833 time= 0.0178\n",
      "Epoch: 0076 train_loss= 0.9190 train_acc= 0.8929 val_loss= 1.1728 val_acc= 0.7767 time= 0.0166\n",
      "Epoch: 0077 train_loss= 0.9088 train_acc= 0.8929 val_loss= 1.1655 val_acc= 0.7733 time= 0.0165\n",
      "Epoch: 0078 train_loss= 0.8997 train_acc= 0.8929 val_loss= 1.1568 val_acc= 0.7767 time= 0.0166\n",
      "Epoch: 0079 train_loss= 0.8904 train_acc= 0.8929 val_loss= 1.1502 val_acc= 0.7767 time= 0.0166\n",
      "Epoch: 0080 train_loss= 0.8814 train_acc= 0.9000 val_loss= 1.1428 val_acc= 0.7833 time= 0.0164\n",
      "Epoch: 0081 train_loss= 0.8719 train_acc= 0.9000 val_loss= 1.1352 val_acc= 0.7900 time= 0.0165\n",
      "Epoch: 0082 train_loss= 0.8635 train_acc= 0.9071 val_loss= 1.1286 val_acc= 0.7900 time= 0.0165\n",
      "Epoch: 0083 train_loss= 0.8548 train_acc= 0.9071 val_loss= 1.1219 val_acc= 0.7933 time= 0.0165\n",
      "Epoch: 0084 train_loss= 0.8462 train_acc= 0.9071 val_loss= 1.1149 val_acc= 0.7933 time= 0.0165\n",
      "Epoch: 0085 train_loss= 0.8379 train_acc= 0.9071 val_loss= 1.1080 val_acc= 0.7967 time= 0.0167\n",
      "Epoch: 0086 train_loss= 0.8294 train_acc= 0.9071 val_loss= 1.1020 val_acc= 0.8000 time= 0.0173\n",
      "Epoch: 0087 train_loss= 0.8210 train_acc= 0.9071 val_loss= 1.0955 val_acc= 0.8033 time= 0.0168\n",
      "Epoch: 0088 train_loss= 0.8133 train_acc= 0.9071 val_loss= 1.0885 val_acc= 0.8000 time= 0.0164\n",
      "Epoch: 0089 train_loss= 0.8057 train_acc= 0.9071 val_loss= 1.0826 val_acc= 0.8033 time= 0.0163\n",
      "Epoch: 0090 train_loss= 0.7979 train_acc= 0.9071 val_loss= 1.0763 val_acc= 0.8000 time= 0.0164\n",
      "Epoch: 0091 train_loss= 0.7905 train_acc= 0.9071 val_loss= 1.0701 val_acc= 0.8033 time= 0.0164\n",
      "Epoch: 0092 train_loss= 0.7835 train_acc= 0.9071 val_loss= 1.0641 val_acc= 0.8033 time= 0.0162\n",
      "Epoch: 0093 train_loss= 0.7768 train_acc= 0.9071 val_loss= 1.0578 val_acc= 0.8033 time= 0.0165\n",
      "Epoch: 0094 train_loss= 0.7693 train_acc= 0.9071 val_loss= 1.0521 val_acc= 0.8000 time= 0.0164\n",
      "Epoch: 0095 train_loss= 0.7624 train_acc= 0.9071 val_loss= 1.0455 val_acc= 0.8000 time= 0.0164\n",
      "Epoch: 0096 train_loss= 0.7564 train_acc= 0.9071 val_loss= 1.0404 val_acc= 0.8000 time= 0.0165\n",
      "Epoch: 0097 train_loss= 0.7500 train_acc= 0.9071 val_loss= 1.0345 val_acc= 0.8000 time= 0.0165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0098 train_loss= 0.7427 train_acc= 0.9071 val_loss= 1.0282 val_acc= 0.8000 time= 0.0187\n",
      "Epoch: 0099 train_loss= 0.7367 train_acc= 0.9071 val_loss= 1.0224 val_acc= 0.8000 time= 0.0180\n",
      "Epoch: 0100 train_loss= 0.7299 train_acc= 0.9071 val_loss= 1.0172 val_acc= 0.8000 time= 0.0164\n",
      "Epoch: 0101 train_loss= 0.7236 train_acc= 0.9071 val_loss= 1.0116 val_acc= 0.8033 time= 0.0166\n",
      "Epoch: 0102 train_loss= 0.7178 train_acc= 0.9071 val_loss= 1.0073 val_acc= 0.8000 time= 0.0163\n",
      "Epoch: 0103 train_loss= 0.7119 train_acc= 0.9071 val_loss= 1.0028 val_acc= 0.8033 time= 0.0164\n",
      "Epoch: 0104 train_loss= 0.7063 train_acc= 0.9071 val_loss= 0.9980 val_acc= 0.8000 time= 0.0164\n",
      "Epoch: 0105 train_loss= 0.6999 train_acc= 0.9071 val_loss= 0.9936 val_acc= 0.8033 time= 0.0166\n",
      "Epoch: 0106 train_loss= 0.6954 train_acc= 0.9143 val_loss= 0.9893 val_acc= 0.8033 time= 0.0165\n",
      "Epoch: 0107 train_loss= 0.6882 train_acc= 0.9214 val_loss= 0.9854 val_acc= 0.8033 time= 0.0172\n",
      "Epoch: 0108 train_loss= 0.6829 train_acc= 0.9143 val_loss= 0.9811 val_acc= 0.8067 time= 0.0165\n",
      "Epoch: 0109 train_loss= 0.6778 train_acc= 0.9143 val_loss= 0.9780 val_acc= 0.8033 time= 0.0167\n",
      "Epoch: 0110 train_loss= 0.6737 train_acc= 0.9143 val_loss= 0.9733 val_acc= 0.8033 time= 0.0165\n",
      "Epoch: 0111 train_loss= 0.6693 train_acc= 0.9143 val_loss= 0.9694 val_acc= 0.8033 time= 0.0212\n",
      "Epoch: 0112 train_loss= 0.6624 train_acc= 0.9143 val_loss= 0.9653 val_acc= 0.8033 time= 0.0184\n",
      "Epoch: 0113 train_loss= 0.6582 train_acc= 0.9214 val_loss= 0.9620 val_acc= 0.8033 time= 0.0169\n",
      "Epoch: 0114 train_loss= 0.6541 train_acc= 0.9143 val_loss= 0.9578 val_acc= 0.8033 time= 0.0165\n",
      "Epoch: 0115 train_loss= 0.6491 train_acc= 0.9214 val_loss= 0.9546 val_acc= 0.8033 time= 0.0162\n",
      "Epoch: 0116 train_loss= 0.6442 train_acc= 0.9214 val_loss= 0.9501 val_acc= 0.8067 time= 0.0164\n",
      "Epoch: 0117 train_loss= 0.6392 train_acc= 0.9286 val_loss= 0.9448 val_acc= 0.8067 time= 0.0165\n",
      "Epoch: 0118 train_loss= 0.6358 train_acc= 0.9214 val_loss= 0.9422 val_acc= 0.8067 time= 0.0165\n",
      "Epoch: 0119 train_loss= 0.6314 train_acc= 0.9214 val_loss= 0.9391 val_acc= 0.8067 time= 0.0167\n",
      "Epoch: 0120 train_loss= 0.6250 train_acc= 0.9357 val_loss= 0.9357 val_acc= 0.8067 time= 0.0165\n",
      "Epoch: 0121 train_loss= 0.6203 train_acc= 0.9357 val_loss= 0.9326 val_acc= 0.8033 time= 0.0165\n",
      "Epoch: 0122 train_loss= 0.6170 train_acc= 0.9357 val_loss= 0.9300 val_acc= 0.8033 time= 0.0166\n",
      "Epoch: 0123 train_loss= 0.6118 train_acc= 0.9429 val_loss= 0.9270 val_acc= 0.8033 time= 0.0165\n",
      "Epoch: 0124 train_loss= 0.6090 train_acc= 0.9357 val_loss= 0.9243 val_acc= 0.8033 time= 0.0165\n",
      "Epoch: 0125 train_loss= 0.6034 train_acc= 0.9429 val_loss= 0.9217 val_acc= 0.8033 time= 0.0167\n",
      "Epoch: 0126 train_loss= 0.6000 train_acc= 0.9429 val_loss= 0.9182 val_acc= 0.8033 time= 0.0165\n",
      "Epoch: 0127 train_loss= 0.5953 train_acc= 0.9429 val_loss= 0.9149 val_acc= 0.8067 time= 0.0163\n",
      "Epoch: 0128 train_loss= 0.5919 train_acc= 0.9429 val_loss= 0.9106 val_acc= 0.8100 time= 0.0164\n",
      "Epoch: 0129 train_loss= 0.5876 train_acc= 0.9429 val_loss= 0.9075 val_acc= 0.8100 time= 0.0166\n",
      "Epoch: 0130 train_loss= 0.5839 train_acc= 0.9429 val_loss= 0.9040 val_acc= 0.8100 time= 0.0164\n",
      "Epoch: 0131 train_loss= 0.5797 train_acc= 0.9429 val_loss= 0.9008 val_acc= 0.8100 time= 0.0167\n",
      "Epoch: 0132 train_loss= 0.5753 train_acc= 0.9429 val_loss= 0.8979 val_acc= 0.8100 time= 0.0169\n",
      "Epoch: 0133 train_loss= 0.5730 train_acc= 0.9357 val_loss= 0.8935 val_acc= 0.8100 time= 0.0165\n",
      "Epoch: 0134 train_loss= 0.5685 train_acc= 0.9429 val_loss= 0.8905 val_acc= 0.8100 time= 0.0166\n",
      "Epoch: 0135 train_loss= 0.5658 train_acc= 0.9429 val_loss= 0.8880 val_acc= 0.8100 time= 0.0164\n",
      "Epoch: 0136 train_loss= 0.5622 train_acc= 0.9429 val_loss= 0.8844 val_acc= 0.8067 time= 0.0166\n",
      "Epoch: 0137 train_loss= 0.5583 train_acc= 0.9429 val_loss= 0.8816 val_acc= 0.8033 time= 0.0168\n",
      "Epoch: 0138 train_loss= 0.5556 train_acc= 0.9357 val_loss= 0.8795 val_acc= 0.8033 time= 0.0164\n",
      "Epoch: 0139 train_loss= 0.5529 train_acc= 0.9429 val_loss= 0.8775 val_acc= 0.8033 time= 0.0165\n",
      "Epoch: 0140 train_loss= 0.5495 train_acc= 0.9357 val_loss= 0.8738 val_acc= 0.8067 time= 0.0166\n",
      "Epoch: 0141 train_loss= 0.5441 train_acc= 0.9357 val_loss= 0.8722 val_acc= 0.8033 time= 0.0165\n",
      "Epoch: 0142 train_loss= 0.5426 train_acc= 0.9357 val_loss= 0.8696 val_acc= 0.8033 time= 0.0164\n",
      "Epoch: 0143 train_loss= 0.5395 train_acc= 0.9429 val_loss= 0.8686 val_acc= 0.8033 time= 0.0164\n",
      "Epoch: 0144 train_loss= 0.5364 train_acc= 0.9357 val_loss= 0.8660 val_acc= 0.8000 time= 0.0166\n",
      "Epoch: 0145 train_loss= 0.5328 train_acc= 0.9429 val_loss= 0.8640 val_acc= 0.8000 time= 0.0166\n",
      "Epoch: 0146 train_loss= 0.5293 train_acc= 0.9429 val_loss= 0.8618 val_acc= 0.8000 time= 0.0164\n",
      "Epoch: 0147 train_loss= 0.5274 train_acc= 0.9429 val_loss= 0.8604 val_acc= 0.7967 time= 0.0166\n",
      "Epoch: 0148 train_loss= 0.5246 train_acc= 0.9429 val_loss= 0.8590 val_acc= 0.7967 time= 0.0170\n",
      "Epoch: 0149 train_loss= 0.5215 train_acc= 0.9429 val_loss= 0.8564 val_acc= 0.7967 time= 0.0168\n",
      "Epoch: 0150 train_loss= 0.5182 train_acc= 0.9429 val_loss= 0.8543 val_acc= 0.8000 time= 0.0164\n",
      "Epoch: 0151 train_loss= 0.5156 train_acc= 0.9429 val_loss= 0.8517 val_acc= 0.8000 time= 0.0164\n",
      "Epoch: 0152 train_loss= 0.5125 train_acc= 0.9429 val_loss= 0.8488 val_acc= 0.8000 time= 0.0166\n",
      "Epoch: 0153 train_loss= 0.5096 train_acc= 0.9429 val_loss= 0.8462 val_acc= 0.8067 time= 0.0163\n",
      "Epoch: 0154 train_loss= 0.5071 train_acc= 0.9429 val_loss= 0.8438 val_acc= 0.8067 time= 0.0163\n",
      "Epoch: 0155 train_loss= 0.5045 train_acc= 0.9429 val_loss= 0.8421 val_acc= 0.8067 time= 0.0163\n",
      "Epoch: 0156 train_loss= 0.5020 train_acc= 0.9429 val_loss= 0.8406 val_acc= 0.8067 time= 0.0165\n",
      "Epoch: 0157 train_loss= 0.4989 train_acc= 0.9429 val_loss= 0.8384 val_acc= 0.8067 time= 0.0171\n",
      "Epoch: 0158 train_loss= 0.4965 train_acc= 0.9429 val_loss= 0.8372 val_acc= 0.8067 time= 0.0168\n",
      "Epoch: 0159 train_loss= 0.4944 train_acc= 0.9429 val_loss= 0.8340 val_acc= 0.8067 time= 0.0167\n",
      "Epoch: 0160 train_loss= 0.4912 train_acc= 0.9429 val_loss= 0.8328 val_acc= 0.8067 time= 0.0165\n",
      "Epoch: 0161 train_loss= 0.4883 train_acc= 0.9429 val_loss= 0.8300 val_acc= 0.8067 time= 0.0168\n",
      "Epoch: 0162 train_loss= 0.4854 train_acc= 0.9571 val_loss= 0.8267 val_acc= 0.8067 time= 0.0169\n",
      "Epoch: 0163 train_loss= 0.4842 train_acc= 0.9571 val_loss= 0.8254 val_acc= 0.8067 time= 0.0169\n",
      "Epoch: 0164 train_loss= 0.4816 train_acc= 0.9500 val_loss= 0.8224 val_acc= 0.8033 time= 0.0171\n",
      "Epoch: 0165 train_loss= 0.4793 train_acc= 0.9571 val_loss= 0.8195 val_acc= 0.8033 time= 0.0169\n",
      "Epoch: 0166 train_loss= 0.4759 train_acc= 0.9571 val_loss= 0.8179 val_acc= 0.8033 time= 0.0173\n",
      "Epoch: 0167 train_loss= 0.4744 train_acc= 0.9571 val_loss= 0.8161 val_acc= 0.8033 time= 0.0181\n",
      "Epoch: 0168 train_loss= 0.4718 train_acc= 0.9500 val_loss= 0.8148 val_acc= 0.8067 time= 0.0174\n",
      "Epoch: 0169 train_loss= 0.4688 train_acc= 0.9571 val_loss= 0.8135 val_acc= 0.8100 time= 0.0172\n",
      "Epoch: 0170 train_loss= 0.4669 train_acc= 0.9571 val_loss= 0.8127 val_acc= 0.8067 time= 0.0185\n",
      "Epoch: 0171 train_loss= 0.4651 train_acc= 0.9571 val_loss= 0.8107 val_acc= 0.8067 time= 0.0171\n",
      "Epoch: 0172 train_loss= 0.4621 train_acc= 0.9571 val_loss= 0.8092 val_acc= 0.8100 time= 0.0191\n",
      "Epoch: 0173 train_loss= 0.4599 train_acc= 0.9571 val_loss= 0.8083 val_acc= 0.8100 time= 0.0197\n",
      "Epoch: 0174 train_loss= 0.4572 train_acc= 0.9571 val_loss= 0.8060 val_acc= 0.8100 time= 0.0170\n",
      "Epoch: 0175 train_loss= 0.4540 train_acc= 0.9571 val_loss= 0.8044 val_acc= 0.8100 time= 0.0179\n",
      "Epoch: 0176 train_loss= 0.4521 train_acc= 0.9571 val_loss= 0.8030 val_acc= 0.8133 time= 0.0187\n",
      "Epoch: 0177 train_loss= 0.4500 train_acc= 0.9571 val_loss= 0.8006 val_acc= 0.8133 time= 0.0186\n",
      "Epoch: 0178 train_loss= 0.4469 train_acc= 0.9571 val_loss= 0.7997 val_acc= 0.8167 time= 0.0170\n",
      "Epoch: 0179 train_loss= 0.4446 train_acc= 0.9571 val_loss= 0.7981 val_acc= 0.8167 time= 0.0164\n",
      "Epoch: 0180 train_loss= 0.4417 train_acc= 0.9571 val_loss= 0.7963 val_acc= 0.8133 time= 0.0166\n",
      "Epoch: 0181 train_loss= 0.4400 train_acc= 0.9571 val_loss= 0.7952 val_acc= 0.8167 time= 0.0163\n",
      "Epoch: 0182 train_loss= 0.4380 train_acc= 0.9571 val_loss= 0.7939 val_acc= 0.8133 time= 0.0163\n",
      "Epoch: 0183 train_loss= 0.4360 train_acc= 0.9643 val_loss= 0.7934 val_acc= 0.8133 time= 0.0162\n",
      "Epoch: 0184 train_loss= 0.4338 train_acc= 0.9714 val_loss= 0.7922 val_acc= 0.8133 time= 0.0162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0185 train_loss= 0.4315 train_acc= 0.9643 val_loss= 0.7907 val_acc= 0.8100 time= 0.0167\n",
      "Epoch: 0186 train_loss= 0.4298 train_acc= 0.9643 val_loss= 0.7891 val_acc= 0.8067 time= 0.0167\n",
      "Epoch: 0187 train_loss= 0.4270 train_acc= 0.9714 val_loss= 0.7870 val_acc= 0.8100 time= 0.0166\n",
      "Epoch: 0188 train_loss= 0.4241 train_acc= 0.9714 val_loss= 0.7850 val_acc= 0.8100 time= 0.0167\n",
      "Epoch: 0189 train_loss= 0.4230 train_acc= 0.9714 val_loss= 0.7837 val_acc= 0.8167 time= 0.0163\n",
      "Epoch: 0190 train_loss= 0.4204 train_acc= 0.9714 val_loss= 0.7830 val_acc= 0.8133 time= 0.0164\n",
      "Epoch: 0191 train_loss= 0.4186 train_acc= 0.9714 val_loss= 0.7817 val_acc= 0.8167 time= 0.0164\n",
      "Epoch: 0192 train_loss= 0.4162 train_acc= 0.9714 val_loss= 0.7822 val_acc= 0.8200 time= 0.0163\n",
      "Epoch: 0193 train_loss= 0.4147 train_acc= 0.9714 val_loss= 0.7811 val_acc= 0.8200 time= 0.0166\n",
      "Epoch: 0194 train_loss= 0.4129 train_acc= 0.9714 val_loss= 0.7793 val_acc= 0.8200 time= 0.0163\n",
      "Epoch: 0195 train_loss= 0.4112 train_acc= 0.9714 val_loss= 0.7772 val_acc= 0.8167 time= 0.0165\n",
      "Epoch: 0196 train_loss= 0.4088 train_acc= 0.9714 val_loss= 0.7756 val_acc= 0.8200 time= 0.0166\n",
      "Epoch: 0197 train_loss= 0.4079 train_acc= 0.9714 val_loss= 0.7735 val_acc= 0.8167 time= 0.0174\n",
      "Epoch: 0198 train_loss= 0.4055 train_acc= 0.9643 val_loss= 0.7731 val_acc= 0.8167 time= 0.0164\n",
      "Epoch: 0199 train_loss= 0.4044 train_acc= 0.9714 val_loss= 0.7727 val_acc= 0.8200 time= 0.0163\n",
      "Epoch: 0200 train_loss= 0.4023 train_acc= 0.9714 val_loss= 0.7729 val_acc= 0.8200 time= 0.0165\n"
     ]
    }
   ],
   "source": [
    "preds = train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.8267\n",
      "accuracy = 0.8100\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate_preds(preds, [y_test], [idx_test])\n",
    "print(f\"\"\"\n",
    "loss = {test_loss[0]:.4f}\n",
    "accuracy = {test_acc[0]:.4f}\n",
    "\"\"\".strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
