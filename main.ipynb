{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the path of execution\n",
    "EXE_PATH = os.path.abspath(os.path.curdir)\n",
    "# the path of the vendor files\n",
    "VENDOR_PATH = os.path.join(EXE_PATH, 'vendor')\n",
    "# the vendors to include in the system path\n",
    "VENDORS = ['keras-gcn']\n",
    "# create the absolute paths for all vendors\n",
    "VENDORS = list(map(lambda x: os.path.join(VENDOR_PATH, x), VENDORS))\n",
    "# update the Python path to include necessary vendor module\n",
    "sys.path += VENDORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kegra.layers.graph import GraphConvolution\n",
    "from kegra.utils import load_data, preprocess_adj, get_splits, evaluate_preds\n",
    "from ggcn import GaussianGraphConvolution, kl_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "Dataset has 2708 nodes, 5429 edges, 1433 features.\n"
     ]
    }
   ],
   "source": [
    "X, A, y = load_data('data/cora/', dataset='cora')\n",
    "X /= X.sum(1).reshape(-1, 1)\n",
    "A = preprocess_adj(A)\n",
    "y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask = get_splits(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2708x2708 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 13264 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1]], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1110 17:26:24.214613 140268265989952 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1110 17:26:24.221558 140268265989952 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:515: The name tf.sparse_placeholder is deprecated. Please use tf.compat.v1.sparse_placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "support = 1\n",
    "graph = [X, A]\n",
    "G = [Input(shape=(None, None), batch_shape=(None, None), sparse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 17:26:24.226097 140268265989952 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_in = Input(shape=(X.shape[1],))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 17:26:24.234217 140268265989952 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1110 17:26:24.240765 140268265989952 deprecation.py:506] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1110 17:26:24.249119 140268265989952 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define model architecture\n",
    "# NOTE: We pass arguments for graph convolutional layers as a list of tensors.\n",
    "# This is somewhat hacky, more elegant options would require rewriting the Layer base class.\n",
    "H = Dropout(0.5)(X_in)\n",
    "H = GraphConvolution(32, support, activation='relu', kernel_regularizer=l2(5e-4))([H]+G)\n",
    "H = Dropout(0.5)(H)\n",
    "Y = GraphConvolution(y.shape[1], support, activation='softmax')([H]+G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 17:26:24.290334 140268265989952 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[X_in]+G, outputs=Y)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 1433)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1433)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_1 (GraphConvo (None, 32)           45888       dropout_1[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32)           0           graph_convolution_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_2 (GraphConvo (None, 7)            231         dropout_2[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 46,119\n",
      "Trainable params: 46,119\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 17:26:24.366671 140268265989952 deprecation.py:323] From /home/bitcommander/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.9311 train_acc= 0.2929 val_loss= 1.9319 val_acc= 0.3500 time= 1.0749\n",
      "Epoch: 0002 train_loss= 1.9151 train_acc= 0.2929 val_loss= 1.9171 val_acc= 0.3500 time= 0.0192\n",
      "Epoch: 0003 train_loss= 1.8975 train_acc= 0.2929 val_loss= 1.9011 val_acc= 0.3500 time= 0.0238\n",
      "Epoch: 0004 train_loss= 1.8792 train_acc= 0.2929 val_loss= 1.8845 val_acc= 0.3500 time= 0.0199\n",
      "Epoch: 0005 train_loss= 1.8608 train_acc= 0.2929 val_loss= 1.8679 val_acc= 0.3500 time= 0.0253\n",
      "Epoch: 0006 train_loss= 1.8418 train_acc= 0.2929 val_loss= 1.8512 val_acc= 0.3500 time= 0.0241\n",
      "Epoch: 0007 train_loss= 1.8228 train_acc= 0.2929 val_loss= 1.8347 val_acc= 0.3500 time= 0.0196\n",
      "Epoch: 0008 train_loss= 1.8039 train_acc= 0.2929 val_loss= 1.8187 val_acc= 0.3500 time= 0.0189\n",
      "Epoch: 0009 train_loss= 1.7853 train_acc= 0.2929 val_loss= 1.8036 val_acc= 0.3500 time= 0.0189\n",
      "Epoch: 0010 train_loss= 1.7672 train_acc= 0.2929 val_loss= 1.7896 val_acc= 0.3500 time= 0.0216\n",
      "Epoch: 0011 train_loss= 1.7509 train_acc= 0.2929 val_loss= 1.7777 val_acc= 0.3500 time= 0.0218\n",
      "Epoch: 0012 train_loss= 1.7359 train_acc= 0.2929 val_loss= 1.7668 val_acc= 0.3500 time= 0.0238\n",
      "Epoch: 0013 train_loss= 1.7218 train_acc= 0.2929 val_loss= 1.7571 val_acc= 0.3500 time= 0.0229\n",
      "Epoch: 0014 train_loss= 1.7084 train_acc= 0.2929 val_loss= 1.7480 val_acc= 0.3500 time= 0.0192\n",
      "Epoch: 0015 train_loss= 1.6954 train_acc= 0.3000 val_loss= 1.7394 val_acc= 0.3500 time= 0.0247\n",
      "Epoch: 0016 train_loss= 1.6825 train_acc= 0.3286 val_loss= 1.7310 val_acc= 0.3567 time= 0.0198\n",
      "Epoch: 0017 train_loss= 1.6694 train_acc= 0.3571 val_loss= 1.7224 val_acc= 0.3633 time= 0.0238\n",
      "Epoch: 0018 train_loss= 1.6560 train_acc= 0.3643 val_loss= 1.7135 val_acc= 0.3633 time= 0.0204\n",
      "Epoch: 0019 train_loss= 1.6419 train_acc= 0.3857 val_loss= 1.7042 val_acc= 0.3633 time= 0.0184\n",
      "Epoch: 0020 train_loss= 1.6274 train_acc= 0.3929 val_loss= 1.6946 val_acc= 0.3667 time= 0.0194\n",
      "Epoch: 0021 train_loss= 1.6127 train_acc= 0.4071 val_loss= 1.6850 val_acc= 0.3700 time= 0.0190\n",
      "Epoch: 0022 train_loss= 1.5981 train_acc= 0.4214 val_loss= 1.6754 val_acc= 0.3800 time= 0.0192\n",
      "Epoch: 0023 train_loss= 1.5833 train_acc= 0.4214 val_loss= 1.6659 val_acc= 0.3900 time= 0.0198\n",
      "Epoch: 0024 train_loss= 1.5683 train_acc= 0.4357 val_loss= 1.6562 val_acc= 0.4000 time= 0.0229\n",
      "Epoch: 0025 train_loss= 1.5534 train_acc= 0.4643 val_loss= 1.6466 val_acc= 0.4133 time= 0.0195\n",
      "Epoch: 0026 train_loss= 1.5383 train_acc= 0.5000 val_loss= 1.6368 val_acc= 0.4400 time= 0.0238\n",
      "Epoch: 0027 train_loss= 1.5232 train_acc= 0.5143 val_loss= 1.6266 val_acc= 0.4633 time= 0.0240\n",
      "Epoch: 0028 train_loss= 1.5079 train_acc= 0.5357 val_loss= 1.6161 val_acc= 0.4700 time= 0.0276\n",
      "Epoch: 0029 train_loss= 1.4926 train_acc= 0.5429 val_loss= 1.6054 val_acc= 0.4800 time= 0.0185\n",
      "Epoch: 0030 train_loss= 1.4772 train_acc= 0.5500 val_loss= 1.5946 val_acc= 0.4900 time= 0.0196\n",
      "Epoch: 0031 train_loss= 1.4616 train_acc= 0.5714 val_loss= 1.5836 val_acc= 0.5067 time= 0.0232\n",
      "Epoch: 0032 train_loss= 1.4458 train_acc= 0.5714 val_loss= 1.5723 val_acc= 0.5133 time= 0.0190\n",
      "Epoch: 0033 train_loss= 1.4298 train_acc= 0.5786 val_loss= 1.5610 val_acc= 0.5200 time= 0.0193\n",
      "Epoch: 0034 train_loss= 1.4140 train_acc= 0.5857 val_loss= 1.5497 val_acc= 0.5367 time= 0.0186\n",
      "Epoch: 0035 train_loss= 1.3981 train_acc= 0.5857 val_loss= 1.5382 val_acc= 0.5467 time= 0.0192\n",
      "Epoch: 0036 train_loss= 1.3824 train_acc= 0.6071 val_loss= 1.5269 val_acc= 0.5467 time= 0.0229\n",
      "Epoch: 0037 train_loss= 1.3668 train_acc= 0.6286 val_loss= 1.5152 val_acc= 0.5467 time= 0.0242\n",
      "Epoch: 0038 train_loss= 1.3513 train_acc= 0.6357 val_loss= 1.5036 val_acc= 0.5600 time= 0.0189\n",
      "Epoch: 0039 train_loss= 1.3360 train_acc= 0.6429 val_loss= 1.4921 val_acc= 0.5700 time= 0.0240\n",
      "Epoch: 0040 train_loss= 1.3207 train_acc= 0.6643 val_loss= 1.4807 val_acc= 0.5767 time= 0.0227\n",
      "Epoch: 0041 train_loss= 1.3053 train_acc= 0.6857 val_loss= 1.4693 val_acc= 0.5933 time= 0.0231\n",
      "Epoch: 0042 train_loss= 1.2897 train_acc= 0.6929 val_loss= 1.4577 val_acc= 0.5867 time= 0.0191\n",
      "Epoch: 0043 train_loss= 1.2741 train_acc= 0.7000 val_loss= 1.4462 val_acc= 0.5933 time= 0.0248\n",
      "Epoch: 0044 train_loss= 1.2586 train_acc= 0.7071 val_loss= 1.4346 val_acc= 0.6033 time= 0.0254\n",
      "Epoch: 0045 train_loss= 1.2433 train_acc= 0.7286 val_loss= 1.4229 val_acc= 0.6133 time= 0.0231\n",
      "Epoch: 0046 train_loss= 1.2283 train_acc= 0.7357 val_loss= 1.4113 val_acc= 0.6167 time= 0.0231\n",
      "Epoch: 0047 train_loss= 1.2137 train_acc= 0.7429 val_loss= 1.3996 val_acc= 0.6200 time= 0.0258\n",
      "Epoch: 0048 train_loss= 1.1993 train_acc= 0.7500 val_loss= 1.3880 val_acc= 0.6200 time= 0.0232\n",
      "Epoch: 0049 train_loss= 1.1853 train_acc= 0.7571 val_loss= 1.3767 val_acc= 0.6233 time= 0.0217\n",
      "Epoch: 0050 train_loss= 1.1713 train_acc= 0.7571 val_loss= 1.3655 val_acc= 0.6267 time= 0.0202\n",
      "Epoch: 0051 train_loss= 1.1573 train_acc= 0.7500 val_loss= 1.3545 val_acc= 0.6333 time= 0.0237\n",
      "Epoch: 0052 train_loss= 1.1432 train_acc= 0.7643 val_loss= 1.3439 val_acc= 0.6467 time= 0.0199\n",
      "Epoch: 0053 train_loss= 1.1291 train_acc= 0.7786 val_loss= 1.3335 val_acc= 0.6567 time= 0.0235\n",
      "Epoch: 0054 train_loss= 1.1152 train_acc= 0.7929 val_loss= 1.3232 val_acc= 0.6667 time= 0.0199\n",
      "Epoch: 0055 train_loss= 1.1014 train_acc= 0.7929 val_loss= 1.3127 val_acc= 0.6700 time= 0.0237\n",
      "Epoch: 0056 train_loss= 1.0879 train_acc= 0.8000 val_loss= 1.3023 val_acc= 0.6767 time= 0.0236\n",
      "Epoch: 0057 train_loss= 1.0745 train_acc= 0.8000 val_loss= 1.2920 val_acc= 0.6800 time= 0.0189\n",
      "Epoch: 0058 train_loss= 1.0613 train_acc= 0.8071 val_loss= 1.2817 val_acc= 0.6900 time= 0.0272\n",
      "Epoch: 0059 train_loss= 1.0482 train_acc= 0.8071 val_loss= 1.2718 val_acc= 0.6867 time= 0.0230\n",
      "Epoch: 0060 train_loss= 1.0354 train_acc= 0.8143 val_loss= 1.2620 val_acc= 0.6833 time= 0.0256\n",
      "Epoch: 0061 train_loss= 1.0225 train_acc= 0.8143 val_loss= 1.2520 val_acc= 0.6833 time= 0.0231\n",
      "Epoch: 0062 train_loss= 1.0097 train_acc= 0.8143 val_loss= 1.2417 val_acc= 0.6867 time= 0.0238\n",
      "Epoch: 0063 train_loss= 0.9974 train_acc= 0.8143 val_loss= 1.2317 val_acc= 0.6867 time= 0.0241\n",
      "Epoch: 0064 train_loss= 0.9854 train_acc= 0.8214 val_loss= 1.2224 val_acc= 0.6900 time= 0.0184\n",
      "Epoch: 0065 train_loss= 0.9734 train_acc= 0.8214 val_loss= 1.2134 val_acc= 0.6967 time= 0.0202\n",
      "Epoch: 0066 train_loss= 0.9617 train_acc= 0.8214 val_loss= 1.2046 val_acc= 0.7033 time= 0.0242\n",
      "Epoch: 0067 train_loss= 0.9498 train_acc= 0.8214 val_loss= 1.1965 val_acc= 0.7233 time= 0.0191\n",
      "Epoch: 0068 train_loss= 0.9383 train_acc= 0.8429 val_loss= 1.1882 val_acc= 0.7367 time= 0.0191\n",
      "Epoch: 0069 train_loss= 0.9271 train_acc= 0.8429 val_loss= 1.1803 val_acc= 0.7400 time= 0.0199\n",
      "Epoch: 0070 train_loss= 0.9163 train_acc= 0.8357 val_loss= 1.1716 val_acc= 0.7367 time= 0.0195\n",
      "Epoch: 0071 train_loss= 0.9057 train_acc= 0.8357 val_loss= 1.1636 val_acc= 0.7367 time= 0.0187\n",
      "Epoch: 0072 train_loss= 0.8950 train_acc= 0.8429 val_loss= 1.1546 val_acc= 0.7367 time= 0.0191\n",
      "Epoch: 0073 train_loss= 0.8845 train_acc= 0.8714 val_loss= 1.1460 val_acc= 0.7500 time= 0.0192\n",
      "Epoch: 0074 train_loss= 0.8741 train_acc= 0.8857 val_loss= 1.1373 val_acc= 0.7467 time= 0.0186\n",
      "Epoch: 0075 train_loss= 0.8641 train_acc= 0.8857 val_loss= 1.1292 val_acc= 0.7567 time= 0.0196\n",
      "Epoch: 0076 train_loss= 0.8549 train_acc= 0.8857 val_loss= 1.1213 val_acc= 0.7567 time= 0.0197\n",
      "Epoch: 0077 train_loss= 0.8459 train_acc= 0.8857 val_loss= 1.1138 val_acc= 0.7500 time= 0.0191\n",
      "Epoch: 0078 train_loss= 0.8368 train_acc= 0.8857 val_loss= 1.1060 val_acc= 0.7500 time= 0.0248\n",
      "Epoch: 0079 train_loss= 0.8279 train_acc= 0.8857 val_loss= 1.0985 val_acc= 0.7467 time= 0.0243\n",
      "Epoch: 0080 train_loss= 0.8184 train_acc= 0.8857 val_loss= 1.0915 val_acc= 0.7433 time= 0.0251\n",
      "Epoch: 0081 train_loss= 0.8090 train_acc= 0.8857 val_loss= 1.0849 val_acc= 0.7500 time= 0.0249\n",
      "Epoch: 0082 train_loss= 0.7998 train_acc= 0.8857 val_loss= 1.0789 val_acc= 0.7600 time= 0.0231\n",
      "Epoch: 0083 train_loss= 0.7912 train_acc= 0.8929 val_loss= 1.0724 val_acc= 0.7667 time= 0.0234\n",
      "Epoch: 0084 train_loss= 0.7828 train_acc= 0.8929 val_loss= 1.0663 val_acc= 0.7700 time= 0.0242\n",
      "Epoch: 0085 train_loss= 0.7747 train_acc= 0.8929 val_loss= 1.0606 val_acc= 0.7800 time= 0.0211\n",
      "Epoch: 0086 train_loss= 0.7668 train_acc= 0.9000 val_loss= 1.0552 val_acc= 0.7767 time= 0.0230\n",
      "Epoch: 0087 train_loss= 0.7589 train_acc= 0.9000 val_loss= 1.0494 val_acc= 0.7800 time= 0.0240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0088 train_loss= 0.7507 train_acc= 0.9000 val_loss= 1.0426 val_acc= 0.7800 time= 0.0186\n",
      "Epoch: 0089 train_loss= 0.7427 train_acc= 0.9000 val_loss= 1.0347 val_acc= 0.7833 time= 0.0257\n",
      "Epoch: 0090 train_loss= 0.7350 train_acc= 0.9000 val_loss= 1.0278 val_acc= 0.7833 time= 0.0237\n",
      "Epoch: 0091 train_loss= 0.7277 train_acc= 0.9000 val_loss= 1.0209 val_acc= 0.7800 time= 0.0206\n",
      "Epoch: 0092 train_loss= 0.7204 train_acc= 0.9000 val_loss= 1.0136 val_acc= 0.7833 time= 0.0198\n",
      "Epoch: 0093 train_loss= 0.7135 train_acc= 0.9071 val_loss= 1.0067 val_acc= 0.7767 time= 0.0240\n",
      "Epoch: 0094 train_loss= 0.7067 train_acc= 0.9071 val_loss= 1.0010 val_acc= 0.7733 time= 0.0184\n",
      "Epoch: 0095 train_loss= 0.6996 train_acc= 0.9143 val_loss= 0.9966 val_acc= 0.7800 time= 0.0235\n",
      "Epoch: 0096 train_loss= 0.6929 train_acc= 0.9214 val_loss= 0.9932 val_acc= 0.7800 time= 0.0235\n",
      "Epoch: 0097 train_loss= 0.6868 train_acc= 0.9214 val_loss= 0.9901 val_acc= 0.7800 time= 0.0255\n",
      "Epoch: 0098 train_loss= 0.6807 train_acc= 0.9214 val_loss= 0.9861 val_acc= 0.7867 time= 0.0205\n",
      "Epoch: 0099 train_loss= 0.6746 train_acc= 0.9214 val_loss= 0.9813 val_acc= 0.7867 time= 0.0192\n",
      "Epoch: 0100 train_loss= 0.6683 train_acc= 0.9214 val_loss= 0.9767 val_acc= 0.7933 time= 0.0194\n",
      "Epoch: 0101 train_loss= 0.6622 train_acc= 0.9286 val_loss= 0.9720 val_acc= 0.7933 time= 0.0236\n",
      "Epoch: 0102 train_loss= 0.6561 train_acc= 0.9357 val_loss= 0.9663 val_acc= 0.7900 time= 0.0190\n",
      "Epoch: 0103 train_loss= 0.6503 train_acc= 0.9357 val_loss= 0.9600 val_acc= 0.7867 time= 0.0231\n",
      "Epoch: 0104 train_loss= 0.6445 train_acc= 0.9357 val_loss= 0.9540 val_acc= 0.7800 time= 0.0237\n",
      "Epoch: 0105 train_loss= 0.6388 train_acc= 0.9357 val_loss= 0.9473 val_acc= 0.7800 time= 0.0244\n",
      "Epoch: 0106 train_loss= 0.6330 train_acc= 0.9429 val_loss= 0.9417 val_acc= 0.7800 time= 0.0244\n",
      "Epoch: 0107 train_loss= 0.6275 train_acc= 0.9357 val_loss= 0.9371 val_acc= 0.7800 time= 0.0244\n",
      "Epoch: 0108 train_loss= 0.6221 train_acc= 0.9429 val_loss= 0.9337 val_acc= 0.7867 time= 0.0196\n",
      "Epoch: 0109 train_loss= 0.6178 train_acc= 0.9500 val_loss= 0.9317 val_acc= 0.7867 time= 0.0289\n",
      "Epoch: 0110 train_loss= 0.6137 train_acc= 0.9571 val_loss= 0.9308 val_acc= 0.7867 time= 0.0195\n",
      "Epoch: 0111 train_loss= 0.6097 train_acc= 0.9571 val_loss= 0.9299 val_acc= 0.7933 time= 0.0193\n",
      "Epoch: 0112 train_loss= 0.6035 train_acc= 0.9571 val_loss= 0.9247 val_acc= 0.7967 time= 0.0242\n",
      "Epoch: 0113 train_loss= 0.5974 train_acc= 0.9571 val_loss= 0.9201 val_acc= 0.7967 time= 0.0269\n",
      "Epoch: 0114 train_loss= 0.5917 train_acc= 0.9500 val_loss= 0.9161 val_acc= 0.7933 time= 0.0193\n",
      "Epoch: 0115 train_loss= 0.5871 train_acc= 0.9500 val_loss= 0.9135 val_acc= 0.7900 time= 0.0191\n",
      "Epoch: 0116 train_loss= 0.5832 train_acc= 0.9500 val_loss= 0.9109 val_acc= 0.7833 time= 0.0283\n",
      "Epoch: 0117 train_loss= 0.5793 train_acc= 0.9500 val_loss= 0.9073 val_acc= 0.7800 time= 0.0204\n",
      "Epoch: 0118 train_loss= 0.5756 train_acc= 0.9500 val_loss= 0.9038 val_acc= 0.7800 time= 0.0235\n",
      "Epoch: 0119 train_loss= 0.5716 train_acc= 0.9429 val_loss= 0.9002 val_acc= 0.7800 time= 0.0186\n",
      "Epoch: 0120 train_loss= 0.5668 train_acc= 0.9500 val_loss= 0.8970 val_acc= 0.7833 time= 0.0243\n",
      "Epoch: 0121 train_loss= 0.5622 train_acc= 0.9500 val_loss= 0.8943 val_acc= 0.7833 time= 0.0200\n",
      "Epoch: 0122 train_loss= 0.5579 train_acc= 0.9500 val_loss= 0.8912 val_acc= 0.7833 time= 0.0192\n",
      "Epoch: 0123 train_loss= 0.5538 train_acc= 0.9500 val_loss= 0.8887 val_acc= 0.7900 time= 0.0237\n",
      "Epoch: 0124 train_loss= 0.5497 train_acc= 0.9571 val_loss= 0.8859 val_acc= 0.7933 time= 0.0241\n",
      "Epoch: 0125 train_loss= 0.5459 train_acc= 0.9571 val_loss= 0.8836 val_acc= 0.8000 time= 0.0238\n",
      "Epoch: 0126 train_loss= 0.5420 train_acc= 0.9500 val_loss= 0.8811 val_acc= 0.8000 time= 0.0250\n",
      "Epoch: 0127 train_loss= 0.5382 train_acc= 0.9429 val_loss= 0.8786 val_acc= 0.8000 time= 0.0232\n",
      "Epoch: 0128 train_loss= 0.5345 train_acc= 0.9429 val_loss= 0.8755 val_acc= 0.7967 time= 0.0192\n",
      "Epoch: 0129 train_loss= 0.5305 train_acc= 0.9429 val_loss= 0.8725 val_acc= 0.7933 time= 0.0193\n",
      "Epoch: 0130 train_loss= 0.5267 train_acc= 0.9500 val_loss= 0.8691 val_acc= 0.7967 time= 0.0236\n",
      "Epoch: 0131 train_loss= 0.5225 train_acc= 0.9500 val_loss= 0.8657 val_acc= 0.7967 time= 0.0187\n",
      "Epoch: 0132 train_loss= 0.5180 train_acc= 0.9500 val_loss= 0.8621 val_acc= 0.8000 time= 0.0241\n",
      "Epoch: 0133 train_loss= 0.5142 train_acc= 0.9571 val_loss= 0.8571 val_acc= 0.8000 time= 0.0204\n",
      "Epoch: 0134 train_loss= 0.5109 train_acc= 0.9571 val_loss= 0.8522 val_acc= 0.8033 time= 0.0206\n",
      "Epoch: 0135 train_loss= 0.5084 train_acc= 0.9643 val_loss= 0.8473 val_acc= 0.8000 time= 0.0238\n",
      "Epoch: 0136 train_loss= 0.5064 train_acc= 0.9643 val_loss= 0.8431 val_acc= 0.7967 time= 0.0191\n",
      "Epoch: 0137 train_loss= 0.5034 train_acc= 0.9643 val_loss= 0.8403 val_acc= 0.8033 time= 0.0215\n",
      "Epoch: 0138 train_loss= 0.5000 train_acc= 0.9643 val_loss= 0.8392 val_acc= 0.8067 time= 0.0241\n",
      "Epoch: 0139 train_loss= 0.4971 train_acc= 0.9643 val_loss= 0.8398 val_acc= 0.8133 time= 0.0279\n",
      "Epoch: 0140 train_loss= 0.4946 train_acc= 0.9643 val_loss= 0.8398 val_acc= 0.8067 time= 0.0245\n",
      "Epoch: 0141 train_loss= 0.4919 train_acc= 0.9643 val_loss= 0.8397 val_acc= 0.8067 time= 0.0236\n",
      "Epoch: 0142 train_loss= 0.4891 train_acc= 0.9571 val_loss= 0.8391 val_acc= 0.7967 time= 0.0186\n",
      "Epoch: 0143 train_loss= 0.4867 train_acc= 0.9571 val_loss= 0.8388 val_acc= 0.7933 time= 0.0231\n",
      "Epoch: 0144 train_loss= 0.4846 train_acc= 0.9571 val_loss= 0.8374 val_acc= 0.7933 time= 0.0197\n",
      "Epoch: 0145 train_loss= 0.4818 train_acc= 0.9571 val_loss= 0.8342 val_acc= 0.7967 time= 0.0233\n",
      "Epoch: 0146 train_loss= 0.4783 train_acc= 0.9571 val_loss= 0.8300 val_acc= 0.8000 time= 0.0186\n",
      "Epoch: 0147 train_loss= 0.4756 train_acc= 0.9571 val_loss= 0.8265 val_acc= 0.7900 time= 0.0193\n",
      "Epoch: 0148 train_loss= 0.4729 train_acc= 0.9643 val_loss= 0.8232 val_acc= 0.7900 time= 0.0272\n",
      "Epoch: 0149 train_loss= 0.4692 train_acc= 0.9643 val_loss= 0.8203 val_acc= 0.7967 time= 0.0183\n",
      "Epoch: 0150 train_loss= 0.4663 train_acc= 0.9643 val_loss= 0.8189 val_acc= 0.8067 time= 0.0190\n",
      "Epoch: 0151 train_loss= 0.4641 train_acc= 0.9714 val_loss= 0.8184 val_acc= 0.8100 time= 0.0200\n",
      "Epoch: 0152 train_loss= 0.4627 train_acc= 0.9714 val_loss= 0.8193 val_acc= 0.8233 time= 0.0200\n",
      "Epoch: 0153 train_loss= 0.4621 train_acc= 0.9714 val_loss= 0.8208 val_acc= 0.8233 time= 0.0273\n",
      "Epoch: 0154 train_loss= 0.4603 train_acc= 0.9714 val_loss= 0.8183 val_acc= 0.8233 time= 0.0218\n",
      "Epoch: 0155 train_loss= 0.4569 train_acc= 0.9714 val_loss= 0.8125 val_acc= 0.8333 time= 0.0197\n",
      "Epoch: 0156 train_loss= 0.4536 train_acc= 0.9714 val_loss= 0.8079 val_acc= 0.8233 time= 0.0213\n",
      "Epoch: 0157 train_loss= 0.4512 train_acc= 0.9714 val_loss= 0.8046 val_acc= 0.8167 time= 0.0205\n",
      "Epoch: 0158 train_loss= 0.4493 train_acc= 0.9714 val_loss= 0.8020 val_acc= 0.8000 time= 0.0255\n",
      "Epoch: 0159 train_loss= 0.4466 train_acc= 0.9714 val_loss= 0.8008 val_acc= 0.8000 time= 0.0233\n",
      "Epoch: 0160 train_loss= 0.4441 train_acc= 0.9714 val_loss= 0.8007 val_acc= 0.8000 time= 0.0233\n",
      "Epoch: 0161 train_loss= 0.4416 train_acc= 0.9643 val_loss= 0.8010 val_acc= 0.8067 time= 0.0202\n",
      "Epoch: 0162 train_loss= 0.4397 train_acc= 0.9714 val_loss= 0.8015 val_acc= 0.8100 time= 0.0239\n",
      "Epoch: 0163 train_loss= 0.4379 train_acc= 0.9714 val_loss= 0.8016 val_acc= 0.8167 time= 0.0230\n",
      "Epoch: 0164 train_loss= 0.4359 train_acc= 0.9714 val_loss= 0.8005 val_acc= 0.8267 time= 0.0234\n",
      "Epoch: 0165 train_loss= 0.4342 train_acc= 0.9714 val_loss= 0.8009 val_acc= 0.8233 time= 0.0239\n",
      "Epoch: 0166 train_loss= 0.4332 train_acc= 0.9714 val_loss= 0.8014 val_acc= 0.8233 time= 0.0186\n",
      "Epoch: 0167 train_loss= 0.4316 train_acc= 0.9714 val_loss= 0.8009 val_acc= 0.8233 time= 0.0194\n",
      "Epoch: 0168 train_loss= 0.4292 train_acc= 0.9714 val_loss= 0.7991 val_acc= 0.8233 time= 0.0243\n",
      "Epoch: 0169 train_loss= 0.4268 train_acc= 0.9714 val_loss= 0.7960 val_acc= 0.8233 time= 0.0232\n",
      "Epoch: 0170 train_loss= 0.4238 train_acc= 0.9714 val_loss= 0.7919 val_acc= 0.8267 time= 0.0193\n",
      "Epoch: 0171 train_loss= 0.4211 train_acc= 0.9714 val_loss= 0.7880 val_acc= 0.8167 time= 0.0237\n",
      "Epoch: 0172 train_loss= 0.4190 train_acc= 0.9714 val_loss= 0.7855 val_acc= 0.8100 time= 0.0241\n",
      "Epoch: 0173 train_loss= 0.4176 train_acc= 0.9714 val_loss= 0.7843 val_acc= 0.8067 time= 0.0192\n",
      "Epoch: 0174 train_loss= 0.4165 train_acc= 0.9714 val_loss= 0.7840 val_acc= 0.8033 time= 0.0202\n",
      "Epoch: 0175 train_loss= 0.4145 train_acc= 0.9786 val_loss= 0.7847 val_acc= 0.7967 time= 0.0193\n",
      "Epoch: 0176 train_loss= 0.4121 train_acc= 0.9786 val_loss= 0.7857 val_acc= 0.8033 time= 0.0202\n",
      "Epoch: 0177 train_loss= 0.4100 train_acc= 0.9786 val_loss= 0.7881 val_acc= 0.8100 time= 0.0237\n",
      "Epoch: 0178 train_loss= 0.4083 train_acc= 0.9786 val_loss= 0.7901 val_acc= 0.8067 time= 0.0247\n",
      "Epoch: 0179 train_loss= 0.4066 train_acc= 0.9786 val_loss= 0.7905 val_acc= 0.8067 time= 0.0190\n",
      "Epoch: 0180 train_loss= 0.4052 train_acc= 0.9786 val_loss= 0.7892 val_acc= 0.8033 time= 0.0237\n",
      "Epoch: 0181 train_loss= 0.4044 train_acc= 0.9786 val_loss= 0.7875 val_acc= 0.8033 time= 0.0184\n",
      "Epoch: 0182 train_loss= 0.4034 train_acc= 0.9786 val_loss= 0.7852 val_acc= 0.8033 time= 0.0208\n",
      "Epoch: 0183 train_loss= 0.4011 train_acc= 0.9786 val_loss= 0.7816 val_acc= 0.8033 time= 0.0236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0184 train_loss= 0.3990 train_acc= 0.9714 val_loss= 0.7784 val_acc= 0.8100 time= 0.0245\n",
      "Epoch: 0185 train_loss= 0.3968 train_acc= 0.9714 val_loss= 0.7764 val_acc= 0.8067 time= 0.0237\n",
      "Epoch: 0186 train_loss= 0.3951 train_acc= 0.9714 val_loss= 0.7755 val_acc= 0.8067 time= 0.0233\n",
      "Epoch: 0187 train_loss= 0.3939 train_acc= 0.9714 val_loss= 0.7753 val_acc= 0.8067 time= 0.0189\n",
      "Epoch: 0188 train_loss= 0.3924 train_acc= 0.9714 val_loss= 0.7754 val_acc= 0.8067 time= 0.0248\n",
      "Epoch: 0189 train_loss= 0.3909 train_acc= 0.9714 val_loss= 0.7759 val_acc= 0.8067 time= 0.0233\n",
      "Epoch: 0190 train_loss= 0.3889 train_acc= 0.9786 val_loss= 0.7762 val_acc= 0.8067 time= 0.0244\n",
      "Epoch: 0191 train_loss= 0.3872 train_acc= 0.9786 val_loss= 0.7751 val_acc= 0.8067 time= 0.0196\n",
      "Epoch: 0192 train_loss= 0.3862 train_acc= 0.9857 val_loss= 0.7763 val_acc= 0.8067 time= 0.0291\n",
      "Epoch: 0193 train_loss= 0.3854 train_acc= 0.9786 val_loss= 0.7761 val_acc= 0.8100 time= 0.0236\n",
      "Epoch: 0194 train_loss= 0.3836 train_acc= 0.9786 val_loss= 0.7749 val_acc= 0.8100 time= 0.0204\n",
      "Epoch: 0195 train_loss= 0.3809 train_acc= 0.9786 val_loss= 0.7720 val_acc= 0.8133 time= 0.0234\n",
      "Epoch: 0196 train_loss= 0.3779 train_acc= 0.9786 val_loss= 0.7688 val_acc= 0.8133 time= 0.0192\n",
      "Epoch: 0197 train_loss= 0.3759 train_acc= 0.9857 val_loss= 0.7673 val_acc= 0.8133 time= 0.0186\n",
      "Epoch: 0198 train_loss= 0.3747 train_acc= 0.9857 val_loss= 0.7671 val_acc= 0.8133 time= 0.0187\n",
      "Epoch: 0199 train_loss= 0.3723 train_acc= 0.9857 val_loss= 0.7652 val_acc= 0.8133 time= 0.0247\n",
      "Epoch: 0200 train_loss= 0.3694 train_acc= 0.9857 val_loss= 0.7608 val_acc= 0.8133 time= 0.0232\n"
     ]
    }
   ],
   "source": [
    "NB_EPOCH = 200\n",
    "PATIENCE = 10  # early stopping patience\n",
    "# Helper variables for main training loop\n",
    "wait = 0\n",
    "preds = None\n",
    "best_val_loss = 99999\n",
    "\n",
    "# Fit\n",
    "for epoch in range(1, NB_EPOCH+1):\n",
    "\n",
    "    # Log wall-clock time\n",
    "    t = time.time()\n",
    "\n",
    "    # Single training iteration (we mask nodes without labels for loss calculation)\n",
    "    model.fit(graph, y_train, sample_weight=train_mask,\n",
    "              batch_size=A.shape[0], epochs=1, shuffle=False, verbose=0)\n",
    "\n",
    "    # Predict on full dataset\n",
    "    preds = model.predict(graph, batch_size=A.shape[0])\n",
    "\n",
    "    # Train / validation scores\n",
    "    train_val_loss, train_val_acc = evaluate_preds(preds, [y_train, y_val],\n",
    "                                                   [idx_train, idx_val])\n",
    "    print(\"Epoch: {:04d}\".format(epoch),\n",
    "          \"train_loss= {:.4f}\".format(train_val_loss[0]),\n",
    "          \"train_acc= {:.4f}\".format(train_val_acc[0]),\n",
    "          \"val_loss= {:.4f}\".format(train_val_loss[1]),\n",
    "          \"val_acc= {:.4f}\".format(train_val_acc[1]),\n",
    "          \"time= {:.4f}\".format(time.time() - t))\n",
    "\n",
    "    # Early stopping\n",
    "    if train_val_loss[1] < best_val_loss:\n",
    "        best_val_loss = train_val_loss[1]\n",
    "        wait = 0\n",
    "    else:\n",
    "        if wait >= PATIENCE:\n",
    "            print('Epoch {}: early stopping'.format(epoch))\n",
    "            break\n",
    "        wait += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.8003\n",
      "accuracy = 0.8050\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate_preds(preds, [y_test], [idx_test])\n",
    "print(f\"\"\"\n",
    "loss = {test_loss[0]:.4f}\n",
    "accuracy = {test_acc[0]:.4f}\n",
    "\"\"\".strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Graph Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 17:26:29.839422 140268265989952 deprecation.py:323] From /home/bitcommander/Desktop/robust-graph-convolutional-networks-against-adversarial-attacks-implementation/ggcn/ggcl.py:28: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "W1110 17:26:29.840713 140268265989952 deprecation.py:323] From /home/bitcommander/.local/lib/python3.7/site-packages/tensorflow/python/ops/distributions/normal.py:160: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    }
   ],
   "source": [
    "H = Dropout(0.5)(X_in)\n",
    "H1 = GaussianGraphConvolution(16,\n",
    "    is_first=True,\n",
    "    activation='relu',\n",
    ")([H]+G)\n",
    "Y = GaussianGraphConvolution(y.shape[1],\n",
    "    is_last=True,\n",
    "    activation='softmax',\n",
    ")(H1+G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[X_in]+G, outputs=Y)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01))\n",
    "# model.add_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 17:26:29.879734 140268265989952 deprecation.py:323] From /home/bitcommander/Desktop/robust-graph-convolutional-networks-against-adversarial-attacks-implementation/ggcn/losses.py:27: kl_divergence (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    }
   ],
   "source": [
    "model.add_loss(kl_reg(*H1), H1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 1433)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1433)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_graph_convolution_1 (G [(None, 16), (None,  45856       dropout_3[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_graph_convolution_2 (G (None, 7)            224         gaussian_graph_convolution_1[0][0\n",
      "                                                                 gaussian_graph_convolution_1[0][1\n",
      "                                                                 input_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 46,080\n",
      "Trainable params: 46,080\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.9409 train_acc= 0.3929 val_loss= 1.9429 val_acc= 0.2700 time= 0.4139\n",
      "Epoch: 0002 train_loss= 1.9334 train_acc= 0.5429 val_loss= 1.9387 val_acc= 0.4200 time= 0.0234\n",
      "Epoch: 0003 train_loss= 1.9242 train_acc= 0.6786 val_loss= 1.9324 val_acc= 0.5500 time= 0.0250\n",
      "Epoch: 0004 train_loss= 1.9131 train_acc= 0.7143 val_loss= 1.9257 val_acc= 0.5933 time= 0.0282\n",
      "Epoch: 0005 train_loss= 1.9012 train_acc= 0.6857 val_loss= 1.9177 val_acc= 0.5800 time= 0.0231\n",
      "Epoch: 0006 train_loss= 1.8897 train_acc= 0.6857 val_loss= 1.9095 val_acc= 0.5600 time= 0.0193\n",
      "Epoch: 0007 train_loss= 1.8765 train_acc= 0.6286 val_loss= 1.8994 val_acc= 0.5400 time= 0.0274\n",
      "Epoch: 0008 train_loss= 1.8617 train_acc= 0.6643 val_loss= 1.8907 val_acc= 0.5600 time= 0.0226\n",
      "Epoch: 0009 train_loss= 1.8451 train_acc= 0.6500 val_loss= 1.8810 val_acc= 0.5133 time= 0.0236\n",
      "Epoch: 0010 train_loss= 1.8308 train_acc= 0.6357 val_loss= 1.8671 val_acc= 0.5267 time= 0.0183\n",
      "Epoch: 0011 train_loss= 1.8154 train_acc= 0.6286 val_loss= 1.8578 val_acc= 0.5200 time= 0.0212\n",
      "Epoch: 0012 train_loss= 1.7972 train_acc= 0.6214 val_loss= 1.8439 val_acc= 0.5400 time= 0.0287\n",
      "Epoch: 0013 train_loss= 1.7793 train_acc= 0.6429 val_loss= 1.8322 val_acc= 0.5333 time= 0.0187\n",
      "Epoch: 0014 train_loss= 1.7596 train_acc= 0.6286 val_loss= 1.8177 val_acc= 0.5400 time= 0.0230\n",
      "Epoch: 0015 train_loss= 1.7397 train_acc= 0.6143 val_loss= 1.8033 val_acc= 0.5300 time= 0.0196\n",
      "Epoch: 0016 train_loss= 1.7184 train_acc= 0.6500 val_loss= 1.7877 val_acc= 0.5400 time= 0.0210\n",
      "Epoch: 0017 train_loss= 1.6947 train_acc= 0.6500 val_loss= 1.7735 val_acc= 0.5333 time= 0.0189\n",
      "Epoch: 0018 train_loss= 1.6741 train_acc= 0.6357 val_loss= 1.7566 val_acc= 0.5400 time= 0.0199\n",
      "Epoch: 0019 train_loss= 1.6522 train_acc= 0.6286 val_loss= 1.7406 val_acc= 0.5267 time= 0.0200\n",
      "Epoch: 0020 train_loss= 1.6289 train_acc= 0.6214 val_loss= 1.7250 val_acc= 0.5333 time= 0.0195\n",
      "Epoch: 0021 train_loss= 1.6051 train_acc= 0.6286 val_loss= 1.7051 val_acc= 0.5333 time= 0.0205\n",
      "Epoch: 0022 train_loss= 1.5802 train_acc= 0.6071 val_loss= 1.6871 val_acc= 0.5433 time= 0.0258\n",
      "Epoch: 0023 train_loss= 1.5546 train_acc= 0.6071 val_loss= 1.6700 val_acc= 0.5333 time= 0.0279\n",
      "Epoch: 0024 train_loss= 1.5309 train_acc= 0.6071 val_loss= 1.6515 val_acc= 0.5333 time= 0.0237\n",
      "Epoch: 0025 train_loss= 1.5037 train_acc= 0.6357 val_loss= 1.6323 val_acc= 0.5400 time= 0.0236\n",
      "Epoch: 0026 train_loss= 1.4803 train_acc= 0.6286 val_loss= 1.6130 val_acc= 0.5333 time= 0.0187\n",
      "Epoch: 0027 train_loss= 1.4511 train_acc= 0.6500 val_loss= 1.5944 val_acc= 0.5500 time= 0.0286\n",
      "Epoch: 0028 train_loss= 1.4278 train_acc= 0.6357 val_loss= 1.5746 val_acc= 0.5533 time= 0.0234\n",
      "Epoch: 0029 train_loss= 1.4035 train_acc= 0.6286 val_loss= 1.5557 val_acc= 0.5433 time= 0.0192\n",
      "Epoch: 0030 train_loss= 1.3740 train_acc= 0.6357 val_loss= 1.5366 val_acc= 0.5533 time= 0.0194\n",
      "Epoch: 0031 train_loss= 1.3493 train_acc= 0.6500 val_loss= 1.5164 val_acc= 0.5567 time= 0.0202\n",
      "Epoch: 0032 train_loss= 1.3243 train_acc= 0.6643 val_loss= 1.4968 val_acc= 0.5600 time= 0.0192\n",
      "Epoch: 0033 train_loss= 1.2980 train_acc= 0.6643 val_loss= 1.4774 val_acc= 0.5633 time= 0.0237\n",
      "Epoch: 0034 train_loss= 1.2722 train_acc= 0.6857 val_loss= 1.4580 val_acc= 0.5667 time= 0.0247\n",
      "Epoch: 0035 train_loss= 1.2472 train_acc= 0.6857 val_loss= 1.4387 val_acc= 0.5700 time= 0.0236\n",
      "Epoch: 0036 train_loss= 1.2221 train_acc= 0.6929 val_loss= 1.4191 val_acc= 0.5833 time= 0.0211\n",
      "Epoch: 0037 train_loss= 1.1986 train_acc= 0.7000 val_loss= 1.3999 val_acc= 0.5833 time= 0.0290\n",
      "Epoch: 0038 train_loss= 1.1739 train_acc= 0.7071 val_loss= 1.3805 val_acc= 0.5867 time= 0.0238\n",
      "Epoch: 0039 train_loss= 1.1483 train_acc= 0.7143 val_loss= 1.3622 val_acc= 0.5867 time= 0.0254\n",
      "Epoch: 0040 train_loss= 1.1250 train_acc= 0.7286 val_loss= 1.3432 val_acc= 0.5933 time= 0.0239\n",
      "Epoch: 0041 train_loss= 1.1006 train_acc= 0.7357 val_loss= 1.3239 val_acc= 0.5967 time= 0.0234\n",
      "Epoch: 0042 train_loss= 1.0773 train_acc= 0.7500 val_loss= 1.3057 val_acc= 0.6000 time= 0.0233\n",
      "Epoch: 0043 train_loss= 1.0527 train_acc= 0.7500 val_loss= 1.2879 val_acc= 0.6133 time= 0.0186\n",
      "Epoch: 0044 train_loss= 1.0324 train_acc= 0.7643 val_loss= 1.2690 val_acc= 0.6200 time= 0.0238\n",
      "Epoch: 0045 train_loss= 1.0091 train_acc= 0.7786 val_loss= 1.2512 val_acc= 0.6267 time= 0.0262\n",
      "Epoch: 0046 train_loss= 0.9877 train_acc= 0.7786 val_loss= 1.2327 val_acc= 0.6333 time= 0.0247\n",
      "Epoch: 0047 train_loss= 0.9659 train_acc= 0.7857 val_loss= 1.2168 val_acc= 0.6433 time= 0.0210\n",
      "Epoch: 0048 train_loss= 0.9444 train_acc= 0.7857 val_loss= 1.2003 val_acc= 0.6533 time= 0.0291\n",
      "Epoch: 0049 train_loss= 0.9212 train_acc= 0.7929 val_loss= 1.1821 val_acc= 0.6567 time= 0.0210\n",
      "Epoch: 0050 train_loss= 0.9012 train_acc= 0.8000 val_loss= 1.1651 val_acc= 0.6600 time= 0.0284\n",
      "Epoch: 0051 train_loss= 0.8803 train_acc= 0.8000 val_loss= 1.1486 val_acc= 0.6700 time= 0.0203\n",
      "Epoch: 0052 train_loss= 0.8598 train_acc= 0.8000 val_loss= 1.1324 val_acc= 0.6767 time= 0.0194\n",
      "Epoch: 0053 train_loss= 0.8396 train_acc= 0.8143 val_loss= 1.1177 val_acc= 0.6833 time= 0.0236\n",
      "Epoch: 0054 train_loss= 0.8189 train_acc= 0.8143 val_loss= 1.1014 val_acc= 0.6833 time= 0.0238\n",
      "Epoch: 0055 train_loss= 0.8022 train_acc= 0.8143 val_loss= 1.0872 val_acc= 0.6833 time= 0.0243\n",
      "Epoch: 0056 train_loss= 0.7834 train_acc= 0.8286 val_loss= 1.0715 val_acc= 0.6867 time= 0.0206\n",
      "Epoch: 0057 train_loss= 0.7654 train_acc= 0.8214 val_loss= 1.0583 val_acc= 0.6867 time= 0.0253\n",
      "Epoch: 0058 train_loss= 0.7455 train_acc= 0.8286 val_loss= 1.0422 val_acc= 0.7100 time= 0.0236\n",
      "Epoch: 0059 train_loss= 0.7283 train_acc= 0.8357 val_loss= 1.0281 val_acc= 0.7067 time= 0.0242\n",
      "Epoch: 0060 train_loss= 0.7099 train_acc= 0.8500 val_loss= 1.0159 val_acc= 0.7100 time= 0.0189\n",
      "Epoch: 0061 train_loss= 0.6924 train_acc= 0.8571 val_loss= 1.0021 val_acc= 0.7167 time= 0.0198\n",
      "Epoch: 0062 train_loss= 0.6744 train_acc= 0.8714 val_loss= 0.9873 val_acc= 0.7233 time= 0.0191\n",
      "Epoch: 0063 train_loss= 0.6571 train_acc= 0.8714 val_loss= 0.9752 val_acc= 0.7333 time= 0.0201\n",
      "Epoch: 0064 train_loss= 0.6419 train_acc= 0.8857 val_loss= 0.9626 val_acc= 0.7233 time= 0.0187\n",
      "Epoch: 0065 train_loss= 0.6243 train_acc= 0.8929 val_loss= 0.9491 val_acc= 0.7333 time= 0.0191\n",
      "Epoch: 0066 train_loss= 0.6101 train_acc= 0.9000 val_loss= 0.9364 val_acc= 0.7300 time= 0.0203\n",
      "Epoch: 0067 train_loss= 0.5982 train_acc= 0.8857 val_loss= 0.9231 val_acc= 0.7400 time= 0.0198\n",
      "Epoch: 0068 train_loss= 0.5778 train_acc= 0.9143 val_loss= 0.9132 val_acc= 0.7467 time= 0.0194\n",
      "Epoch: 0069 train_loss= 0.5642 train_acc= 0.9143 val_loss= 0.9016 val_acc= 0.7467 time= 0.0199\n",
      "Epoch: 0070 train_loss= 0.5488 train_acc= 0.9143 val_loss= 0.8920 val_acc= 0.7400 time= 0.0186\n",
      "Epoch: 0071 train_loss= 0.5356 train_acc= 0.9143 val_loss= 0.8813 val_acc= 0.7400 time= 0.0186\n",
      "Epoch: 0072 train_loss= 0.5201 train_acc= 0.9286 val_loss= 0.8687 val_acc= 0.7400 time= 0.0240\n",
      "Epoch: 0073 train_loss= 0.5062 train_acc= 0.9500 val_loss= 0.8578 val_acc= 0.7533 time= 0.0189\n",
      "Epoch: 0074 train_loss= 0.4935 train_acc= 0.9500 val_loss= 0.8481 val_acc= 0.7733 time= 0.0197\n",
      "Epoch: 0075 train_loss= 0.4808 train_acc= 0.9571 val_loss= 0.8385 val_acc= 0.7800 time= 0.0229\n",
      "Epoch: 0076 train_loss= 0.4698 train_acc= 0.9571 val_loss= 0.8301 val_acc= 0.7800 time= 0.0230\n",
      "Epoch: 0077 train_loss= 0.4576 train_acc= 0.9571 val_loss= 0.8206 val_acc= 0.7867 time= 0.0198\n",
      "Epoch: 0078 train_loss= 0.4421 train_acc= 0.9571 val_loss= 0.8112 val_acc= 0.7900 time= 0.0197\n",
      "Epoch: 0079 train_loss= 0.4314 train_acc= 0.9571 val_loss= 0.8013 val_acc= 0.7967 time= 0.0189\n",
      "Epoch: 0080 train_loss= 0.4155 train_acc= 0.9643 val_loss= 0.7920 val_acc= 0.8033 time= 0.0237\n",
      "Epoch: 0081 train_loss= 0.4080 train_acc= 0.9643 val_loss= 0.7854 val_acc= 0.8033 time= 0.0231\n",
      "Epoch: 0082 train_loss= 0.3979 train_acc= 0.9571 val_loss= 0.7752 val_acc= 0.8067 time= 0.0235\n",
      "Epoch: 0083 train_loss= 0.3838 train_acc= 0.9643 val_loss= 0.7689 val_acc= 0.8133 time= 0.0187\n",
      "Epoch: 0084 train_loss= 0.3756 train_acc= 0.9714 val_loss= 0.7586 val_acc= 0.8133 time= 0.0193\n",
      "Epoch: 0085 train_loss= 0.3637 train_acc= 0.9714 val_loss= 0.7525 val_acc= 0.8167 time= 0.0289\n",
      "Epoch: 0086 train_loss= 0.3533 train_acc= 0.9786 val_loss= 0.7446 val_acc= 0.8167 time= 0.0186\n",
      "Epoch: 0087 train_loss= 0.3424 train_acc= 0.9714 val_loss= 0.7364 val_acc= 0.8233 time= 0.0198\n",
      "Epoch: 0088 train_loss= 0.3334 train_acc= 0.9786 val_loss= 0.7302 val_acc= 0.8167 time= 0.0189\n",
      "Epoch: 0089 train_loss= 0.3226 train_acc= 0.9786 val_loss= 0.7233 val_acc= 0.8167 time= 0.0186\n",
      "Epoch: 0090 train_loss= 0.3130 train_acc= 0.9786 val_loss= 0.7179 val_acc= 0.8200 time= 0.0191\n",
      "Epoch: 0091 train_loss= 0.3040 train_acc= 0.9786 val_loss= 0.7123 val_acc= 0.8233 time= 0.0190\n",
      "Epoch: 0092 train_loss= 0.2965 train_acc= 0.9786 val_loss= 0.7045 val_acc= 0.8267 time= 0.0201\n",
      "Epoch: 0093 train_loss= 0.2878 train_acc= 0.9786 val_loss= 0.7000 val_acc= 0.8267 time= 0.0188\n",
      "Epoch: 0094 train_loss= 0.2797 train_acc= 0.9786 val_loss= 0.6953 val_acc= 0.8233 time= 0.0185\n",
      "Epoch: 0095 train_loss= 0.2709 train_acc= 0.9786 val_loss= 0.6885 val_acc= 0.8233 time= 0.0190\n",
      "Epoch: 0096 train_loss= 0.2657 train_acc= 0.9786 val_loss= 0.6834 val_acc= 0.8300 time= 0.0189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0097 train_loss= 0.2553 train_acc= 0.9786 val_loss= 0.6791 val_acc= 0.8267 time= 0.0235\n",
      "Epoch: 0098 train_loss= 0.2508 train_acc= 0.9786 val_loss= 0.6740 val_acc= 0.8300 time= 0.0204\n",
      "Epoch: 0099 train_loss= 0.2436 train_acc= 0.9786 val_loss= 0.6698 val_acc= 0.8233 time= 0.0281\n",
      "Epoch: 0100 train_loss= 0.2370 train_acc= 0.9786 val_loss= 0.6646 val_acc= 0.8200 time= 0.0186\n",
      "Epoch: 0101 train_loss= 0.2294 train_acc= 0.9786 val_loss= 0.6611 val_acc= 0.8200 time= 0.0183\n",
      "Epoch: 0102 train_loss= 0.2229 train_acc= 0.9786 val_loss= 0.6569 val_acc= 0.8200 time= 0.0189\n",
      "Epoch: 0103 train_loss= 0.2171 train_acc= 0.9786 val_loss= 0.6535 val_acc= 0.8200 time= 0.0184\n",
      "Epoch: 0104 train_loss= 0.2103 train_acc= 0.9786 val_loss= 0.6488 val_acc= 0.8233 time= 0.0227\n",
      "Epoch: 0105 train_loss= 0.2043 train_acc= 0.9786 val_loss= 0.6454 val_acc= 0.8233 time= 0.0194\n",
      "Epoch: 0106 train_loss= 0.2014 train_acc= 0.9786 val_loss= 0.6425 val_acc= 0.8233 time= 0.0194\n",
      "Epoch: 0107 train_loss= 0.1938 train_acc= 0.9857 val_loss= 0.6407 val_acc= 0.8233 time= 0.0239\n",
      "Epoch: 0108 train_loss= 0.1897 train_acc= 0.9786 val_loss= 0.6368 val_acc= 0.8200 time= 0.0212\n",
      "Epoch: 0109 train_loss= 0.1847 train_acc= 0.9857 val_loss= 0.6340 val_acc= 0.8200 time= 0.0196\n",
      "Epoch: 0110 train_loss= 0.1812 train_acc= 0.9857 val_loss= 0.6296 val_acc= 0.8267 time= 0.0237\n",
      "Epoch: 0111 train_loss= 0.1747 train_acc= 0.9857 val_loss= 0.6279 val_acc= 0.8267 time= 0.0188\n",
      "Epoch: 0112 train_loss= 0.1699 train_acc= 0.9857 val_loss= 0.6250 val_acc= 0.8267 time= 0.0198\n",
      "Epoch: 0113 train_loss= 0.1670 train_acc= 0.9857 val_loss= 0.6225 val_acc= 0.8267 time= 0.0209\n",
      "Epoch: 0114 train_loss= 0.1633 train_acc= 0.9857 val_loss= 0.6195 val_acc= 0.8267 time= 0.0233\n",
      "Epoch: 0115 train_loss= 0.1580 train_acc= 0.9857 val_loss= 0.6174 val_acc= 0.8267 time= 0.0185\n",
      "Epoch: 0116 train_loss= 0.1528 train_acc= 0.9857 val_loss= 0.6150 val_acc= 0.8267 time= 0.0194\n",
      "Epoch: 0117 train_loss= 0.1500 train_acc= 0.9857 val_loss= 0.6131 val_acc= 0.8267 time= 0.0253\n",
      "Epoch: 0118 train_loss= 0.1458 train_acc= 0.9857 val_loss= 0.6096 val_acc= 0.8267 time= 0.0235\n",
      "Epoch: 0119 train_loss= 0.1428 train_acc= 0.9857 val_loss= 0.6075 val_acc= 0.8267 time= 0.0252\n",
      "Epoch: 0120 train_loss= 0.1392 train_acc= 0.9857 val_loss= 0.6058 val_acc= 0.8267 time= 0.0295\n",
      "Epoch: 0121 train_loss= 0.1363 train_acc= 0.9857 val_loss= 0.6039 val_acc= 0.8267 time= 0.0200\n",
      "Epoch: 0122 train_loss= 0.1332 train_acc= 0.9857 val_loss= 0.6025 val_acc= 0.8267 time= 0.0196\n",
      "Epoch: 0123 train_loss= 0.1301 train_acc= 0.9857 val_loss= 0.6017 val_acc= 0.8267 time= 0.0191\n",
      "Epoch: 0124 train_loss= 0.1275 train_acc= 0.9857 val_loss= 0.6011 val_acc= 0.8267 time= 0.0238\n",
      "Epoch: 0125 train_loss= 0.1247 train_acc= 0.9857 val_loss= 0.5991 val_acc= 0.8267 time= 0.0191\n",
      "Epoch: 0126 train_loss= 0.1209 train_acc= 0.9857 val_loss= 0.5988 val_acc= 0.8267 time= 0.0190\n",
      "Epoch: 0127 train_loss= 0.1184 train_acc= 0.9857 val_loss= 0.5973 val_acc= 0.8267 time= 0.0196\n",
      "Epoch: 0128 train_loss= 0.1156 train_acc= 0.9857 val_loss= 0.5981 val_acc= 0.8267 time= 0.0187\n",
      "Epoch: 0129 train_loss= 0.1145 train_acc= 0.9929 val_loss= 0.5967 val_acc= 0.8267 time= 0.0200\n",
      "Epoch: 0130 train_loss= 0.1109 train_acc= 0.9929 val_loss= 0.5948 val_acc= 0.8267 time= 0.0190\n",
      "Epoch: 0131 train_loss= 0.1091 train_acc= 0.9929 val_loss= 0.5940 val_acc= 0.8267 time= 0.0191\n",
      "Epoch: 0132 train_loss= 0.1065 train_acc= 0.9929 val_loss= 0.5947 val_acc= 0.8267 time= 0.0233\n",
      "Epoch: 0133 train_loss= 0.1039 train_acc= 0.9929 val_loss= 0.5932 val_acc= 0.8267 time= 0.0198\n",
      "Epoch: 0134 train_loss= 0.1024 train_acc= 0.9929 val_loss= 0.5920 val_acc= 0.8267 time= 0.0192\n",
      "Epoch: 0135 train_loss= 0.1003 train_acc= 0.9929 val_loss= 0.5908 val_acc= 0.8267 time= 0.0190\n",
      "Epoch: 0136 train_loss= 0.0978 train_acc= 0.9929 val_loss= 0.5897 val_acc= 0.8267 time= 0.0198\n",
      "Epoch: 0137 train_loss= 0.0962 train_acc= 0.9929 val_loss= 0.5889 val_acc= 0.8267 time= 0.0194\n",
      "Epoch: 0138 train_loss= 0.0941 train_acc= 0.9929 val_loss= 0.5880 val_acc= 0.8267 time= 0.0194\n",
      "Epoch: 0139 train_loss= 0.0917 train_acc= 0.9929 val_loss= 0.5868 val_acc= 0.8267 time= 0.0192\n",
      "Epoch: 0140 train_loss= 0.0909 train_acc= 0.9929 val_loss= 0.5860 val_acc= 0.8300 time= 0.0196\n",
      "Epoch: 0141 train_loss= 0.0887 train_acc= 0.9929 val_loss= 0.5849 val_acc= 0.8300 time= 0.0193\n",
      "Epoch: 0142 train_loss= 0.0866 train_acc= 0.9929 val_loss= 0.5837 val_acc= 0.8300 time= 0.0199\n",
      "Epoch: 0143 train_loss= 0.0858 train_acc= 0.9929 val_loss= 0.5828 val_acc= 0.8300 time= 0.0202\n",
      "Epoch: 0144 train_loss= 0.0841 train_acc= 0.9929 val_loss= 0.5823 val_acc= 0.8333 time= 0.0234\n",
      "Epoch: 0145 train_loss= 0.0820 train_acc= 1.0000 val_loss= 0.5829 val_acc= 0.8333 time= 0.0238\n",
      "Epoch: 0146 train_loss= 0.0807 train_acc= 1.0000 val_loss= 0.5827 val_acc= 0.8333 time= 0.0231\n",
      "Epoch: 0147 train_loss= 0.0788 train_acc= 1.0000 val_loss= 0.5816 val_acc= 0.8333 time= 0.0234\n",
      "Epoch: 0148 train_loss= 0.0775 train_acc= 1.0000 val_loss= 0.5820 val_acc= 0.8333 time= 0.0195\n",
      "Epoch: 0149 train_loss= 0.0764 train_acc= 1.0000 val_loss= 0.5817 val_acc= 0.8333 time= 0.0230\n",
      "Epoch: 0150 train_loss= 0.0749 train_acc= 1.0000 val_loss= 0.5806 val_acc= 0.8333 time= 0.0186\n",
      "Epoch: 0151 train_loss= 0.0729 train_acc= 1.0000 val_loss= 0.5812 val_acc= 0.8333 time= 0.0189\n",
      "Epoch: 0152 train_loss= 0.0722 train_acc= 1.0000 val_loss= 0.5801 val_acc= 0.8333 time= 0.0184\n",
      "Epoch: 0153 train_loss= 0.0707 train_acc= 1.0000 val_loss= 0.5812 val_acc= 0.8333 time= 0.0193\n",
      "Epoch: 0154 train_loss= 0.0696 train_acc= 1.0000 val_loss= 0.5802 val_acc= 0.8333 time= 0.0183\n",
      "Epoch: 0155 train_loss= 0.0688 train_acc= 1.0000 val_loss= 0.5813 val_acc= 0.8333 time= 0.0200\n",
      "Epoch: 0156 train_loss= 0.0668 train_acc= 1.0000 val_loss= 0.5805 val_acc= 0.8333 time= 0.0198\n",
      "Epoch: 0157 train_loss= 0.0660 train_acc= 1.0000 val_loss= 0.5810 val_acc= 0.8333 time= 0.0192\n",
      "Epoch: 0158 train_loss= 0.0651 train_acc= 1.0000 val_loss= 0.5800 val_acc= 0.8333 time= 0.0187\n",
      "Epoch: 0159 train_loss= 0.0634 train_acc= 1.0000 val_loss= 0.5813 val_acc= 0.8333 time= 0.0231\n",
      "Epoch: 0160 train_loss= 0.0628 train_acc= 1.0000 val_loss= 0.5814 val_acc= 0.8333 time= 0.0285\n",
      "Epoch: 0161 train_loss= 0.0618 train_acc= 1.0000 val_loss= 0.5808 val_acc= 0.8333 time= 0.0254\n",
      "Epoch: 0162 train_loss= 0.0610 train_acc= 1.0000 val_loss= 0.5793 val_acc= 0.8333 time= 0.0259\n",
      "Epoch: 0163 train_loss= 0.0599 train_acc= 1.0000 val_loss= 0.5821 val_acc= 0.8333 time= 0.0213\n",
      "Epoch: 0164 train_loss= 0.0588 train_acc= 1.0000 val_loss= 0.5814 val_acc= 0.8333 time= 0.0215\n",
      "Epoch: 0165 train_loss= 0.0578 train_acc= 1.0000 val_loss= 0.5816 val_acc= 0.8333 time= 0.0251\n",
      "Epoch: 0166 train_loss= 0.0566 train_acc= 1.0000 val_loss= 0.5827 val_acc= 0.8333 time= 0.0249\n",
      "Epoch: 0167 train_loss= 0.0561 train_acc= 1.0000 val_loss= 0.5836 val_acc= 0.8333 time= 0.0201\n",
      "Epoch: 0168 train_loss= 0.0556 train_acc= 1.0000 val_loss= 0.5826 val_acc= 0.8333 time= 0.0251\n",
      "Epoch: 0169 train_loss= 0.0541 train_acc= 1.0000 val_loss= 0.5826 val_acc= 0.8333 time= 0.0202\n",
      "Epoch: 0170 train_loss= 0.0535 train_acc= 1.0000 val_loss= 0.5822 val_acc= 0.8300 time= 0.0189\n",
      "Epoch: 0171 train_loss= 0.0525 train_acc= 1.0000 val_loss= 0.5831 val_acc= 0.8300 time= 0.0189\n",
      "Epoch: 0172 train_loss= 0.0516 train_acc= 1.0000 val_loss= 0.5832 val_acc= 0.8300 time= 0.0186\n",
      "Epoch: 0173 train_loss= 0.0513 train_acc= 1.0000 val_loss= 0.5827 val_acc= 0.8300 time= 0.0233\n",
      "Epoch 173: early stopping\n"
     ]
    }
   ],
   "source": [
    "NB_EPOCH = 200\n",
    "PATIENCE = 10  # early stopping patience\n",
    "# Helper variables for main training loop\n",
    "wait = 0\n",
    "preds = None\n",
    "best_val_loss = 99999\n",
    "\n",
    "# Fit\n",
    "for epoch in range(1, NB_EPOCH+1):\n",
    "\n",
    "    # Log wall-clock time\n",
    "    t = time.time()\n",
    "\n",
    "    # Single training iteration (we mask nodes without labels for loss calculation)\n",
    "    model.fit(graph, y_train, sample_weight=train_mask,\n",
    "              batch_size=A.shape[0], epochs=1, shuffle=False, verbose=0)\n",
    "\n",
    "    # Predict on full dataset\n",
    "    preds = model.predict(graph, batch_size=A.shape[0])\n",
    "\n",
    "    # Train / validation scores\n",
    "    train_val_loss, train_val_acc = evaluate_preds(preds, [y_train, y_val],\n",
    "                                                   [idx_train, idx_val])\n",
    "    print(\"Epoch: {:04d}\".format(epoch),\n",
    "          \"train_loss= {:.4f}\".format(train_val_loss[0]),\n",
    "          \"train_acc= {:.4f}\".format(train_val_acc[0]),\n",
    "          \"val_loss= {:.4f}\".format(train_val_loss[1]),\n",
    "          \"val_acc= {:.4f}\".format(train_val_acc[1]),\n",
    "          \"time= {:.4f}\".format(time.time() - t))\n",
    "\n",
    "    # Early stopping\n",
    "    if train_val_loss[1] < best_val_loss:\n",
    "        best_val_loss = train_val_loss[1]\n",
    "        wait = 0\n",
    "    else:\n",
    "        if wait >= PATIENCE:\n",
    "            print('Epoch {}: early stopping'.format(epoch))\n",
    "            break\n",
    "        wait += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.6229\n",
      "accuracy = 0.8030\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate_preds(preds, [y_test], [idx_test])\n",
    "print(f\"\"\"\n",
    "loss = {test_loss[0]:.4f}\n",
    "accuracy = {test_acc[0]:.4f}\n",
    "\"\"\".strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
