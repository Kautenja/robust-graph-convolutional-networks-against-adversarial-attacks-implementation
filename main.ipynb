{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the path of execution\n",
    "EXE_PATH = os.path.abspath(os.path.curdir)\n",
    "# the path of the vendor files\n",
    "VENDOR_PATH = os.path.join(EXE_PATH, 'vendor')\n",
    "# the vendors to include in the system path\n",
    "VENDORS = ['keras-gcn']\n",
    "# create the absolute paths for all vendors\n",
    "VENDORS = list(map(lambda x: os.path.join(VENDOR_PATH, x), VENDORS))\n",
    "# update the Python path to include necessary vendor module\n",
    "sys.path += VENDORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kegra.layers.graph import GraphConvolution\n",
    "from kegra.utils import load_data, preprocess_adj, get_splits, evaluate_preds\n",
    "from ggcn import GaussianGraphConvolution, kl_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "Dataset has 2708 nodes, 5429 edges, 1433 features.\n"
     ]
    }
   ],
   "source": [
    "X, A, y = load_data('data/cora/', dataset='cora')\n",
    "X /= X.sum(1).reshape(-1, 1)\n",
    "A = preprocess_adj(A)\n",
    "y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask = get_splits(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2708x2708 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 13264 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1110 18:17:57.215714 139680131151680 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1110 18:17:57.222760 139680131151680 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:515: The name tf.sparse_placeholder is deprecated. Please use tf.compat.v1.sparse_placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "support = 1\n",
    "graph = [X, A]\n",
    "G = [Input(shape=(None, None), batch_shape=(None, None), sparse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 18:17:57.227551 139680131151680 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_in = Input(shape=(X.shape[1],))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 18:17:57.232685 139680131151680 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1110 18:17:57.236961 139680131151680 deprecation.py:506] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1110 18:17:57.245815 139680131151680 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define model architecture\n",
    "# NOTE: We pass arguments for graph convolutional layers as a list of tensors.\n",
    "# This is somewhat hacky, more elegant options would require rewriting the Layer base class.\n",
    "H = Dropout(0.5)(X_in)\n",
    "H = GraphConvolution(32, support, activation='relu', kernel_regularizer=l2(5e-4))([H]+G)\n",
    "H = Dropout(0.5)(H)\n",
    "Y = GraphConvolution(y.shape[1], support, activation='softmax')([H]+G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 18:17:57.286554 139680131151680 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[X_in]+G, outputs=Y)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 1433)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1433)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_1 (GraphConvo (None, 32)           45888       dropout_1[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32)           0           graph_convolution_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_2 (GraphConvo (None, 7)            231         dropout_2[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 46,119\n",
      "Trainable params: 46,119\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 18:17:57.359720 139680131151680 deprecation.py:323] From /home/bitcommander/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.9277 train_acc= 0.3071 val_loss= 1.9293 val_acc= 0.3500 time= 1.0580\n",
      "Epoch: 0002 train_loss= 1.9079 train_acc= 0.3000 val_loss= 1.9119 val_acc= 0.3500 time= 0.0170\n",
      "Epoch: 0003 train_loss= 1.8853 train_acc= 0.2929 val_loss= 1.8922 val_acc= 0.3500 time= 0.0164\n",
      "Epoch: 0004 train_loss= 1.8617 train_acc= 0.2929 val_loss= 1.8709 val_acc= 0.3500 time= 0.0164\n",
      "Epoch: 0005 train_loss= 1.8386 train_acc= 0.2929 val_loss= 1.8499 val_acc= 0.3500 time= 0.0166\n",
      "Epoch: 0006 train_loss= 1.8157 train_acc= 0.2929 val_loss= 1.8292 val_acc= 0.3500 time= 0.0166\n",
      "Epoch: 0007 train_loss= 1.7932 train_acc= 0.2929 val_loss= 1.8096 val_acc= 0.3500 time= 0.0170\n",
      "Epoch: 0008 train_loss= 1.7720 train_acc= 0.2929 val_loss= 1.7916 val_acc= 0.3500 time= 0.0172\n",
      "Epoch: 0009 train_loss= 1.7522 train_acc= 0.2929 val_loss= 1.7754 val_acc= 0.3500 time= 0.0170\n",
      "Epoch: 0010 train_loss= 1.7343 train_acc= 0.2929 val_loss= 1.7616 val_acc= 0.3500 time= 0.0174\n",
      "Epoch: 0011 train_loss= 1.7183 train_acc= 0.2929 val_loss= 1.7501 val_acc= 0.3500 time= 0.0171\n",
      "Epoch: 0012 train_loss= 1.7038 train_acc= 0.2929 val_loss= 1.7406 val_acc= 0.3500 time= 0.0178\n",
      "Epoch: 0013 train_loss= 1.6902 train_acc= 0.3143 val_loss= 1.7322 val_acc= 0.3533 time= 0.0211\n",
      "Epoch: 0014 train_loss= 1.6769 train_acc= 0.3571 val_loss= 1.7246 val_acc= 0.3633 time= 0.0180\n",
      "Epoch: 0015 train_loss= 1.6634 train_acc= 0.3714 val_loss= 1.7168 val_acc= 0.3633 time= 0.0199\n",
      "Epoch: 0016 train_loss= 1.6491 train_acc= 0.4071 val_loss= 1.7084 val_acc= 0.3667 time= 0.0177\n",
      "Epoch: 0017 train_loss= 1.6340 train_acc= 0.4214 val_loss= 1.6995 val_acc= 0.3867 time= 0.0168\n",
      "Epoch: 0018 train_loss= 1.6182 train_acc= 0.4286 val_loss= 1.6901 val_acc= 0.4067 time= 0.0176\n",
      "Epoch: 0019 train_loss= 1.6019 train_acc= 0.4643 val_loss= 1.6802 val_acc= 0.4200 time= 0.0172\n",
      "Epoch: 0020 train_loss= 1.5854 train_acc= 0.4714 val_loss= 1.6698 val_acc= 0.4367 time= 0.0169\n",
      "Epoch: 0021 train_loss= 1.5689 train_acc= 0.4714 val_loss= 1.6590 val_acc= 0.4533 time= 0.0167\n",
      "Epoch: 0022 train_loss= 1.5527 train_acc= 0.4857 val_loss= 1.6479 val_acc= 0.4600 time= 0.0164\n",
      "Epoch: 0023 train_loss= 1.5366 train_acc= 0.4929 val_loss= 1.6365 val_acc= 0.4633 time= 0.0166\n",
      "Epoch: 0024 train_loss= 1.5207 train_acc= 0.5000 val_loss= 1.6249 val_acc= 0.4700 time= 0.0166\n",
      "Epoch: 0025 train_loss= 1.5046 train_acc= 0.5071 val_loss= 1.6127 val_acc= 0.4633 time= 0.0173\n",
      "Epoch: 0026 train_loss= 1.4885 train_acc= 0.5071 val_loss= 1.6002 val_acc= 0.4567 time= 0.0195\n",
      "Epoch: 0027 train_loss= 1.4723 train_acc= 0.5071 val_loss= 1.5878 val_acc= 0.4567 time= 0.0177\n",
      "Epoch: 0028 train_loss= 1.4559 train_acc= 0.5143 val_loss= 1.5755 val_acc= 0.4567 time= 0.0172\n",
      "Epoch: 0029 train_loss= 1.4392 train_acc= 0.5214 val_loss= 1.5634 val_acc= 0.4700 time= 0.0168\n",
      "Epoch: 0030 train_loss= 1.4226 train_acc= 0.5214 val_loss= 1.5517 val_acc= 0.4833 time= 0.0175\n",
      "Epoch: 0031 train_loss= 1.4060 train_acc= 0.5357 val_loss= 1.5402 val_acc= 0.4833 time= 0.0164\n",
      "Epoch: 0032 train_loss= 1.3896 train_acc= 0.5429 val_loss= 1.5288 val_acc= 0.5000 time= 0.0165\n",
      "Epoch: 0033 train_loss= 1.3733 train_acc= 0.5429 val_loss= 1.5174 val_acc= 0.5033 time= 0.0165\n",
      "Epoch: 0034 train_loss= 1.3571 train_acc= 0.5429 val_loss= 1.5057 val_acc= 0.5100 time= 0.0168\n",
      "Epoch: 0035 train_loss= 1.3408 train_acc= 0.5500 val_loss= 1.4940 val_acc= 0.5267 time= 0.0164\n",
      "Epoch: 0036 train_loss= 1.3247 train_acc= 0.5643 val_loss= 1.4824 val_acc= 0.5400 time= 0.0165\n",
      "Epoch: 0037 train_loss= 1.3087 train_acc= 0.5857 val_loss= 1.4711 val_acc= 0.5433 time= 0.0179\n",
      "Epoch: 0038 train_loss= 1.2928 train_acc= 0.6071 val_loss= 1.4598 val_acc= 0.5467 time= 0.0167\n",
      "Epoch: 0039 train_loss= 1.2771 train_acc= 0.6357 val_loss= 1.4482 val_acc= 0.5567 time= 0.0169\n",
      "Epoch: 0040 train_loss= 1.2616 train_acc= 0.6429 val_loss= 1.4366 val_acc= 0.5600 time= 0.0168\n",
      "Epoch: 0041 train_loss= 1.2460 train_acc= 0.6500 val_loss= 1.4244 val_acc= 0.5667 time= 0.0177\n",
      "Epoch: 0042 train_loss= 1.2305 train_acc= 0.6571 val_loss= 1.4120 val_acc= 0.5833 time= 0.0169\n",
      "Epoch: 0043 train_loss= 1.2151 train_acc= 0.6714 val_loss= 1.4000 val_acc= 0.5900 time= 0.0177\n",
      "Epoch: 0044 train_loss= 1.1999 train_acc= 0.6714 val_loss= 1.3885 val_acc= 0.5967 time= 0.0171\n",
      "Epoch: 0045 train_loss= 1.1849 train_acc= 0.6714 val_loss= 1.3769 val_acc= 0.6000 time= 0.0178\n",
      "Epoch: 0046 train_loss= 1.1703 train_acc= 0.6714 val_loss= 1.3655 val_acc= 0.5967 time= 0.0167\n",
      "Epoch: 0047 train_loss= 1.1556 train_acc= 0.6786 val_loss= 1.3543 val_acc= 0.5967 time= 0.0167\n",
      "Epoch: 0048 train_loss= 1.1409 train_acc= 0.6857 val_loss= 1.3438 val_acc= 0.6033 time= 0.0169\n",
      "Epoch: 0049 train_loss= 1.1264 train_acc= 0.7143 val_loss= 1.3336 val_acc= 0.6067 time= 0.0176\n",
      "Epoch: 0050 train_loss= 1.1121 train_acc= 0.7286 val_loss= 1.3233 val_acc= 0.6133 time= 0.0167\n",
      "Epoch: 0051 train_loss= 1.0979 train_acc= 0.7357 val_loss= 1.3133 val_acc= 0.6233 time= 0.0168\n",
      "Epoch: 0052 train_loss= 1.0838 train_acc= 0.7429 val_loss= 1.3029 val_acc= 0.6333 time= 0.0165\n",
      "Epoch: 0053 train_loss= 1.0698 train_acc= 0.7500 val_loss= 1.2923 val_acc= 0.6467 time= 0.0169\n",
      "Epoch: 0054 train_loss= 1.0561 train_acc= 0.7571 val_loss= 1.2811 val_acc= 0.6533 time= 0.0163\n",
      "Epoch: 0055 train_loss= 1.0424 train_acc= 0.7714 val_loss= 1.2698 val_acc= 0.6667 time= 0.0164\n",
      "Epoch: 0056 train_loss= 1.0290 train_acc= 0.7929 val_loss= 1.2590 val_acc= 0.6800 time= 0.0170\n",
      "Epoch: 0057 train_loss= 1.0158 train_acc= 0.8000 val_loss= 1.2481 val_acc= 0.6833 time= 0.0171\n",
      "Epoch: 0058 train_loss= 1.0026 train_acc= 0.8000 val_loss= 1.2375 val_acc= 0.6867 time= 0.0159\n",
      "Epoch: 0059 train_loss= 0.9896 train_acc= 0.8071 val_loss= 1.2272 val_acc= 0.7033 time= 0.0162\n",
      "Epoch: 0060 train_loss= 0.9768 train_acc= 0.8286 val_loss= 1.2168 val_acc= 0.7100 time= 0.0168\n",
      "Epoch: 0061 train_loss= 0.9640 train_acc= 0.8357 val_loss= 1.2063 val_acc= 0.7100 time= 0.0159\n",
      "Epoch: 0062 train_loss= 0.9516 train_acc= 0.8357 val_loss= 1.1964 val_acc= 0.7133 time= 0.0166\n",
      "Epoch: 0063 train_loss= 0.9393 train_acc= 0.8429 val_loss= 1.1872 val_acc= 0.7167 time= 0.0163\n",
      "Epoch: 0064 train_loss= 0.9279 train_acc= 0.8571 val_loss= 1.1787 val_acc= 0.7200 time= 0.0161\n",
      "Epoch: 0065 train_loss= 0.9167 train_acc= 0.8571 val_loss= 1.1708 val_acc= 0.7300 time= 0.0173\n",
      "Epoch: 0066 train_loss= 0.9049 train_acc= 0.8571 val_loss= 1.1614 val_acc= 0.7300 time= 0.0168\n",
      "Epoch: 0067 train_loss= 0.8924 train_acc= 0.8571 val_loss= 1.1504 val_acc= 0.7433 time= 0.0161\n",
      "Epoch: 0068 train_loss= 0.8799 train_acc= 0.8571 val_loss= 1.1394 val_acc= 0.7533 time= 0.0164\n",
      "Epoch: 0069 train_loss= 0.8681 train_acc= 0.8714 val_loss= 1.1286 val_acc= 0.7667 time= 0.0178\n",
      "Epoch: 0070 train_loss= 0.8572 train_acc= 0.8714 val_loss= 1.1188 val_acc= 0.7667 time= 0.0165\n",
      "Epoch: 0071 train_loss= 0.8468 train_acc= 0.8643 val_loss= 1.1103 val_acc= 0.7633 time= 0.0167\n",
      "Epoch: 0072 train_loss= 0.8363 train_acc= 0.8714 val_loss= 1.1021 val_acc= 0.7667 time= 0.0166\n",
      "Epoch: 0073 train_loss= 0.8261 train_acc= 0.8786 val_loss= 1.0940 val_acc= 0.7700 time= 0.0165\n",
      "Epoch: 0074 train_loss= 0.8159 train_acc= 0.8714 val_loss= 1.0866 val_acc= 0.7700 time= 0.0173\n",
      "Epoch: 0075 train_loss= 0.8063 train_acc= 0.8714 val_loss= 1.0803 val_acc= 0.7700 time= 0.0164\n",
      "Epoch: 0076 train_loss= 0.7971 train_acc= 0.8786 val_loss= 1.0735 val_acc= 0.7700 time= 0.0167\n",
      "Epoch: 0077 train_loss= 0.7880 train_acc= 0.8714 val_loss= 1.0654 val_acc= 0.7667 time= 0.0166\n",
      "Epoch: 0078 train_loss= 0.7789 train_acc= 0.8786 val_loss= 1.0573 val_acc= 0.7633 time= 0.0167\n",
      "Epoch: 0079 train_loss= 0.7699 train_acc= 0.8786 val_loss= 1.0492 val_acc= 0.7700 time= 0.0166\n",
      "Epoch: 0080 train_loss= 0.7611 train_acc= 0.8857 val_loss= 1.0418 val_acc= 0.7700 time= 0.0171\n",
      "Epoch: 0081 train_loss= 0.7522 train_acc= 0.8857 val_loss= 1.0345 val_acc= 0.7667 time= 0.0170\n",
      "Epoch: 0082 train_loss= 0.7441 train_acc= 0.8857 val_loss= 1.0278 val_acc= 0.7733 time= 0.0167\n",
      "Epoch: 0083 train_loss= 0.7363 train_acc= 0.8857 val_loss= 1.0217 val_acc= 0.7733 time= 0.0165\n",
      "Epoch: 0084 train_loss= 0.7285 train_acc= 0.9143 val_loss= 1.0166 val_acc= 0.7800 time= 0.0170\n",
      "Epoch: 0085 train_loss= 0.7205 train_acc= 0.9000 val_loss= 1.0120 val_acc= 0.7833 time= 0.0180\n",
      "Epoch: 0086 train_loss= 0.7129 train_acc= 0.9000 val_loss= 1.0076 val_acc= 0.7900 time= 0.0176\n",
      "Epoch: 0087 train_loss= 0.7053 train_acc= 0.9071 val_loss= 1.0013 val_acc= 0.7933 time= 0.0168\n",
      "Epoch: 0088 train_loss= 0.6978 train_acc= 0.9071 val_loss= 0.9952 val_acc= 0.7933 time= 0.0185\n",
      "Epoch: 0089 train_loss= 0.6903 train_acc= 0.9214 val_loss= 0.9883 val_acc= 0.7900 time= 0.0173\n",
      "Epoch: 0090 train_loss= 0.6833 train_acc= 0.9214 val_loss= 0.9820 val_acc= 0.7900 time= 0.0165\n",
      "Epoch: 0091 train_loss= 0.6769 train_acc= 0.9214 val_loss= 0.9766 val_acc= 0.7933 time= 0.0170\n",
      "Epoch: 0092 train_loss= 0.6705 train_acc= 0.9143 val_loss= 0.9730 val_acc= 0.7933 time= 0.0161\n",
      "Epoch: 0093 train_loss= 0.6643 train_acc= 0.9143 val_loss= 0.9689 val_acc= 0.7933 time= 0.0170\n",
      "Epoch: 0094 train_loss= 0.6582 train_acc= 0.9143 val_loss= 0.9646 val_acc= 0.7933 time= 0.0166\n",
      "Epoch: 0095 train_loss= 0.6523 train_acc= 0.9143 val_loss= 0.9604 val_acc= 0.7933 time= 0.0163\n",
      "Epoch: 0096 train_loss= 0.6462 train_acc= 0.9214 val_loss= 0.9559 val_acc= 0.7933 time= 0.0167\n",
      "Epoch: 0097 train_loss= 0.6402 train_acc= 0.9286 val_loss= 0.9500 val_acc= 0.7933 time= 0.0168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0098 train_loss= 0.6350 train_acc= 0.9286 val_loss= 0.9445 val_acc= 0.7933 time= 0.0180\n",
      "Epoch: 0099 train_loss= 0.6315 train_acc= 0.9214 val_loss= 0.9394 val_acc= 0.7967 time= 0.0163\n",
      "Epoch: 0100 train_loss= 0.6271 train_acc= 0.9214 val_loss= 0.9355 val_acc= 0.7967 time= 0.0170\n",
      "Epoch: 0101 train_loss= 0.6220 train_acc= 0.9214 val_loss= 0.9317 val_acc= 0.7967 time= 0.0167\n",
      "Epoch: 0102 train_loss= 0.6149 train_acc= 0.9286 val_loss= 0.9280 val_acc= 0.7967 time= 0.0169\n",
      "Epoch: 0103 train_loss= 0.6085 train_acc= 0.9286 val_loss= 0.9255 val_acc= 0.7933 time= 0.0166\n",
      "Epoch: 0104 train_loss= 0.6034 train_acc= 0.9214 val_loss= 0.9241 val_acc= 0.7933 time= 0.0172\n",
      "Epoch: 0105 train_loss= 0.5990 train_acc= 0.9286 val_loss= 0.9219 val_acc= 0.7933 time= 0.0174\n",
      "Epoch: 0106 train_loss= 0.5942 train_acc= 0.9357 val_loss= 0.9191 val_acc= 0.7933 time= 0.0165\n",
      "Epoch: 0107 train_loss= 0.5890 train_acc= 0.9286 val_loss= 0.9155 val_acc= 0.7967 time= 0.0172\n",
      "Epoch: 0108 train_loss= 0.5841 train_acc= 0.9357 val_loss= 0.9129 val_acc= 0.8000 time= 0.0167\n",
      "Epoch: 0109 train_loss= 0.5789 train_acc= 0.9357 val_loss= 0.9084 val_acc= 0.8000 time= 0.0170\n",
      "Epoch: 0110 train_loss= 0.5738 train_acc= 0.9429 val_loss= 0.9028 val_acc= 0.8000 time= 0.0170\n",
      "Epoch: 0111 train_loss= 0.5695 train_acc= 0.9429 val_loss= 0.8970 val_acc= 0.8000 time= 0.0172\n",
      "Epoch: 0112 train_loss= 0.5656 train_acc= 0.9429 val_loss= 0.8919 val_acc= 0.8000 time= 0.0171\n",
      "Epoch: 0113 train_loss= 0.5621 train_acc= 0.9429 val_loss= 0.8873 val_acc= 0.7967 time= 0.0166\n",
      "Epoch: 0114 train_loss= 0.5581 train_acc= 0.9429 val_loss= 0.8828 val_acc= 0.7933 time= 0.0169\n",
      "Epoch: 0115 train_loss= 0.5527 train_acc= 0.9429 val_loss= 0.8789 val_acc= 0.7967 time= 0.0168\n",
      "Epoch: 0116 train_loss= 0.5474 train_acc= 0.9429 val_loss= 0.8765 val_acc= 0.8000 time= 0.0167\n",
      "Epoch: 0117 train_loss= 0.5434 train_acc= 0.9429 val_loss= 0.8748 val_acc= 0.8000 time= 0.0162\n",
      "Epoch: 0118 train_loss= 0.5404 train_acc= 0.9429 val_loss= 0.8743 val_acc= 0.8067 time= 0.0166\n",
      "Epoch: 0119 train_loss= 0.5373 train_acc= 0.9429 val_loss= 0.8732 val_acc= 0.8133 time= 0.0168\n",
      "Epoch: 0120 train_loss= 0.5339 train_acc= 0.9429 val_loss= 0.8713 val_acc= 0.8100 time= 0.0161\n",
      "Epoch: 0121 train_loss= 0.5297 train_acc= 0.9429 val_loss= 0.8681 val_acc= 0.8067 time= 0.0162\n",
      "Epoch: 0122 train_loss= 0.5256 train_acc= 0.9429 val_loss= 0.8646 val_acc= 0.8100 time= 0.0163\n"
     ]
    }
   ],
   "source": [
    "NB_EPOCH = 200\n",
    "PATIENCE = 10  # early stopping patience\n",
    "# Helper variables for main training loop\n",
    "wait = 0\n",
    "preds = None\n",
    "best_val_loss = 99999\n",
    "\n",
    "# Fit\n",
    "for epoch in range(1, NB_EPOCH+1):\n",
    "\n",
    "    # Log wall-clock time\n",
    "    t = time.time()\n",
    "\n",
    "    # Single training iteration (we mask nodes without labels for loss calculation)\n",
    "    model.fit(graph, y_train, sample_weight=train_mask,\n",
    "              batch_size=A.shape[0], epochs=1, shuffle=False, verbose=0)\n",
    "\n",
    "    # Predict on full dataset\n",
    "    preds = model.predict(graph, batch_size=A.shape[0])\n",
    "\n",
    "    # Train / validation scores\n",
    "    train_val_loss, train_val_acc = evaluate_preds(preds, [y_train, y_val],\n",
    "                                                   [idx_train, idx_val])\n",
    "    print(\"Epoch: {:04d}\".format(epoch),\n",
    "          \"train_loss= {:.4f}\".format(train_val_loss[0]),\n",
    "          \"train_acc= {:.4f}\".format(train_val_acc[0]),\n",
    "          \"val_loss= {:.4f}\".format(train_val_loss[1]),\n",
    "          \"val_acc= {:.4f}\".format(train_val_acc[1]),\n",
    "          \"time= {:.4f}\".format(time.time() - t))\n",
    "\n",
    "    # Early stopping\n",
    "    if train_val_loss[1] < best_val_loss:\n",
    "        best_val_loss = train_val_loss[1]\n",
    "        wait = 0\n",
    "    else:\n",
    "        if wait >= PATIENCE:\n",
    "            print('Epoch {}: early stopping'.format(epoch))\n",
    "            break\n",
    "        wait += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate_preds(preds, [y_test], [idx_test])\n",
    "print(f\"\"\"\n",
    "loss = {test_loss[0]:.4f}\n",
    "accuracy = {test_acc[0]:.4f}\n",
    "\"\"\".strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Graph Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B1 = 5e-4\n",
    "B2 = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "H = Dropout(0.5)(X_in)\n",
    "H1 = GaussianGraphConvolution(16,\n",
    "    is_first=True,\n",
    "    activation='relu',\n",
    "    mean_regularizer=l2(B1),\n",
    "    variance_regularizer=l2(B1),\n",
    "#     dropout=0.5\n",
    ")([H]+G)\n",
    "Y = GaussianGraphConvolution(y.shape[1],\n",
    "    is_last=True,\n",
    "    activation='softmax',\n",
    ")(H1+G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[X_in]+G, outputs=Y)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01))\n",
    "model.add_loss(kl_reg(*H1, B2), H1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_EPOCH = 200\n",
    "PATIENCE = 10  # early stopping patience\n",
    "# Helper variables for main training loop\n",
    "wait = 0\n",
    "preds = None\n",
    "best_val_loss = 99999\n",
    "\n",
    "# Fit\n",
    "for epoch in range(1, NB_EPOCH+1):\n",
    "\n",
    "    # Log wall-clock time\n",
    "    t = time.time()\n",
    "\n",
    "    # Single training iteration (we mask nodes without labels for loss calculation)\n",
    "    model.fit(graph, y_train, sample_weight=train_mask,\n",
    "              batch_size=A.shape[0], epochs=1, shuffle=False, verbose=0)\n",
    "\n",
    "    # Predict on full dataset\n",
    "    preds = model.predict(graph, batch_size=A.shape[0])\n",
    "\n",
    "    # Train / validation scores\n",
    "    train_val_loss, train_val_acc = evaluate_preds(preds, [y_train, y_val],\n",
    "                                                   [idx_train, idx_val])\n",
    "    print(\"Epoch: {:04d}\".format(epoch),\n",
    "          \"train_loss= {:.4f}\".format(train_val_loss[0]),\n",
    "          \"train_acc= {:.4f}\".format(train_val_acc[0]),\n",
    "          \"val_loss= {:.4f}\".format(train_val_loss[1]),\n",
    "          \"val_acc= {:.4f}\".format(train_val_acc[1]),\n",
    "          \"time= {:.4f}\".format(time.time() - t))\n",
    "\n",
    "    # Early stopping\n",
    "    if train_val_loss[1] < best_val_loss:\n",
    "        best_val_loss = train_val_loss[1]\n",
    "        wait = 0\n",
    "    else:\n",
    "        if wait >= PATIENCE:\n",
    "            print('Epoch {}: early stopping'.format(epoch))\n",
    "            break\n",
    "        wait += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate_preds(preds, [y_test], [idx_test])\n",
    "print(f\"\"\"\n",
    "loss = {test_loss[0]:.4f}\n",
    "accuracy = {test_acc[0]:.4f}\n",
    "\"\"\".strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
