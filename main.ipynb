{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the path of execution\n",
    "EXE_PATH = os.path.abspath(os.path.curdir)\n",
    "# the path of the vendor files\n",
    "VENDOR_PATH = os.path.join(EXE_PATH, 'vendor')\n",
    "# the vendors to include in the system path\n",
    "VENDORS = ['keras-gcn']\n",
    "# create the absolute paths for all vendors\n",
    "VENDORS = list(map(lambda x: os.path.join(VENDOR_PATH, x), VENDORS))\n",
    "# update the Python path to include necessary vendor module\n",
    "sys.path += VENDORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kegra.layers.graph import GraphConvolution\n",
    "from kegra.utils import load_data, preprocess_adj, get_splits, evaluate_preds\n",
    "from ggcn import GaussianGraphConvolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "Dataset has 2708 nodes, 5429 edges, 1433 features.\n"
     ]
    }
   ],
   "source": [
    "X, A, y = load_data('data/cora/', dataset='cora')\n",
    "X /= X.sum(1).reshape(-1, 1)\n",
    "A = preprocess_adj(A)\n",
    "y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask = get_splits(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2708x2708 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 13264 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 1],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1]], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1110 16:52:37.544222 140277997516608 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1110 16:52:37.553567 140277997516608 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:515: The name tf.sparse_placeholder is deprecated. Please use tf.compat.v1.sparse_placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "support = 1\n",
    "graph = [X, A]\n",
    "G = [Input(shape=(None, None), batch_shape=(None, None), sparse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 16:52:37.559082 140277997516608 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_in = Input(shape=(X.shape[1],))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 16:52:37.565951 140277997516608 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1110 16:52:37.571066 140277997516608 deprecation.py:506] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1110 16:52:37.579940 140277997516608 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define model architecture\n",
    "# NOTE: We pass arguments for graph convolutional layers as a list of tensors.\n",
    "# This is somewhat hacky, more elegant options would require rewriting the Layer base class.\n",
    "H = Dropout(0.5)(X_in)\n",
    "H = GraphConvolution(32, support, activation='relu', kernel_regularizer=l2(5e-4))([H]+G)\n",
    "H = Dropout(0.5)(H)\n",
    "Y = GraphConvolution(y.shape[1], support, activation='softmax')([H]+G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 16:52:37.622932 140277997516608 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[X_in]+G, outputs=Y)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 1433)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1433)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_1 (GraphConvo (None, 32)           45888       dropout_1[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32)           0           graph_convolution_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_2 (GraphConvo (None, 7)            231         dropout_2[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 46,119\n",
      "Trainable params: 46,119\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 16:52:37.702895 140277997516608 deprecation.py:323] From /home/bitcommander/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.9320 train_acc= 0.3857 val_loss= 1.9337 val_acc= 0.3833 time= 1.0840\n",
      "Epoch: 0002 train_loss= 1.9169 train_acc= 0.3214 val_loss= 1.9200 val_acc= 0.3533 time= 0.0235\n",
      "Epoch: 0003 train_loss= 1.9003 train_acc= 0.3357 val_loss= 1.9057 val_acc= 0.3600 time= 0.0202\n",
      "Epoch: 0004 train_loss= 1.8826 train_acc= 0.3857 val_loss= 1.8908 val_acc= 0.3633 time= 0.0264\n",
      "Epoch: 0005 train_loss= 1.8641 train_acc= 0.3929 val_loss= 1.8754 val_acc= 0.3667 time= 0.0245\n",
      "Epoch: 0006 train_loss= 1.8454 train_acc= 0.4143 val_loss= 1.8600 val_acc= 0.3700 time= 0.0245\n",
      "Epoch: 0007 train_loss= 1.8266 train_acc= 0.4143 val_loss= 1.8447 val_acc= 0.3767 time= 0.0187\n",
      "Epoch: 0008 train_loss= 1.8078 train_acc= 0.4214 val_loss= 1.8296 val_acc= 0.3867 time= 0.0227\n",
      "Epoch: 0009 train_loss= 1.7895 train_acc= 0.4286 val_loss= 1.8151 val_acc= 0.3967 time= 0.0286\n",
      "Epoch: 0010 train_loss= 1.7718 train_acc= 0.4286 val_loss= 1.8013 val_acc= 0.4000 time= 0.0255\n",
      "Epoch: 0011 train_loss= 1.7550 train_acc= 0.4286 val_loss= 1.7883 val_acc= 0.4033 time= 0.0233\n",
      "Epoch: 0012 train_loss= 1.7392 train_acc= 0.4286 val_loss= 1.7764 val_acc= 0.4000 time= 0.0184\n",
      "Epoch: 0013 train_loss= 1.7245 train_acc= 0.4214 val_loss= 1.7655 val_acc= 0.3900 time= 0.0193\n",
      "Epoch: 0014 train_loss= 1.7106 train_acc= 0.4214 val_loss= 1.7558 val_acc= 0.3900 time= 0.0231\n",
      "Epoch: 0015 train_loss= 1.6977 train_acc= 0.4214 val_loss= 1.7473 val_acc= 0.3900 time= 0.0197\n",
      "Epoch: 0016 train_loss= 1.6856 train_acc= 0.4286 val_loss= 1.7396 val_acc= 0.4000 time= 0.0235\n",
      "Epoch: 0017 train_loss= 1.6735 train_acc= 0.4286 val_loss= 1.7320 val_acc= 0.4067 time= 0.0236\n",
      "Epoch: 0018 train_loss= 1.6612 train_acc= 0.4357 val_loss= 1.7243 val_acc= 0.4100 time= 0.0207\n",
      "Epoch: 0019 train_loss= 1.6484 train_acc= 0.4357 val_loss= 1.7163 val_acc= 0.4100 time= 0.0237\n",
      "Epoch: 0020 train_loss= 1.6353 train_acc= 0.4571 val_loss= 1.7079 val_acc= 0.4200 time= 0.0186\n",
      "Epoch: 0021 train_loss= 1.6216 train_acc= 0.4643 val_loss= 1.6991 val_acc= 0.4367 time= 0.0192\n",
      "Epoch: 0022 train_loss= 1.6075 train_acc= 0.4643 val_loss= 1.6900 val_acc= 0.4433 time= 0.0193\n",
      "Epoch: 0023 train_loss= 1.5932 train_acc= 0.4643 val_loss= 1.6806 val_acc= 0.4533 time= 0.0187\n",
      "Epoch: 0024 train_loss= 1.5787 train_acc= 0.4643 val_loss= 1.6709 val_acc= 0.4667 time= 0.0184\n",
      "Epoch: 0025 train_loss= 1.5644 train_acc= 0.4643 val_loss= 1.6613 val_acc= 0.4700 time= 0.0245\n",
      "Epoch: 0026 train_loss= 1.5503 train_acc= 0.4857 val_loss= 1.6519 val_acc= 0.4700 time= 0.0232\n",
      "Epoch: 0027 train_loss= 1.5363 train_acc= 0.4857 val_loss= 1.6425 val_acc= 0.4733 time= 0.0197\n",
      "Epoch: 0028 train_loss= 1.5221 train_acc= 0.4929 val_loss= 1.6330 val_acc= 0.4767 time= 0.0187\n",
      "Epoch: 0029 train_loss= 1.5081 train_acc= 0.5143 val_loss= 1.6233 val_acc= 0.4967 time= 0.0197\n",
      "Epoch: 0030 train_loss= 1.4939 train_acc= 0.5214 val_loss= 1.6135 val_acc= 0.5000 time= 0.0241\n",
      "Epoch: 0031 train_loss= 1.4792 train_acc= 0.5357 val_loss= 1.6027 val_acc= 0.5100 time= 0.0235\n",
      "Epoch: 0032 train_loss= 1.4640 train_acc= 0.5429 val_loss= 1.5915 val_acc= 0.5100 time= 0.0244\n",
      "Epoch: 0033 train_loss= 1.4485 train_acc= 0.5429 val_loss= 1.5798 val_acc= 0.5167 time= 0.0228\n",
      "Epoch: 0034 train_loss= 1.4329 train_acc= 0.5429 val_loss= 1.5681 val_acc= 0.5167 time= 0.0247\n",
      "Epoch: 0035 train_loss= 1.4172 train_acc= 0.5500 val_loss= 1.5565 val_acc= 0.5167 time= 0.0242\n",
      "Epoch: 0036 train_loss= 1.4015 train_acc= 0.5500 val_loss= 1.5454 val_acc= 0.5100 time= 0.0227\n",
      "Epoch: 0037 train_loss= 1.3861 train_acc= 0.5500 val_loss= 1.5344 val_acc= 0.5233 time= 0.0187\n",
      "Epoch: 0038 train_loss= 1.3708 train_acc= 0.5571 val_loss= 1.5235 val_acc= 0.5267 time= 0.0195\n",
      "Epoch: 0039 train_loss= 1.3557 train_acc= 0.5786 val_loss= 1.5125 val_acc= 0.5300 time= 0.0187\n",
      "Epoch: 0040 train_loss= 1.3406 train_acc= 0.5786 val_loss= 1.5014 val_acc= 0.5400 time= 0.0241\n",
      "Epoch: 0041 train_loss= 1.3254 train_acc= 0.5857 val_loss= 1.4905 val_acc= 0.5467 time= 0.0232\n",
      "Epoch: 0042 train_loss= 1.3103 train_acc= 0.6357 val_loss= 1.4795 val_acc= 0.5467 time= 0.0194\n",
      "Epoch: 0043 train_loss= 1.2951 train_acc= 0.6643 val_loss= 1.4683 val_acc= 0.5600 time= 0.0242\n",
      "Epoch: 0044 train_loss= 1.2798 train_acc= 0.6929 val_loss= 1.4571 val_acc= 0.5633 time= 0.0236\n",
      "Epoch: 0045 train_loss= 1.2645 train_acc= 0.7000 val_loss= 1.4458 val_acc= 0.5900 time= 0.0186\n",
      "Epoch: 0046 train_loss= 1.2491 train_acc= 0.7214 val_loss= 1.4345 val_acc= 0.5933 time= 0.0239\n",
      "Epoch: 0047 train_loss= 1.2337 train_acc= 0.7357 val_loss= 1.4230 val_acc= 0.6033 time= 0.0240\n",
      "Epoch: 0048 train_loss= 1.2186 train_acc= 0.7429 val_loss= 1.4113 val_acc= 0.6033 time= 0.0235\n",
      "Epoch: 0049 train_loss= 1.2039 train_acc= 0.7500 val_loss= 1.3999 val_acc= 0.6133 time= 0.0233\n",
      "Epoch: 0050 train_loss= 1.1894 train_acc= 0.7500 val_loss= 1.3888 val_acc= 0.6267 time= 0.0233\n",
      "Epoch: 0051 train_loss= 1.1753 train_acc= 0.7786 val_loss= 1.3779 val_acc= 0.6333 time= 0.0184\n",
      "Epoch: 0052 train_loss= 1.1612 train_acc= 0.7857 val_loss= 1.3671 val_acc= 0.6433 time= 0.0241\n",
      "Epoch: 0053 train_loss= 1.1472 train_acc= 0.7857 val_loss= 1.3561 val_acc= 0.6533 time= 0.0274\n",
      "Epoch: 0054 train_loss= 1.1333 train_acc= 0.7857 val_loss= 1.3450 val_acc= 0.6600 time= 0.0226\n",
      "Epoch: 0055 train_loss= 1.1192 train_acc= 0.7857 val_loss= 1.3340 val_acc= 0.6700 time= 0.0234\n",
      "Epoch: 0056 train_loss= 1.1054 train_acc= 0.7857 val_loss= 1.3232 val_acc= 0.6800 time= 0.0188\n",
      "Epoch: 0057 train_loss= 1.0920 train_acc= 0.7857 val_loss= 1.3125 val_acc= 0.6767 time= 0.0229\n",
      "Epoch: 0058 train_loss= 1.0790 train_acc= 0.7929 val_loss= 1.3021 val_acc= 0.6733 time= 0.0193\n",
      "Epoch: 0059 train_loss= 1.0662 train_acc= 0.8000 val_loss= 1.2916 val_acc= 0.6733 time= 0.0253\n",
      "Epoch: 0060 train_loss= 1.0534 train_acc= 0.8000 val_loss= 1.2810 val_acc= 0.6667 time= 0.0202\n",
      "Epoch: 0061 train_loss= 1.0407 train_acc= 0.8000 val_loss= 1.2706 val_acc= 0.6633 time= 0.0232\n",
      "Epoch: 0062 train_loss= 1.0280 train_acc= 0.8071 val_loss= 1.2605 val_acc= 0.6700 time= 0.0188\n",
      "Epoch: 0063 train_loss= 1.0153 train_acc= 0.8143 val_loss= 1.2510 val_acc= 0.6767 time= 0.0233\n",
      "Epoch: 0064 train_loss= 1.0030 train_acc= 0.8214 val_loss= 1.2418 val_acc= 0.6867 time= 0.0241\n",
      "Epoch: 0065 train_loss= 0.9909 train_acc= 0.8286 val_loss= 1.2328 val_acc= 0.6967 time= 0.0237\n",
      "Epoch: 0066 train_loss= 0.9792 train_acc= 0.8357 val_loss= 1.2241 val_acc= 0.7133 time= 0.0195\n",
      "Epoch: 0067 train_loss= 0.9679 train_acc= 0.8357 val_loss= 1.2151 val_acc= 0.7200 time= 0.0236\n",
      "Epoch: 0068 train_loss= 0.9568 train_acc= 0.8357 val_loss= 1.2059 val_acc= 0.7233 time= 0.0185\n",
      "Epoch: 0069 train_loss= 0.9462 train_acc= 0.8286 val_loss= 1.1968 val_acc= 0.7233 time= 0.0189\n",
      "Epoch: 0070 train_loss= 0.9357 train_acc= 0.8286 val_loss= 1.1882 val_acc= 0.7200 time= 0.0214\n",
      "Epoch: 0071 train_loss= 0.9250 train_acc= 0.8357 val_loss= 1.1791 val_acc= 0.7200 time= 0.0243\n",
      "Epoch: 0072 train_loss= 0.9142 train_acc= 0.8357 val_loss= 1.1702 val_acc= 0.7233 time= 0.0233\n",
      "Epoch: 0073 train_loss= 0.9032 train_acc= 0.8357 val_loss= 1.1612 val_acc= 0.7300 time= 0.0238\n",
      "Epoch: 0074 train_loss= 0.8921 train_acc= 0.8500 val_loss= 1.1523 val_acc= 0.7333 time= 0.0187\n",
      "Epoch: 0075 train_loss= 0.8815 train_acc= 0.8571 val_loss= 1.1438 val_acc= 0.7333 time= 0.0183\n",
      "Epoch: 0076 train_loss= 0.8712 train_acc= 0.8571 val_loss= 1.1357 val_acc= 0.7400 time= 0.0187\n",
      "Epoch: 0077 train_loss= 0.8615 train_acc= 0.8571 val_loss= 1.1285 val_acc= 0.7467 time= 0.0233\n",
      "Epoch: 0078 train_loss= 0.8525 train_acc= 0.8571 val_loss= 1.1223 val_acc= 0.7567 time= 0.0182\n",
      "Epoch: 0079 train_loss= 0.8438 train_acc= 0.8714 val_loss= 1.1161 val_acc= 0.7633 time= 0.0232\n",
      "Epoch: 0080 train_loss= 0.8346 train_acc= 0.8929 val_loss= 1.1082 val_acc= 0.7667 time= 0.0185\n",
      "Epoch: 0081 train_loss= 0.8253 train_acc= 0.8929 val_loss= 1.0997 val_acc= 0.7700 time= 0.0227\n",
      "Epoch: 0082 train_loss= 0.8159 train_acc= 0.8929 val_loss= 1.0904 val_acc= 0.7667 time= 0.0182\n",
      "Epoch: 0083 train_loss= 0.8072 train_acc= 0.8786 val_loss= 1.0811 val_acc= 0.7600 time= 0.0193\n",
      "Epoch: 0084 train_loss= 0.7996 train_acc= 0.8786 val_loss= 1.0725 val_acc= 0.7633 time= 0.0257\n",
      "Epoch: 0085 train_loss= 0.7918 train_acc= 0.8786 val_loss= 1.0653 val_acc= 0.7600 time= 0.0193\n",
      "Epoch: 0086 train_loss= 0.7841 train_acc= 0.8786 val_loss= 1.0587 val_acc= 0.7567 time= 0.0194\n",
      "Epoch: 0087 train_loss= 0.7756 train_acc= 0.8786 val_loss= 1.0523 val_acc= 0.7567 time= 0.0199\n",
      "Epoch: 0088 train_loss= 0.7670 train_acc= 0.8786 val_loss= 1.0466 val_acc= 0.7667 time= 0.0287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0089 train_loss= 0.7586 train_acc= 0.8857 val_loss= 1.0415 val_acc= 0.7733 time= 0.0239\n",
      "Epoch: 0090 train_loss= 0.7508 train_acc= 0.8857 val_loss= 1.0369 val_acc= 0.7767 time= 0.0229\n",
      "Epoch: 0091 train_loss= 0.7433 train_acc= 0.9000 val_loss= 1.0327 val_acc= 0.7867 time= 0.0187\n",
      "Epoch: 0092 train_loss= 0.7365 train_acc= 0.9071 val_loss= 1.0287 val_acc= 0.7833 time= 0.0197\n",
      "Epoch: 0093 train_loss= 0.7296 train_acc= 0.9000 val_loss= 1.0236 val_acc= 0.7867 time= 0.0189\n",
      "Epoch: 0094 train_loss= 0.7231 train_acc= 0.9000 val_loss= 1.0181 val_acc= 0.7867 time= 0.0238\n",
      "Epoch: 0095 train_loss= 0.7167 train_acc= 0.9000 val_loss= 1.0122 val_acc= 0.7867 time= 0.0199\n",
      "Epoch: 0096 train_loss= 0.7104 train_acc= 0.9000 val_loss= 1.0053 val_acc= 0.7867 time= 0.0186\n",
      "Epoch: 0097 train_loss= 0.7044 train_acc= 0.9143 val_loss= 0.9992 val_acc= 0.7900 time= 0.0201\n",
      "Epoch: 0098 train_loss= 0.6984 train_acc= 0.9143 val_loss= 0.9937 val_acc= 0.7867 time= 0.0234\n",
      "Epoch: 0099 train_loss= 0.6927 train_acc= 0.9143 val_loss= 0.9883 val_acc= 0.7900 time= 0.0182\n",
      "Epoch: 0100 train_loss= 0.6872 train_acc= 0.9143 val_loss= 0.9835 val_acc= 0.7867 time= 0.0240\n",
      "Epoch: 0101 train_loss= 0.6815 train_acc= 0.9143 val_loss= 0.9779 val_acc= 0.7867 time= 0.0180\n",
      "Epoch: 0102 train_loss= 0.6758 train_acc= 0.9143 val_loss= 0.9720 val_acc= 0.7900 time= 0.0185\n",
      "Epoch: 0103 train_loss= 0.6698 train_acc= 0.9214 val_loss= 0.9674 val_acc= 0.7900 time= 0.0230\n",
      "Epoch: 0104 train_loss= 0.6639 train_acc= 0.9214 val_loss= 0.9632 val_acc= 0.7867 time= 0.0194\n",
      "Epoch: 0105 train_loss= 0.6580 train_acc= 0.9214 val_loss= 0.9600 val_acc= 0.7933 time= 0.0205\n",
      "Epoch: 0106 train_loss= 0.6522 train_acc= 0.9286 val_loss= 0.9576 val_acc= 0.8000 time= 0.0281\n",
      "Epoch: 0107 train_loss= 0.6467 train_acc= 0.9286 val_loss= 0.9548 val_acc= 0.8000 time= 0.0227\n",
      "Epoch: 0108 train_loss= 0.6416 train_acc= 0.9286 val_loss= 0.9514 val_acc= 0.8000 time= 0.0250\n",
      "Epoch: 0109 train_loss= 0.6365 train_acc= 0.9286 val_loss= 0.9477 val_acc= 0.8000 time= 0.0252\n",
      "Epoch: 0110 train_loss= 0.6315 train_acc= 0.9214 val_loss= 0.9436 val_acc= 0.8000 time= 0.0192\n",
      "Epoch: 0111 train_loss= 0.6262 train_acc= 0.9286 val_loss= 0.9385 val_acc= 0.8000 time= 0.0184\n",
      "Epoch: 0112 train_loss= 0.6210 train_acc= 0.9286 val_loss= 0.9339 val_acc= 0.7967 time= 0.0241\n",
      "Epoch: 0113 train_loss= 0.6163 train_acc= 0.9286 val_loss= 0.9294 val_acc= 0.7867 time= 0.0287\n",
      "Epoch: 0114 train_loss= 0.6120 train_acc= 0.9357 val_loss= 0.9247 val_acc= 0.7867 time= 0.0196\n",
      "Epoch: 0115 train_loss= 0.6080 train_acc= 0.9357 val_loss= 0.9207 val_acc= 0.7867 time= 0.0272\n",
      "Epoch: 0116 train_loss= 0.6031 train_acc= 0.9357 val_loss= 0.9169 val_acc= 0.7867 time= 0.0208\n",
      "Epoch: 0117 train_loss= 0.5979 train_acc= 0.9357 val_loss= 0.9135 val_acc= 0.7933 time= 0.0279\n",
      "Epoch: 0118 train_loss= 0.5928 train_acc= 0.9357 val_loss= 0.9109 val_acc= 0.7967 time= 0.0209\n",
      "Epoch: 0119 train_loss= 0.5882 train_acc= 0.9357 val_loss= 0.9091 val_acc= 0.8000 time= 0.0237\n",
      "Epoch: 0120 train_loss= 0.5841 train_acc= 0.9500 val_loss= 0.9080 val_acc= 0.8000 time= 0.0235\n",
      "Epoch: 0121 train_loss= 0.5803 train_acc= 0.9500 val_loss= 0.9063 val_acc= 0.7967 time= 0.0196\n",
      "Epoch: 0122 train_loss= 0.5761 train_acc= 0.9500 val_loss= 0.9031 val_acc= 0.8000 time= 0.0241\n",
      "Epoch: 0123 train_loss= 0.5714 train_acc= 0.9500 val_loss= 0.8985 val_acc= 0.8033 time= 0.0229\n",
      "Epoch: 0124 train_loss= 0.5667 train_acc= 0.9500 val_loss= 0.8933 val_acc= 0.7967 time= 0.0245\n",
      "Epoch: 0125 train_loss= 0.5632 train_acc= 0.9500 val_loss= 0.8888 val_acc= 0.7933 time= 0.0232\n",
      "Epoch: 0126 train_loss= 0.5604 train_acc= 0.9500 val_loss= 0.8863 val_acc= 0.7933 time= 0.0238\n",
      "Epoch: 0127 train_loss= 0.5568 train_acc= 0.9500 val_loss= 0.8839 val_acc= 0.7933 time= 0.0200\n",
      "Epoch: 0128 train_loss= 0.5524 train_acc= 0.9500 val_loss= 0.8813 val_acc= 0.7933 time= 0.0190\n",
      "Epoch: 0129 train_loss= 0.5471 train_acc= 0.9571 val_loss= 0.8780 val_acc= 0.7900 time= 0.0201\n",
      "Epoch: 0130 train_loss= 0.5431 train_acc= 0.9571 val_loss= 0.8744 val_acc= 0.8000 time= 0.0275\n",
      "Epoch: 0131 train_loss= 0.5408 train_acc= 0.9571 val_loss= 0.8718 val_acc= 0.7967 time= 0.0185\n",
      "Epoch: 0132 train_loss= 0.5399 train_acc= 0.9429 val_loss= 0.8702 val_acc= 0.7967 time= 0.0239\n",
      "Epoch: 0133 train_loss= 0.5381 train_acc= 0.9357 val_loss= 0.8680 val_acc= 0.7867 time= 0.0234\n",
      "Epoch: 0134 train_loss= 0.5350 train_acc= 0.9357 val_loss= 0.8650 val_acc= 0.7900 time= 0.0242\n",
      "Epoch: 0135 train_loss= 0.5299 train_acc= 0.9500 val_loss= 0.8608 val_acc= 0.8067 time= 0.0191\n",
      "Epoch: 0136 train_loss= 0.5247 train_acc= 0.9500 val_loss= 0.8572 val_acc= 0.8100 time= 0.0183\n",
      "Epoch: 0137 train_loss= 0.5207 train_acc= 0.9571 val_loss= 0.8558 val_acc= 0.8067 time= 0.0239\n",
      "Epoch: 0138 train_loss= 0.5179 train_acc= 0.9571 val_loss= 0.8553 val_acc= 0.7967 time= 0.0248\n",
      "Epoch: 0139 train_loss= 0.5158 train_acc= 0.9571 val_loss= 0.8545 val_acc= 0.7900 time= 0.0232\n",
      "Epoch: 0140 train_loss= 0.5139 train_acc= 0.9571 val_loss= 0.8535 val_acc= 0.7900 time= 0.0192\n",
      "Epoch: 0141 train_loss= 0.5106 train_acc= 0.9571 val_loss= 0.8497 val_acc= 0.7933 time= 0.0232\n",
      "Epoch: 0142 train_loss= 0.5062 train_acc= 0.9571 val_loss= 0.8440 val_acc= 0.7933 time= 0.0289\n",
      "Epoch: 0143 train_loss= 0.5016 train_acc= 0.9571 val_loss= 0.8379 val_acc= 0.8000 time= 0.0224\n",
      "Epoch: 0144 train_loss= 0.4976 train_acc= 0.9571 val_loss= 0.8335 val_acc= 0.7967 time= 0.0181\n",
      "Epoch: 0145 train_loss= 0.4950 train_acc= 0.9571 val_loss= 0.8311 val_acc= 0.8100 time= 0.0227\n",
      "Epoch: 0146 train_loss= 0.4933 train_acc= 0.9571 val_loss= 0.8305 val_acc= 0.8000 time= 0.0183\n",
      "Epoch: 0147 train_loss= 0.4904 train_acc= 0.9571 val_loss= 0.8295 val_acc= 0.8067 time= 0.0195\n",
      "Epoch: 0148 train_loss= 0.4869 train_acc= 0.9571 val_loss= 0.8276 val_acc= 0.8167 time= 0.0238\n",
      "Epoch: 0149 train_loss= 0.4826 train_acc= 0.9571 val_loss= 0.8249 val_acc= 0.8267 time= 0.0235\n",
      "Epoch: 0150 train_loss= 0.4789 train_acc= 0.9643 val_loss= 0.8223 val_acc= 0.8233 time= 0.0211\n",
      "Epoch: 0151 train_loss= 0.4760 train_acc= 0.9643 val_loss= 0.8213 val_acc= 0.8200 time= 0.0237\n",
      "Epoch: 0152 train_loss= 0.4738 train_acc= 0.9643 val_loss= 0.8208 val_acc= 0.8100 time= 0.0238\n",
      "Epoch: 0153 train_loss= 0.4723 train_acc= 0.9643 val_loss= 0.8212 val_acc= 0.8067 time= 0.0193\n",
      "Epoch: 0154 train_loss= 0.4698 train_acc= 0.9643 val_loss= 0.8192 val_acc= 0.8067 time= 0.0243\n",
      "Epoch: 0155 train_loss= 0.4667 train_acc= 0.9571 val_loss= 0.8165 val_acc= 0.8033 time= 0.0250\n",
      "Epoch: 0156 train_loss= 0.4640 train_acc= 0.9571 val_loss= 0.8140 val_acc= 0.8067 time= 0.0248\n",
      "Epoch: 0157 train_loss= 0.4616 train_acc= 0.9643 val_loss= 0.8108 val_acc= 0.8033 time= 0.0230\n",
      "Epoch: 0158 train_loss= 0.4600 train_acc= 0.9643 val_loss= 0.8080 val_acc= 0.8067 time= 0.0240\n",
      "Epoch: 0159 train_loss= 0.4590 train_acc= 0.9643 val_loss= 0.8066 val_acc= 0.8067 time= 0.0241\n",
      "Epoch: 0160 train_loss= 0.4587 train_acc= 0.9643 val_loss= 0.8054 val_acc= 0.8033 time= 0.0271\n",
      "Epoch: 0161 train_loss= 0.4558 train_acc= 0.9643 val_loss= 0.8031 val_acc= 0.8100 time= 0.0185\n",
      "Epoch: 0162 train_loss= 0.4523 train_acc= 0.9643 val_loss= 0.8007 val_acc= 0.8167 time= 0.0257\n",
      "Epoch: 0163 train_loss= 0.4496 train_acc= 0.9643 val_loss= 0.7996 val_acc= 0.8200 time= 0.0284\n",
      "Epoch: 0164 train_loss= 0.4481 train_acc= 0.9643 val_loss= 0.7997 val_acc= 0.8133 time= 0.0233\n",
      "Epoch: 0165 train_loss= 0.4464 train_acc= 0.9643 val_loss= 0.7999 val_acc= 0.8133 time= 0.0231\n",
      "Epoch: 0166 train_loss= 0.4446 train_acc= 0.9714 val_loss= 0.8003 val_acc= 0.8133 time= 0.0195\n",
      "Epoch: 0167 train_loss= 0.4414 train_acc= 0.9714 val_loss= 0.7976 val_acc= 0.8200 time= 0.0278\n",
      "Epoch: 0168 train_loss= 0.4383 train_acc= 0.9714 val_loss= 0.7946 val_acc= 0.8133 time= 0.0281\n",
      "Epoch: 0169 train_loss= 0.4361 train_acc= 0.9714 val_loss= 0.7931 val_acc= 0.8067 time= 0.0200\n",
      "Epoch: 0170 train_loss= 0.4339 train_acc= 0.9643 val_loss= 0.7918 val_acc= 0.8067 time= 0.0226\n",
      "Epoch: 0171 train_loss= 0.4316 train_acc= 0.9643 val_loss= 0.7901 val_acc= 0.8100 time= 0.0232\n",
      "Epoch: 0172 train_loss= 0.4292 train_acc= 0.9643 val_loss= 0.7869 val_acc= 0.8100 time= 0.0200\n",
      "Epoch: 0173 train_loss= 0.4278 train_acc= 0.9643 val_loss= 0.7846 val_acc= 0.8100 time= 0.0293\n",
      "Epoch: 0174 train_loss= 0.4250 train_acc= 0.9643 val_loss= 0.7831 val_acc= 0.8100 time= 0.0294\n",
      "Epoch: 0175 train_loss= 0.4219 train_acc= 0.9714 val_loss= 0.7817 val_acc= 0.8067 time= 0.0187\n",
      "Epoch: 0176 train_loss= 0.4191 train_acc= 0.9714 val_loss= 0.7814 val_acc= 0.8100 time= 0.0187\n",
      "Epoch: 0177 train_loss= 0.4168 train_acc= 0.9714 val_loss= 0.7814 val_acc= 0.8133 time= 0.0240\n",
      "Epoch: 0178 train_loss= 0.4153 train_acc= 0.9714 val_loss= 0.7815 val_acc= 0.8133 time= 0.0186\n",
      "Epoch: 0179 train_loss= 0.4135 train_acc= 0.9714 val_loss= 0.7806 val_acc= 0.8167 time= 0.0228\n",
      "Epoch: 0180 train_loss= 0.4111 train_acc= 0.9714 val_loss= 0.7784 val_acc= 0.8133 time= 0.0191\n",
      "Epoch: 0181 train_loss= 0.4090 train_acc= 0.9714 val_loss= 0.7756 val_acc= 0.8100 time= 0.0238\n",
      "Epoch: 0182 train_loss= 0.4075 train_acc= 0.9714 val_loss= 0.7731 val_acc= 0.8033 time= 0.0233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0183 train_loss= 0.4065 train_acc= 0.9714 val_loss= 0.7718 val_acc= 0.8033 time= 0.0235\n",
      "Epoch: 0184 train_loss= 0.4050 train_acc= 0.9714 val_loss= 0.7709 val_acc= 0.8033 time= 0.0230\n",
      "Epoch: 0185 train_loss= 0.4018 train_acc= 0.9714 val_loss= 0.7701 val_acc= 0.8033 time= 0.0253\n",
      "Epoch: 0186 train_loss= 0.3990 train_acc= 0.9714 val_loss= 0.7701 val_acc= 0.8067 time= 0.0228\n",
      "Epoch: 0187 train_loss= 0.3969 train_acc= 0.9714 val_loss= 0.7690 val_acc= 0.8067 time= 0.0232\n",
      "Epoch: 0188 train_loss= 0.3954 train_acc= 0.9714 val_loss= 0.7691 val_acc= 0.8067 time= 0.0222\n",
      "Epoch: 0189 train_loss= 0.3946 train_acc= 0.9643 val_loss= 0.7693 val_acc= 0.8133 time= 0.0237\n",
      "Epoch: 0190 train_loss= 0.3934 train_acc= 0.9643 val_loss= 0.7704 val_acc= 0.8200 time= 0.0229\n",
      "Epoch: 0191 train_loss= 0.3917 train_acc= 0.9714 val_loss= 0.7701 val_acc= 0.8200 time= 0.0249\n",
      "Epoch: 0192 train_loss= 0.3889 train_acc= 0.9714 val_loss= 0.7678 val_acc= 0.8100 time= 0.0236\n",
      "Epoch: 0193 train_loss= 0.3860 train_acc= 0.9714 val_loss= 0.7642 val_acc= 0.8100 time= 0.0293\n",
      "Epoch: 0194 train_loss= 0.3832 train_acc= 0.9714 val_loss= 0.7596 val_acc= 0.8100 time= 0.0244\n",
      "Epoch: 0195 train_loss= 0.3812 train_acc= 0.9714 val_loss= 0.7563 val_acc= 0.8100 time= 0.0209\n",
      "Epoch: 0196 train_loss= 0.3799 train_acc= 0.9714 val_loss= 0.7524 val_acc= 0.8100 time= 0.0239\n",
      "Epoch: 0197 train_loss= 0.3795 train_acc= 0.9714 val_loss= 0.7498 val_acc= 0.8067 time= 0.0232\n",
      "Epoch: 0198 train_loss= 0.3790 train_acc= 0.9714 val_loss= 0.7479 val_acc= 0.8133 time= 0.0241\n",
      "Epoch: 0199 train_loss= 0.3776 train_acc= 0.9714 val_loss= 0.7462 val_acc= 0.8167 time= 0.0229\n",
      "Epoch: 0200 train_loss= 0.3746 train_acc= 0.9714 val_loss= 0.7445 val_acc= 0.8100 time= 0.0245\n"
     ]
    }
   ],
   "source": [
    "NB_EPOCH = 200\n",
    "PATIENCE = 10  # early stopping patience\n",
    "# Helper variables for main training loop\n",
    "wait = 0\n",
    "preds = None\n",
    "best_val_loss = 99999\n",
    "\n",
    "# Fit\n",
    "for epoch in range(1, NB_EPOCH+1):\n",
    "\n",
    "    # Log wall-clock time\n",
    "    t = time.time()\n",
    "\n",
    "    # Single training iteration (we mask nodes without labels for loss calculation)\n",
    "    model.fit(graph, y_train, sample_weight=train_mask,\n",
    "              batch_size=A.shape[0], epochs=1, shuffle=False, verbose=0)\n",
    "\n",
    "    # Predict on full dataset\n",
    "    preds = model.predict(graph, batch_size=A.shape[0])\n",
    "\n",
    "    # Train / validation scores\n",
    "    train_val_loss, train_val_acc = evaluate_preds(preds, [y_train, y_val],\n",
    "                                                   [idx_train, idx_val])\n",
    "    print(\"Epoch: {:04d}\".format(epoch),\n",
    "          \"train_loss= {:.4f}\".format(train_val_loss[0]),\n",
    "          \"train_acc= {:.4f}\".format(train_val_acc[0]),\n",
    "          \"val_loss= {:.4f}\".format(train_val_loss[1]),\n",
    "          \"val_acc= {:.4f}\".format(train_val_acc[1]),\n",
    "          \"time= {:.4f}\".format(time.time() - t))\n",
    "\n",
    "    # Early stopping\n",
    "    if train_val_loss[1] < best_val_loss:\n",
    "        best_val_loss = train_val_loss[1]\n",
    "        wait = 0\n",
    "    else:\n",
    "        if wait >= PATIENCE:\n",
    "            print('Epoch {}: early stopping'.format(epoch))\n",
    "            break\n",
    "        wait += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.7835\n",
      "accuracy = 0.8130\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate_preds(preds, [y_test], [idx_test])\n",
    "print(f\"\"\"\n",
    "loss = {test_loss[0]:.4f}\n",
    "accuracy = {test_acc[0]:.4f}\n",
    "\"\"\".strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Graph Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 16:52:43.255880 140277997516608 deprecation.py:323] From /home/bitcommander/Desktop/robust-graph-convolutional-networks-against-adversarial-attacks-implementation/ggcn/ggcl.py:153: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "W1110 16:52:43.256954 140277997516608 deprecation.py:323] From /home/bitcommander/.local/lib/python3.7/site-packages/tensorflow/python/ops/distributions/normal.py:160: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    }
   ],
   "source": [
    "H = Dropout(0.5)(X_in)\n",
    "H = GaussianGraphConvolution(16,\n",
    "    is_first=True, \n",
    "    activation='relu', \n",
    "#     kernel_regularizer=l2(5e-4)\n",
    ")([H]+G)\n",
    "Y = GaussianGraphConvolution(y.shape[1],\n",
    "    is_last=True,\n",
    "    activation='softmax'\n",
    ")(H+G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[X_in]+G, outputs=Y)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 1433)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1433)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_graph_convolution_1 (G [(None, 16), (None,  45856       dropout_3[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_graph_convolution_2 (G (None, 7)            224         gaussian_graph_convolution_1[0][0\n",
      "                                                                 gaussian_graph_convolution_1[0][1\n",
      "                                                                 input_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 46,080\n",
      "Trainable params: 46,080\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.9410 train_acc= 0.3643 val_loss= 1.9428 val_acc= 0.2800 time= 0.4193\n",
      "Epoch: 0002 train_loss= 1.9348 train_acc= 0.4571 val_loss= 1.9377 val_acc= 0.3800 time= 0.0237\n",
      "Epoch: 0003 train_loss= 1.9259 train_acc= 0.5143 val_loss= 1.9307 val_acc= 0.4633 time= 0.0242\n",
      "Epoch: 0004 train_loss= 1.9142 train_acc= 0.6000 val_loss= 1.9236 val_acc= 0.4800 time= 0.0228\n",
      "Epoch: 0005 train_loss= 1.9039 train_acc= 0.5714 val_loss= 1.9140 val_acc= 0.4867 time= 0.0251\n",
      "Epoch: 0006 train_loss= 1.8915 train_acc= 0.6143 val_loss= 1.9080 val_acc= 0.4933 time= 0.0206\n",
      "Epoch: 0007 train_loss= 1.8814 train_acc= 0.6571 val_loss= 1.8976 val_acc= 0.4967 time= 0.0242\n",
      "Epoch: 0008 train_loss= 1.8684 train_acc= 0.6286 val_loss= 1.8916 val_acc= 0.4867 time= 0.0230\n",
      "Epoch: 0009 train_loss= 1.8532 train_acc= 0.6857 val_loss= 1.8783 val_acc= 0.5267 time= 0.0183\n",
      "Epoch: 0010 train_loss= 1.8382 train_acc= 0.6857 val_loss= 1.8669 val_acc= 0.5633 time= 0.0234\n",
      "Epoch: 0011 train_loss= 1.8230 train_acc= 0.6929 val_loss= 1.8570 val_acc= 0.5567 time= 0.0261\n",
      "Epoch: 0012 train_loss= 1.8049 train_acc= 0.6857 val_loss= 1.8437 val_acc= 0.5467 time= 0.0234\n",
      "Epoch: 0013 train_loss= 1.7889 train_acc= 0.6857 val_loss= 1.8285 val_acc= 0.5800 time= 0.0193\n",
      "Epoch: 0014 train_loss= 1.7694 train_acc= 0.6786 val_loss= 1.8162 val_acc= 0.5867 time= 0.0192\n",
      "Epoch: 0015 train_loss= 1.7523 train_acc= 0.7000 val_loss= 1.8023 val_acc= 0.5833 time= 0.0189\n",
      "Epoch: 0016 train_loss= 1.7319 train_acc= 0.6714 val_loss= 1.7873 val_acc= 0.5867 time= 0.0233\n",
      "Epoch: 0017 train_loss= 1.7129 train_acc= 0.7000 val_loss= 1.7700 val_acc= 0.6033 time= 0.0244\n",
      "Epoch: 0018 train_loss= 1.6913 train_acc= 0.6714 val_loss= 1.7559 val_acc= 0.5967 time= 0.0231\n",
      "Epoch: 0019 train_loss= 1.6687 train_acc= 0.7071 val_loss= 1.7398 val_acc= 0.6133 time= 0.0211\n",
      "Epoch: 0020 train_loss= 1.6475 train_acc= 0.6714 val_loss= 1.7212 val_acc= 0.5933 time= 0.0285\n",
      "Epoch: 0021 train_loss= 1.6258 train_acc= 0.7000 val_loss= 1.7033 val_acc= 0.6000 time= 0.0202\n",
      "Epoch: 0022 train_loss= 1.5986 train_acc= 0.6929 val_loss= 1.6863 val_acc= 0.5967 time= 0.0189\n",
      "Epoch: 0023 train_loss= 1.5752 train_acc= 0.7071 val_loss= 1.6680 val_acc= 0.6067 time= 0.0196\n",
      "Epoch: 0024 train_loss= 1.5512 train_acc= 0.7214 val_loss= 1.6493 val_acc= 0.6067 time= 0.0194\n",
      "Epoch: 0025 train_loss= 1.5251 train_acc= 0.7071 val_loss= 1.6297 val_acc= 0.6100 time= 0.0238\n",
      "Epoch: 0026 train_loss= 1.4949 train_acc= 0.7071 val_loss= 1.6101 val_acc= 0.6067 time= 0.0206\n",
      "Epoch: 0027 train_loss= 1.4731 train_acc= 0.7071 val_loss= 1.5915 val_acc= 0.6033 time= 0.0234\n",
      "Epoch: 0028 train_loss= 1.4476 train_acc= 0.6929 val_loss= 1.5705 val_acc= 0.6067 time= 0.0249\n",
      "Epoch: 0029 train_loss= 1.4235 train_acc= 0.7071 val_loss= 1.5525 val_acc= 0.6067 time= 0.0278\n",
      "Epoch: 0030 train_loss= 1.3942 train_acc= 0.7143 val_loss= 1.5320 val_acc= 0.6100 time= 0.0196\n",
      "Epoch: 0031 train_loss= 1.3724 train_acc= 0.7000 val_loss= 1.5114 val_acc= 0.6133 time= 0.0274\n",
      "Epoch: 0032 train_loss= 1.3439 train_acc= 0.7071 val_loss= 1.4926 val_acc= 0.6100 time= 0.0187\n",
      "Epoch: 0033 train_loss= 1.3201 train_acc= 0.7000 val_loss= 1.4711 val_acc= 0.6200 time= 0.0190\n",
      "Epoch: 0034 train_loss= 1.2933 train_acc= 0.7357 val_loss= 1.4505 val_acc= 0.6400 time= 0.0211\n",
      "Epoch: 0035 train_loss= 1.2676 train_acc= 0.7357 val_loss= 1.4304 val_acc= 0.6433 time= 0.0278\n",
      "Epoch: 0036 train_loss= 1.2388 train_acc= 0.7286 val_loss= 1.4107 val_acc= 0.6433 time= 0.0203\n",
      "Epoch: 0037 train_loss= 1.2134 train_acc= 0.7429 val_loss= 1.3908 val_acc= 0.6500 time= 0.0198\n",
      "Epoch: 0038 train_loss= 1.1902 train_acc= 0.7357 val_loss= 1.3715 val_acc= 0.6500 time= 0.0277\n",
      "Epoch: 0039 train_loss= 1.1657 train_acc= 0.7429 val_loss= 1.3521 val_acc= 0.6567 time= 0.0190\n",
      "Epoch: 0040 train_loss= 1.1363 train_acc= 0.7714 val_loss= 1.3323 val_acc= 0.6567 time= 0.0192\n",
      "Epoch: 0041 train_loss= 1.1123 train_acc= 0.7786 val_loss= 1.3133 val_acc= 0.6633 time= 0.0206\n",
      "Epoch: 0042 train_loss= 1.0877 train_acc= 0.7857 val_loss= 1.2945 val_acc= 0.6633 time= 0.0188\n",
      "Epoch: 0043 train_loss= 1.0644 train_acc= 0.8000 val_loss= 1.2765 val_acc= 0.6667 time= 0.0233\n",
      "Epoch: 0044 train_loss= 1.0395 train_acc= 0.8143 val_loss= 1.2579 val_acc= 0.6700 time= 0.0234\n",
      "Epoch: 0045 train_loss= 1.0184 train_acc= 0.8214 val_loss= 1.2392 val_acc= 0.6733 time= 0.0204\n",
      "Epoch: 0046 train_loss= 0.9936 train_acc= 0.8143 val_loss= 1.2213 val_acc= 0.6767 time= 0.0239\n",
      "Epoch: 0047 train_loss= 0.9712 train_acc= 0.8214 val_loss= 1.2040 val_acc= 0.6800 time= 0.0238\n",
      "Epoch: 0048 train_loss= 0.9470 train_acc= 0.8286 val_loss= 1.1871 val_acc= 0.6833 time= 0.0189\n",
      "Epoch: 0049 train_loss= 0.9265 train_acc= 0.8286 val_loss= 1.1701 val_acc= 0.6900 time= 0.0276\n",
      "Epoch: 0050 train_loss= 0.9028 train_acc= 0.8357 val_loss= 1.1536 val_acc= 0.7000 time= 0.0194\n",
      "Epoch: 0051 train_loss= 0.8834 train_acc= 0.8571 val_loss= 1.1372 val_acc= 0.7067 time= 0.0186\n",
      "Epoch: 0052 train_loss= 0.8611 train_acc= 0.8643 val_loss= 1.1210 val_acc= 0.7033 time= 0.0189\n",
      "Epoch: 0053 train_loss= 0.8414 train_acc= 0.8571 val_loss= 1.1053 val_acc= 0.7067 time= 0.0194\n",
      "Epoch: 0054 train_loss= 0.8207 train_acc= 0.8643 val_loss= 1.0903 val_acc= 0.7200 time= 0.0269\n",
      "Epoch: 0055 train_loss= 0.8015 train_acc= 0.8714 val_loss= 1.0755 val_acc= 0.7200 time= 0.0223\n",
      "Epoch: 0056 train_loss= 0.7820 train_acc= 0.8714 val_loss= 1.0610 val_acc= 0.7267 time= 0.0189\n",
      "Epoch: 0057 train_loss= 0.7631 train_acc= 0.8714 val_loss= 1.0459 val_acc= 0.7433 time= 0.0186\n",
      "Epoch: 0058 train_loss= 0.7444 train_acc= 0.8786 val_loss= 1.0307 val_acc= 0.7400 time= 0.0198\n",
      "Epoch: 0059 train_loss= 0.7277 train_acc= 0.8786 val_loss= 1.0167 val_acc= 0.7533 time= 0.0204\n",
      "Epoch: 0060 train_loss= 0.7081 train_acc= 0.8786 val_loss= 1.0039 val_acc= 0.7533 time= 0.0193\n",
      "Epoch: 0061 train_loss= 0.6913 train_acc= 0.8786 val_loss= 0.9896 val_acc= 0.7567 time= 0.0204\n",
      "Epoch: 0062 train_loss= 0.6752 train_acc= 0.8857 val_loss= 0.9777 val_acc= 0.7600 time= 0.0184\n",
      "Epoch: 0063 train_loss= 0.6556 train_acc= 0.8857 val_loss= 0.9645 val_acc= 0.7700 time= 0.0187\n",
      "Epoch: 0064 train_loss= 0.6409 train_acc= 0.8929 val_loss= 0.9512 val_acc= 0.7700 time= 0.0240\n",
      "Epoch: 0065 train_loss= 0.6242 train_acc= 0.9000 val_loss= 0.9400 val_acc= 0.7800 time= 0.0207\n",
      "Epoch: 0066 train_loss= 0.6091 train_acc= 0.9000 val_loss= 0.9293 val_acc= 0.7800 time= 0.0241\n",
      "Epoch: 0067 train_loss= 0.5932 train_acc= 0.9071 val_loss= 0.9159 val_acc= 0.7867 time= 0.0229\n",
      "Epoch: 0068 train_loss= 0.5784 train_acc= 0.9071 val_loss= 0.9037 val_acc= 0.8000 time= 0.0242\n",
      "Epoch: 0069 train_loss= 0.5654 train_acc= 0.9143 val_loss= 0.8951 val_acc= 0.7933 time= 0.0255\n",
      "Epoch: 0070 train_loss= 0.5482 train_acc= 0.9286 val_loss= 0.8833 val_acc= 0.8000 time= 0.0234\n",
      "Epoch: 0071 train_loss= 0.5343 train_acc= 0.9357 val_loss= 0.8744 val_acc= 0.8033 time= 0.0186\n",
      "Epoch: 0072 train_loss= 0.5229 train_acc= 0.9429 val_loss= 0.8636 val_acc= 0.8067 time= 0.0234\n",
      "Epoch: 0073 train_loss= 0.5080 train_acc= 0.9429 val_loss= 0.8538 val_acc= 0.8033 time= 0.0245\n",
      "Epoch: 0074 train_loss= 0.4946 train_acc= 0.9429 val_loss= 0.8440 val_acc= 0.8100 time= 0.0189\n",
      "Epoch: 0075 train_loss= 0.4839 train_acc= 0.9500 val_loss= 0.8349 val_acc= 0.8167 time= 0.0207\n",
      "Epoch: 0076 train_loss= 0.4719 train_acc= 0.9500 val_loss= 0.8257 val_acc= 0.8167 time= 0.0281\n",
      "Epoch: 0077 train_loss= 0.4587 train_acc= 0.9500 val_loss= 0.8175 val_acc= 0.8133 time= 0.0184\n",
      "Epoch: 0078 train_loss= 0.4455 train_acc= 0.9500 val_loss= 0.8085 val_acc= 0.8133 time= 0.0187\n",
      "Epoch: 0079 train_loss= 0.4358 train_acc= 0.9500 val_loss= 0.8016 val_acc= 0.8167 time= 0.0256\n",
      "Epoch: 0080 train_loss= 0.4251 train_acc= 0.9571 val_loss= 0.7934 val_acc= 0.8233 time= 0.0210\n",
      "Epoch: 0081 train_loss= 0.4126 train_acc= 0.9571 val_loss= 0.7843 val_acc= 0.8167 time= 0.0200\n",
      "Epoch: 0082 train_loss= 0.4021 train_acc= 0.9571 val_loss= 0.7772 val_acc= 0.8200 time= 0.0291\n",
      "Epoch: 0083 train_loss= 0.3910 train_acc= 0.9643 val_loss= 0.7716 val_acc= 0.8167 time= 0.0244\n",
      "Epoch: 0084 train_loss= 0.3819 train_acc= 0.9643 val_loss= 0.7644 val_acc= 0.8167 time= 0.0192\n",
      "Epoch: 0085 train_loss= 0.3721 train_acc= 0.9714 val_loss= 0.7580 val_acc= 0.8200 time= 0.0241\n",
      "Epoch: 0086 train_loss= 0.3622 train_acc= 0.9714 val_loss= 0.7514 val_acc= 0.8233 time= 0.0232\n",
      "Epoch: 0087 train_loss= 0.3521 train_acc= 0.9714 val_loss= 0.7476 val_acc= 0.8200 time= 0.0245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0088 train_loss= 0.3431 train_acc= 0.9714 val_loss= 0.7405 val_acc= 0.8233 time= 0.0232\n",
      "Epoch: 0089 train_loss= 0.3345 train_acc= 0.9714 val_loss= 0.7343 val_acc= 0.8233 time= 0.0238\n",
      "Epoch: 0090 train_loss= 0.3264 train_acc= 0.9714 val_loss= 0.7301 val_acc= 0.8233 time= 0.0192\n",
      "Epoch: 0091 train_loss= 0.3179 train_acc= 0.9714 val_loss= 0.7254 val_acc= 0.8233 time= 0.0189\n",
      "Epoch: 0092 train_loss= 0.3093 train_acc= 0.9714 val_loss= 0.7199 val_acc= 0.8233 time= 0.0194\n",
      "Epoch: 0093 train_loss= 0.3021 train_acc= 0.9786 val_loss= 0.7151 val_acc= 0.8233 time= 0.0234\n",
      "Epoch: 0094 train_loss= 0.2931 train_acc= 0.9786 val_loss= 0.7094 val_acc= 0.8233 time= 0.0235\n",
      "Epoch: 0095 train_loss= 0.2860 train_acc= 0.9786 val_loss= 0.7065 val_acc= 0.8300 time= 0.0232\n",
      "Epoch: 0096 train_loss= 0.2780 train_acc= 0.9786 val_loss= 0.7013 val_acc= 0.8267 time= 0.0228\n",
      "Epoch: 0097 train_loss= 0.2719 train_acc= 0.9786 val_loss= 0.6988 val_acc= 0.8267 time= 0.0190\n",
      "Epoch: 0098 train_loss= 0.2655 train_acc= 0.9786 val_loss= 0.6947 val_acc= 0.8267 time= 0.0229\n",
      "Epoch: 0099 train_loss= 0.2577 train_acc= 0.9786 val_loss= 0.6914 val_acc= 0.8233 time= 0.0184\n",
      "Epoch: 0100 train_loss= 0.2518 train_acc= 0.9786 val_loss= 0.6869 val_acc= 0.8233 time= 0.0235\n",
      "Epoch: 0101 train_loss= 0.2441 train_acc= 0.9786 val_loss= 0.6823 val_acc= 0.8300 time= 0.0190\n",
      "Epoch: 0102 train_loss= 0.2382 train_acc= 0.9786 val_loss= 0.6785 val_acc= 0.8300 time= 0.0237\n",
      "Epoch: 0103 train_loss= 0.2327 train_acc= 0.9786 val_loss= 0.6757 val_acc= 0.8267 time= 0.0271\n",
      "Epoch: 0104 train_loss= 0.2269 train_acc= 0.9786 val_loss= 0.6723 val_acc= 0.8267 time= 0.0191\n",
      "Epoch: 0105 train_loss= 0.2202 train_acc= 0.9786 val_loss= 0.6688 val_acc= 0.8267 time= 0.0190\n",
      "Epoch: 0106 train_loss= 0.2149 train_acc= 0.9786 val_loss= 0.6634 val_acc= 0.8267 time= 0.0187\n",
      "Epoch: 0107 train_loss= 0.2106 train_acc= 0.9786 val_loss= 0.6602 val_acc= 0.8267 time= 0.0187\n",
      "Epoch: 0108 train_loss= 0.2043 train_acc= 0.9786 val_loss= 0.6592 val_acc= 0.8267 time= 0.0255\n",
      "Epoch: 0109 train_loss= 0.1997 train_acc= 0.9786 val_loss= 0.6565 val_acc= 0.8267 time= 0.0226\n",
      "Epoch: 0110 train_loss= 0.1951 train_acc= 0.9786 val_loss= 0.6549 val_acc= 0.8300 time= 0.0192\n",
      "Epoch: 0111 train_loss= 0.1901 train_acc= 0.9786 val_loss= 0.6518 val_acc= 0.8300 time= 0.0191\n",
      "Epoch: 0112 train_loss= 0.1859 train_acc= 0.9857 val_loss= 0.6491 val_acc= 0.8300 time= 0.0189\n",
      "Epoch: 0113 train_loss= 0.1810 train_acc= 0.9929 val_loss= 0.6475 val_acc= 0.8333 time= 0.0202\n",
      "Epoch: 0114 train_loss= 0.1763 train_acc= 0.9929 val_loss= 0.6461 val_acc= 0.8300 time= 0.0193\n",
      "Epoch: 0115 train_loss= 0.1725 train_acc= 0.9929 val_loss= 0.6451 val_acc= 0.8300 time= 0.0244\n",
      "Epoch: 0116 train_loss= 0.1691 train_acc= 0.9929 val_loss= 0.6426 val_acc= 0.8267 time= 0.0230\n",
      "Epoch: 0117 train_loss= 0.1639 train_acc= 0.9929 val_loss= 0.6407 val_acc= 0.8267 time= 0.0189\n",
      "Epoch: 0118 train_loss= 0.1600 train_acc= 0.9929 val_loss= 0.6382 val_acc= 0.8300 time= 0.0248\n",
      "Epoch: 0119 train_loss= 0.1559 train_acc= 0.9929 val_loss= 0.6381 val_acc= 0.8267 time= 0.0240\n",
      "Epoch: 0120 train_loss= 0.1524 train_acc= 0.9929 val_loss= 0.6377 val_acc= 0.8300 time= 0.0192\n",
      "Epoch: 0121 train_loss= 0.1498 train_acc= 0.9929 val_loss= 0.6360 val_acc= 0.8267 time= 0.0202\n",
      "Epoch: 0122 train_loss= 0.1463 train_acc= 0.9929 val_loss= 0.6338 val_acc= 0.8300 time= 0.0240\n",
      "Epoch: 0123 train_loss= 0.1423 train_acc= 0.9929 val_loss= 0.6323 val_acc= 0.8267 time= 0.0189\n",
      "Epoch: 0124 train_loss= 0.1394 train_acc= 0.9929 val_loss= 0.6320 val_acc= 0.8300 time= 0.0191\n",
      "Epoch: 0125 train_loss= 0.1358 train_acc= 0.9929 val_loss= 0.6298 val_acc= 0.8267 time= 0.0245\n",
      "Epoch: 0126 train_loss= 0.1330 train_acc= 0.9929 val_loss= 0.6298 val_acc= 0.8267 time= 0.0194\n",
      "Epoch: 0127 train_loss= 0.1306 train_acc= 0.9929 val_loss= 0.6283 val_acc= 0.8267 time= 0.0236\n",
      "Epoch: 0128 train_loss= 0.1266 train_acc= 0.9929 val_loss= 0.6266 val_acc= 0.8267 time= 0.0197\n",
      "Epoch: 0129 train_loss= 0.1243 train_acc= 0.9929 val_loss= 0.6277 val_acc= 0.8267 time= 0.0208\n",
      "Epoch: 0130 train_loss= 0.1224 train_acc= 0.9929 val_loss= 0.6249 val_acc= 0.8267 time= 0.0294\n",
      "Epoch: 0131 train_loss= 0.1185 train_acc= 0.9929 val_loss= 0.6234 val_acc= 0.8267 time= 0.0199\n",
      "Epoch: 0132 train_loss= 0.1160 train_acc= 0.9929 val_loss= 0.6210 val_acc= 0.8267 time= 0.0278\n",
      "Epoch: 0133 train_loss= 0.1141 train_acc= 0.9929 val_loss= 0.6217 val_acc= 0.8267 time= 0.0193\n",
      "Epoch: 0134 train_loss= 0.1113 train_acc= 0.9929 val_loss= 0.6179 val_acc= 0.8267 time= 0.0243\n",
      "Epoch: 0135 train_loss= 0.1092 train_acc= 0.9929 val_loss= 0.6164 val_acc= 0.8267 time= 0.0240\n",
      "Epoch: 0136 train_loss= 0.1069 train_acc= 0.9929 val_loss= 0.6133 val_acc= 0.8300 time= 0.0208\n",
      "Epoch: 0137 train_loss= 0.1047 train_acc= 0.9929 val_loss= 0.6130 val_acc= 0.8267 time= 0.0240\n",
      "Epoch: 0138 train_loss= 0.1029 train_acc= 0.9929 val_loss= 0.6117 val_acc= 0.8267 time= 0.0262\n",
      "Epoch: 0139 train_loss= 0.1001 train_acc= 0.9929 val_loss= 0.6102 val_acc= 0.8300 time= 0.0243\n",
      "Epoch: 0140 train_loss= 0.0987 train_acc= 0.9929 val_loss= 0.6100 val_acc= 0.8267 time= 0.0239\n",
      "Epoch: 0141 train_loss= 0.0967 train_acc= 0.9929 val_loss= 0.6097 val_acc= 0.8267 time= 0.0231\n",
      "Epoch: 0142 train_loss= 0.0953 train_acc= 0.9929 val_loss= 0.6094 val_acc= 0.8267 time= 0.0208\n",
      "Epoch: 0143 train_loss= 0.0929 train_acc= 0.9929 val_loss= 0.6086 val_acc= 0.8267 time= 0.0307\n",
      "Epoch: 0144 train_loss= 0.0908 train_acc= 0.9929 val_loss= 0.6084 val_acc= 0.8267 time= 0.0225\n",
      "Epoch: 0145 train_loss= 0.0892 train_acc= 0.9929 val_loss= 0.6083 val_acc= 0.8267 time= 0.0183\n",
      "Epoch: 0146 train_loss= 0.0874 train_acc= 0.9929 val_loss= 0.6083 val_acc= 0.8267 time= 0.0236\n",
      "Epoch: 0147 train_loss= 0.0857 train_acc= 0.9929 val_loss= 0.6089 val_acc= 0.8300 time= 0.0209\n",
      "Epoch: 0148 train_loss= 0.0850 train_acc= 0.9929 val_loss= 0.6087 val_acc= 0.8300 time= 0.0240\n",
      "Epoch: 0149 train_loss= 0.0821 train_acc= 0.9929 val_loss= 0.6074 val_acc= 0.8300 time= 0.0243\n",
      "Epoch: 0150 train_loss= 0.0809 train_acc= 0.9929 val_loss= 0.6087 val_acc= 0.8300 time= 0.0258\n",
      "Epoch: 0151 train_loss= 0.0799 train_acc= 0.9929 val_loss= 0.6086 val_acc= 0.8300 time= 0.0243\n",
      "Epoch: 0152 train_loss= 0.0780 train_acc= 0.9929 val_loss= 0.6077 val_acc= 0.8300 time= 0.0250\n",
      "Epoch: 0153 train_loss= 0.0764 train_acc= 0.9929 val_loss= 0.6084 val_acc= 0.8233 time= 0.0233\n",
      "Epoch: 0154 train_loss= 0.0752 train_acc= 0.9929 val_loss= 0.6094 val_acc= 0.8300 time= 0.0196\n",
      "Epoch: 0155 train_loss= 0.0739 train_acc= 0.9929 val_loss= 0.6080 val_acc= 0.8300 time= 0.0196\n",
      "Epoch: 0156 train_loss= 0.0726 train_acc= 0.9929 val_loss= 0.6085 val_acc= 0.8267 time= 0.0197\n",
      "Epoch: 0157 train_loss= 0.0711 train_acc= 0.9929 val_loss= 0.6085 val_acc= 0.8267 time= 0.0243\n",
      "Epoch: 0158 train_loss= 0.0697 train_acc= 0.9929 val_loss= 0.6075 val_acc= 0.8267 time= 0.0240\n",
      "Epoch: 0159 train_loss= 0.0681 train_acc= 0.9929 val_loss= 0.6080 val_acc= 0.8300 time= 0.0279\n",
      "Epoch: 0160 train_loss= 0.0677 train_acc= 0.9929 val_loss= 0.6080 val_acc= 0.8333 time= 0.0191\n",
      "Epoch 160: early stopping\n"
     ]
    }
   ],
   "source": [
    "NB_EPOCH = 200\n",
    "PATIENCE = 10  # early stopping patience\n",
    "# Helper variables for main training loop\n",
    "wait = 0\n",
    "preds = None\n",
    "best_val_loss = 99999\n",
    "\n",
    "# Fit\n",
    "for epoch in range(1, NB_EPOCH+1):\n",
    "\n",
    "    # Log wall-clock time\n",
    "    t = time.time()\n",
    "\n",
    "    # Single training iteration (we mask nodes without labels for loss calculation)\n",
    "    model.fit(graph, y_train, sample_weight=train_mask,\n",
    "              batch_size=A.shape[0], epochs=1, shuffle=False, verbose=0)\n",
    "\n",
    "    # Predict on full dataset\n",
    "    preds = model.predict(graph, batch_size=A.shape[0])\n",
    "\n",
    "    # Train / validation scores\n",
    "    train_val_loss, train_val_acc = evaluate_preds(preds, [y_train, y_val],\n",
    "                                                   [idx_train, idx_val])\n",
    "    print(\"Epoch: {:04d}\".format(epoch),\n",
    "          \"train_loss= {:.4f}\".format(train_val_loss[0]),\n",
    "          \"train_acc= {:.4f}\".format(train_val_acc[0]),\n",
    "          \"val_loss= {:.4f}\".format(train_val_loss[1]),\n",
    "          \"val_acc= {:.4f}\".format(train_val_acc[1]),\n",
    "          \"time= {:.4f}\".format(time.time() - t))\n",
    "\n",
    "    # Early stopping\n",
    "    if train_val_loss[1] < best_val_loss:\n",
    "        best_val_loss = train_val_loss[1]\n",
    "        wait = 0\n",
    "    else:\n",
    "        if wait >= PATIENCE:\n",
    "            print('Epoch {}: early stopping'.format(epoch))\n",
    "            break\n",
    "        wait += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.6204\n",
      "accuracy = 0.8160\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate_preds(preds, [y_test], [idx_test])\n",
    "print(f\"\"\"\n",
    "loss = {test_loss[0]:.4f}\n",
    "accuracy = {test_acc[0]:.4f}\n",
    "\"\"\".strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
