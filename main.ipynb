{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the path of execution\n",
    "EXE_PATH = os.path.abspath(os.path.curdir)\n",
    "# the path of the vendor files\n",
    "VENDOR_PATH = os.path.join(EXE_PATH, 'vendor')\n",
    "# the vendors to include in the system path\n",
    "VENDORS = ['keras-gcn']\n",
    "# create the absolute paths for all vendors\n",
    "VENDORS = list(map(lambda x: os.path.join(VENDOR_PATH, x), VENDORS))\n",
    "# update the Python path to include necessary vendor module\n",
    "sys.path += VENDORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kegra.layers.graph import GraphConvolution\n",
    "from kegra.utils import load_data, preprocess_adj, get_splits, evaluate_preds\n",
    "from ggcn import GaussianGraphConvolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "Dataset has 2708 nodes, 5429 edges, 1433 features.\n"
     ]
    }
   ],
   "source": [
    "X, A, y = load_data('data/cora/', dataset='cora')\n",
    "X /= X.sum(1).reshape(-1, 1)\n",
    "A = preprocess_adj(A)\n",
    "y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask = get_splits(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2708x2708 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 13264 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1110 16:46:17.470891 140571322152768 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1110 16:46:17.479721 140571322152768 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:515: The name tf.sparse_placeholder is deprecated. Please use tf.compat.v1.sparse_placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "support = 1\n",
    "graph = [X, A]\n",
    "G = [Input(shape=(None, None), batch_shape=(None, None), sparse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 16:46:17.484791 140571322152768 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_in = Input(shape=(X.shape[1],))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 16:46:17.502196 140571322152768 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1110 16:46:17.506567 140571322152768 deprecation.py:506] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1110 16:46:17.515767 140571322152768 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define model architecture\n",
    "# NOTE: We pass arguments for graph convolutional layers as a list of tensors.\n",
    "# This is somewhat hacky, more elegant options would require rewriting the Layer base class.\n",
    "H = Dropout(0.5)(X_in)\n",
    "H = GraphConvolution(32, support, activation='relu', kernel_regularizer=l2(5e-4))([H]+G)\n",
    "H = Dropout(0.5)(H)\n",
    "Y = GraphConvolution(y.shape[1], support, activation='softmax')([H]+G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 16:46:17.558493 140571322152768 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[X_in]+G, outputs=Y)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 1433)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1433)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_1 (GraphConvo (None, 32)           45888       dropout_1[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32)           0           graph_convolution_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_2 (GraphConvo (None, 7)            231         dropout_2[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 46,119\n",
      "Trainable params: 46,119\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 16:46:17.640092 140571322152768 deprecation.py:323] From /home/bitcommander/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.9307 train_acc= 0.3857 val_loss= 1.9326 val_acc= 0.3700 time= 1.0964\n",
      "Epoch: 0002 train_loss= 1.9139 train_acc= 0.3143 val_loss= 1.9175 val_acc= 0.3533 time= 0.0173\n",
      "Epoch: 0003 train_loss= 1.8957 train_acc= 0.3143 val_loss= 1.9016 val_acc= 0.3533 time= 0.0171\n",
      "Epoch: 0004 train_loss= 1.8761 train_acc= 0.3286 val_loss= 1.8847 val_acc= 0.3600 time= 0.0166\n",
      "Epoch: 0005 train_loss= 1.8554 train_acc= 0.3286 val_loss= 1.8666 val_acc= 0.3600 time= 0.0181\n",
      "Epoch: 0006 train_loss= 1.8348 train_acc= 0.3286 val_loss= 1.8488 val_acc= 0.3600 time= 0.0204\n",
      "Epoch: 0007 train_loss= 1.8144 train_acc= 0.3286 val_loss= 1.8312 val_acc= 0.3567 time= 0.0170\n",
      "Epoch: 0008 train_loss= 1.7942 train_acc= 0.3286 val_loss= 1.8144 val_acc= 0.3567 time= 0.0185\n",
      "Epoch: 0009 train_loss= 1.7749 train_acc= 0.3286 val_loss= 1.7987 val_acc= 0.3567 time= 0.0175\n",
      "Epoch: 0010 train_loss= 1.7573 train_acc= 0.3286 val_loss= 1.7847 val_acc= 0.3567 time= 0.0188\n",
      "Epoch: 0011 train_loss= 1.7413 train_acc= 0.3214 val_loss= 1.7723 val_acc= 0.3533 time= 0.0168\n",
      "Epoch: 0012 train_loss= 1.7269 train_acc= 0.3286 val_loss= 1.7616 val_acc= 0.3533 time= 0.0176\n",
      "Epoch: 0013 train_loss= 1.7138 train_acc= 0.3429 val_loss= 1.7526 val_acc= 0.3600 time= 0.0172\n",
      "Epoch: 0014 train_loss= 1.7017 train_acc= 0.3571 val_loss= 1.7449 val_acc= 0.3633 time= 0.0191\n",
      "Epoch: 0015 train_loss= 1.6899 train_acc= 0.3714 val_loss= 1.7379 val_acc= 0.3633 time= 0.0175\n",
      "Epoch: 0016 train_loss= 1.6779 train_acc= 0.3929 val_loss= 1.7309 val_acc= 0.3633 time= 0.0180\n",
      "Epoch: 0017 train_loss= 1.6653 train_acc= 0.4000 val_loss= 1.7234 val_acc= 0.3667 time= 0.0172\n",
      "Epoch: 0018 train_loss= 1.6520 train_acc= 0.4071 val_loss= 1.7151 val_acc= 0.3733 time= 0.0170\n",
      "Epoch: 0019 train_loss= 1.6379 train_acc= 0.4143 val_loss= 1.7061 val_acc= 0.3767 time= 0.0172\n",
      "Epoch: 0020 train_loss= 1.6233 train_acc= 0.4214 val_loss= 1.6966 val_acc= 0.3833 time= 0.0201\n",
      "Epoch: 0021 train_loss= 1.6084 train_acc= 0.4286 val_loss= 1.6870 val_acc= 0.3900 time= 0.0170\n",
      "Epoch: 0022 train_loss= 1.5930 train_acc= 0.4500 val_loss= 1.6771 val_acc= 0.4067 time= 0.0173\n",
      "Epoch: 0023 train_loss= 1.5780 train_acc= 0.4714 val_loss= 1.6674 val_acc= 0.4100 time= 0.0166\n",
      "Epoch: 0024 train_loss= 1.5631 train_acc= 0.4857 val_loss= 1.6577 val_acc= 0.4467 time= 0.0167\n",
      "Epoch: 0025 train_loss= 1.5481 train_acc= 0.5000 val_loss= 1.6478 val_acc= 0.4800 time= 0.0167\n",
      "Epoch: 0026 train_loss= 1.5329 train_acc= 0.5286 val_loss= 1.6376 val_acc= 0.4967 time= 0.0179\n",
      "Epoch: 0027 train_loss= 1.5173 train_acc= 0.5429 val_loss= 1.6268 val_acc= 0.5033 time= 0.0159\n",
      "Epoch: 0028 train_loss= 1.5012 train_acc= 0.5571 val_loss= 1.6154 val_acc= 0.5067 time= 0.0165\n",
      "Epoch: 0029 train_loss= 1.4845 train_acc= 0.5714 val_loss= 1.6033 val_acc= 0.5067 time= 0.0161\n",
      "Epoch: 0030 train_loss= 1.4675 train_acc= 0.5714 val_loss= 1.5908 val_acc= 0.5067 time= 0.0179\n",
      "Epoch: 0031 train_loss= 1.4507 train_acc= 0.5714 val_loss= 1.5780 val_acc= 0.5067 time= 0.0168\n",
      "Epoch: 0032 train_loss= 1.4339 train_acc= 0.5714 val_loss= 1.5650 val_acc= 0.5033 time= 0.0163\n",
      "Epoch: 0033 train_loss= 1.4173 train_acc= 0.5643 val_loss= 1.5521 val_acc= 0.4967 time= 0.0180\n",
      "Epoch: 0034 train_loss= 1.4009 train_acc= 0.5571 val_loss= 1.5395 val_acc= 0.4900 time= 0.0177\n",
      "Epoch: 0035 train_loss= 1.3845 train_acc= 0.5643 val_loss= 1.5274 val_acc= 0.5000 time= 0.0170\n",
      "Epoch: 0036 train_loss= 1.3680 train_acc= 0.5786 val_loss= 1.5157 val_acc= 0.5067 time= 0.0165\n",
      "Epoch: 0037 train_loss= 1.3514 train_acc= 0.5857 val_loss= 1.5038 val_acc= 0.5167 time= 0.0169\n",
      "Epoch: 0038 train_loss= 1.3347 train_acc= 0.6000 val_loss= 1.4917 val_acc= 0.5400 time= 0.0187\n",
      "Epoch: 0039 train_loss= 1.3179 train_acc= 0.6143 val_loss= 1.4798 val_acc= 0.5567 time= 0.0167\n",
      "Epoch: 0040 train_loss= 1.3011 train_acc= 0.6500 val_loss= 1.4677 val_acc= 0.5700 time= 0.0172\n",
      "Epoch: 0041 train_loss= 1.2843 train_acc= 0.6643 val_loss= 1.4556 val_acc= 0.5800 time= 0.0168\n",
      "Epoch: 0042 train_loss= 1.2674 train_acc= 0.6714 val_loss= 1.4433 val_acc= 0.5833 time= 0.0166\n",
      "Epoch: 0043 train_loss= 1.2506 train_acc= 0.6786 val_loss= 1.4311 val_acc= 0.5933 time= 0.0167\n",
      "Epoch: 0044 train_loss= 1.2339 train_acc= 0.6857 val_loss= 1.4188 val_acc= 0.6067 time= 0.0164\n",
      "Epoch: 0045 train_loss= 1.2172 train_acc= 0.7071 val_loss= 1.4065 val_acc= 0.6167 time= 0.0175\n",
      "Epoch: 0046 train_loss= 1.2007 train_acc= 0.7286 val_loss= 1.3944 val_acc= 0.6200 time= 0.0163\n",
      "Epoch: 0047 train_loss= 1.1843 train_acc= 0.7357 val_loss= 1.3820 val_acc= 0.6300 time= 0.0167\n",
      "Epoch: 0048 train_loss= 1.1681 train_acc= 0.7500 val_loss= 1.3694 val_acc= 0.6433 time= 0.0167\n",
      "Epoch: 0049 train_loss= 1.1522 train_acc= 0.7571 val_loss= 1.3571 val_acc= 0.6433 time= 0.0174\n",
      "Epoch: 0050 train_loss= 1.1366 train_acc= 0.7643 val_loss= 1.3449 val_acc= 0.6500 time= 0.0173\n",
      "Epoch: 0051 train_loss= 1.1210 train_acc= 0.7643 val_loss= 1.3326 val_acc= 0.6533 time= 0.0205\n",
      "Epoch: 0052 train_loss= 1.1055 train_acc= 0.7714 val_loss= 1.3211 val_acc= 0.6600 time= 0.0178\n",
      "Epoch: 0053 train_loss= 1.0902 train_acc= 0.7929 val_loss= 1.3102 val_acc= 0.6600 time= 0.0168\n",
      "Epoch: 0054 train_loss= 1.0751 train_acc= 0.8000 val_loss= 1.2994 val_acc= 0.6767 time= 0.0166\n",
      "Epoch: 0055 train_loss= 1.0600 train_acc= 0.8071 val_loss= 1.2881 val_acc= 0.6933 time= 0.0166\n",
      "Epoch: 0056 train_loss= 1.0454 train_acc= 0.8143 val_loss= 1.2774 val_acc= 0.7067 time= 0.0163\n",
      "Epoch: 0057 train_loss= 1.0310 train_acc= 0.8143 val_loss= 1.2669 val_acc= 0.7133 time= 0.0169\n",
      "Epoch: 0058 train_loss= 1.0165 train_acc= 0.8214 val_loss= 1.2557 val_acc= 0.7167 time= 0.0162\n",
      "Epoch: 0059 train_loss= 1.0022 train_acc= 0.8286 val_loss= 1.2440 val_acc= 0.7200 time= 0.0164\n",
      "Epoch: 0060 train_loss= 0.9881 train_acc= 0.8429 val_loss= 1.2322 val_acc= 0.7233 time= 0.0167\n",
      "Epoch: 0061 train_loss= 0.9744 train_acc= 0.8500 val_loss= 1.2207 val_acc= 0.7333 time= 0.0163\n",
      "Epoch: 0062 train_loss= 0.9612 train_acc= 0.8643 val_loss= 1.2098 val_acc= 0.7433 time= 0.0171\n",
      "Epoch: 0063 train_loss= 0.9486 train_acc= 0.8714 val_loss= 1.1990 val_acc= 0.7500 time= 0.0161\n",
      "Epoch: 0064 train_loss= 0.9359 train_acc= 0.8714 val_loss= 1.1886 val_acc= 0.7533 time= 0.0163\n",
      "Epoch: 0065 train_loss= 0.9229 train_acc= 0.8714 val_loss= 1.1780 val_acc= 0.7500 time= 0.0181\n",
      "Epoch: 0066 train_loss= 0.9102 train_acc= 0.8714 val_loss= 1.1672 val_acc= 0.7500 time= 0.0164\n",
      "Epoch: 0067 train_loss= 0.8982 train_acc= 0.8714 val_loss= 1.1559 val_acc= 0.7500 time= 0.0166\n",
      "Epoch: 0068 train_loss= 0.8864 train_acc= 0.8571 val_loss= 1.1463 val_acc= 0.7567 time= 0.0161\n",
      "Epoch: 0069 train_loss= 0.8749 train_acc= 0.8571 val_loss= 1.1367 val_acc= 0.7633 time= 0.0166\n",
      "Epoch: 0070 train_loss= 0.8636 train_acc= 0.8643 val_loss= 1.1282 val_acc= 0.7667 time= 0.0165\n",
      "Epoch: 0071 train_loss= 0.8523 train_acc= 0.8786 val_loss= 1.1195 val_acc= 0.7667 time= 0.0166\n",
      "Epoch: 0072 train_loss= 0.8414 train_acc= 0.8786 val_loss= 1.1113 val_acc= 0.7600 time= 0.0168\n",
      "Epoch: 0073 train_loss= 0.8310 train_acc= 0.8786 val_loss= 1.1040 val_acc= 0.7600 time= 0.0168\n",
      "Epoch: 0074 train_loss= 0.8212 train_acc= 0.8786 val_loss= 1.0968 val_acc= 0.7600 time= 0.0201\n",
      "Epoch: 0075 train_loss= 0.8122 train_acc= 0.8786 val_loss= 1.0901 val_acc= 0.7633 time= 0.0169\n",
      "Epoch: 0076 train_loss= 0.8029 train_acc= 0.8786 val_loss= 1.0826 val_acc= 0.7633 time= 0.0169\n",
      "Epoch: 0077 train_loss= 0.7931 train_acc= 0.8786 val_loss= 1.0733 val_acc= 0.7633 time= 0.0176\n",
      "Epoch: 0078 train_loss= 0.7834 train_acc= 0.8786 val_loss= 1.0642 val_acc= 0.7733 time= 0.0167\n",
      "Epoch: 0079 train_loss= 0.7737 train_acc= 0.8786 val_loss= 1.0553 val_acc= 0.7800 time= 0.0175\n",
      "Epoch: 0080 train_loss= 0.7647 train_acc= 0.8857 val_loss= 1.0470 val_acc= 0.7833 time= 0.0168\n",
      "Epoch: 0081 train_loss= 0.7562 train_acc= 0.8857 val_loss= 1.0396 val_acc= 0.7833 time= 0.0165\n",
      "Epoch: 0082 train_loss= 0.7479 train_acc= 0.8714 val_loss= 1.0330 val_acc= 0.7900 time= 0.0168\n",
      "Epoch: 0083 train_loss= 0.7386 train_acc= 0.8857 val_loss= 1.0269 val_acc= 0.7733 time= 0.0171\n",
      "Epoch: 0084 train_loss= 0.7293 train_acc= 0.9071 val_loss= 1.0206 val_acc= 0.7767 time= 0.0171\n",
      "Epoch: 0085 train_loss= 0.7206 train_acc= 0.9214 val_loss= 1.0151 val_acc= 0.7867 time= 0.0174\n",
      "Epoch: 0086 train_loss= 0.7129 train_acc= 0.9286 val_loss= 1.0101 val_acc= 0.7867 time= 0.0180\n",
      "Epoch: 0087 train_loss= 0.7058 train_acc= 0.9286 val_loss= 1.0057 val_acc= 0.7900 time= 0.0180\n",
      "Epoch: 0088 train_loss= 0.6989 train_acc= 0.9286 val_loss= 1.0017 val_acc= 0.7933 time= 0.0211\n",
      "Epoch: 0089 train_loss= 0.6914 train_acc= 0.9286 val_loss= 0.9957 val_acc= 0.7933 time= 0.0175\n",
      "Epoch: 0090 train_loss= 0.6832 train_acc= 0.9286 val_loss= 0.9871 val_acc= 0.7900 time= 0.0172\n",
      "Epoch: 0091 train_loss= 0.6755 train_acc= 0.9286 val_loss= 0.9794 val_acc= 0.7900 time= 0.0169\n",
      "Epoch: 0092 train_loss= 0.6685 train_acc= 0.9286 val_loss= 0.9725 val_acc= 0.7867 time= 0.0175\n",
      "Epoch: 0093 train_loss= 0.6629 train_acc= 0.9286 val_loss= 0.9662 val_acc= 0.7833 time= 0.0183\n",
      "Epoch: 0094 train_loss= 0.6574 train_acc= 0.9214 val_loss= 0.9604 val_acc= 0.7833 time= 0.0175\n",
      "Epoch: 0095 train_loss= 0.6506 train_acc= 0.9214 val_loss= 0.9554 val_acc= 0.7900 time= 0.0178\n",
      "Epoch: 0096 train_loss= 0.6436 train_acc= 0.9357 val_loss= 0.9504 val_acc= 0.7900 time= 0.0170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0097 train_loss= 0.6360 train_acc= 0.9357 val_loss= 0.9461 val_acc= 0.7900 time= 0.0203\n",
      "Epoch: 0098 train_loss= 0.6292 train_acc= 0.9357 val_loss= 0.9424 val_acc= 0.8000 time= 0.0189\n",
      "Epoch: 0099 train_loss= 0.6237 train_acc= 0.9357 val_loss= 0.9397 val_acc= 0.8033 time= 0.0165\n",
      "Epoch: 0100 train_loss= 0.6183 train_acc= 0.9357 val_loss= 0.9366 val_acc= 0.8000 time= 0.0170\n",
      "Epoch: 0101 train_loss= 0.6128 train_acc= 0.9357 val_loss= 0.9334 val_acc= 0.8000 time= 0.0172\n",
      "Epoch: 0102 train_loss= 0.6068 train_acc= 0.9357 val_loss= 0.9291 val_acc= 0.7967 time= 0.0175\n",
      "Epoch: 0103 train_loss= 0.6009 train_acc= 0.9357 val_loss= 0.9244 val_acc= 0.8000 time= 0.0173\n",
      "Epoch: 0104 train_loss= 0.5953 train_acc= 0.9357 val_loss= 0.9190 val_acc= 0.8000 time= 0.0176\n",
      "Epoch: 0105 train_loss= 0.5902 train_acc= 0.9357 val_loss= 0.9140 val_acc= 0.7933 time= 0.0167\n",
      "Epoch: 0106 train_loss= 0.5857 train_acc= 0.9357 val_loss= 0.9088 val_acc= 0.7867 time= 0.0168\n",
      "Epoch: 0107 train_loss= 0.5810 train_acc= 0.9357 val_loss= 0.9034 val_acc= 0.7900 time= 0.0172\n",
      "Epoch: 0108 train_loss= 0.5761 train_acc= 0.9429 val_loss= 0.8988 val_acc= 0.7933 time= 0.0170\n",
      "Epoch: 0109 train_loss= 0.5713 train_acc= 0.9429 val_loss= 0.8945 val_acc= 0.7933 time= 0.0181\n",
      "Epoch: 0110 train_loss= 0.5655 train_acc= 0.9429 val_loss= 0.8908 val_acc= 0.7967 time= 0.0166\n",
      "Epoch: 0111 train_loss= 0.5601 train_acc= 0.9429 val_loss= 0.8892 val_acc= 0.7967 time= 0.0178\n",
      "Epoch: 0112 train_loss= 0.5555 train_acc= 0.9429 val_loss= 0.8879 val_acc= 0.8033 time= 0.0173\n",
      "Epoch: 0113 train_loss= 0.5512 train_acc= 0.9500 val_loss= 0.8867 val_acc= 0.8100 time= 0.0180\n",
      "Epoch: 0114 train_loss= 0.5464 train_acc= 0.9500 val_loss= 0.8822 val_acc= 0.8100 time= 0.0169\n",
      "Epoch: 0115 train_loss= 0.5419 train_acc= 0.9429 val_loss= 0.8775 val_acc= 0.8100 time= 0.0169\n",
      "Epoch: 0116 train_loss= 0.5374 train_acc= 0.9500 val_loss= 0.8733 val_acc= 0.8000 time= 0.0159\n",
      "Epoch: 0117 train_loss= 0.5333 train_acc= 0.9500 val_loss= 0.8700 val_acc= 0.8000 time= 0.0167\n",
      "Epoch: 0118 train_loss= 0.5293 train_acc= 0.9500 val_loss= 0.8665 val_acc= 0.8000 time= 0.0167\n",
      "Epoch: 0119 train_loss= 0.5249 train_acc= 0.9500 val_loss= 0.8640 val_acc= 0.7967 time= 0.0161\n",
      "Epoch: 0120 train_loss= 0.5210 train_acc= 0.9571 val_loss= 0.8613 val_acc= 0.8067 time= 0.0188\n",
      "Epoch: 0121 train_loss= 0.5173 train_acc= 0.9500 val_loss= 0.8603 val_acc= 0.8100 time= 0.0176\n",
      "Epoch: 0122 train_loss= 0.5140 train_acc= 0.9500 val_loss= 0.8605 val_acc= 0.8067 time= 0.0176\n",
      "Epoch: 0123 train_loss= 0.5104 train_acc= 0.9500 val_loss= 0.8590 val_acc= 0.8100 time= 0.0187\n",
      "Epoch: 0124 train_loss= 0.5070 train_acc= 0.9500 val_loss= 0.8567 val_acc= 0.8100 time= 0.0185\n",
      "Epoch: 0125 train_loss= 0.5031 train_acc= 0.9571 val_loss= 0.8517 val_acc= 0.8100 time= 0.0162\n",
      "Epoch: 0126 train_loss= 0.4992 train_acc= 0.9643 val_loss= 0.8454 val_acc= 0.8133 time= 0.0177\n",
      "Epoch: 0127 train_loss= 0.4958 train_acc= 0.9643 val_loss= 0.8403 val_acc= 0.8167 time= 0.0176\n",
      "Epoch: 0128 train_loss= 0.4927 train_acc= 0.9643 val_loss= 0.8364 val_acc= 0.8167 time= 0.0167\n",
      "Epoch: 0129 train_loss= 0.4897 train_acc= 0.9714 val_loss= 0.8327 val_acc= 0.8133 time= 0.0168\n",
      "Epoch: 0130 train_loss= 0.4869 train_acc= 0.9643 val_loss= 0.8300 val_acc= 0.8167 time= 0.0191\n",
      "Epoch: 0131 train_loss= 0.4841 train_acc= 0.9643 val_loss= 0.8274 val_acc= 0.8200 time= 0.0204\n",
      "Epoch: 0132 train_loss= 0.4811 train_acc= 0.9643 val_loss= 0.8259 val_acc= 0.8200 time= 0.0173\n",
      "Epoch: 0133 train_loss= 0.4781 train_acc= 0.9643 val_loss= 0.8240 val_acc= 0.8233 time= 0.0174\n",
      "Epoch: 0134 train_loss= 0.4748 train_acc= 0.9643 val_loss= 0.8219 val_acc= 0.8300 time= 0.0174\n",
      "Epoch: 0135 train_loss= 0.4719 train_acc= 0.9643 val_loss= 0.8209 val_acc= 0.8267 time= 0.0173\n",
      "Epoch: 0136 train_loss= 0.4691 train_acc= 0.9643 val_loss= 0.8206 val_acc= 0.8267 time= 0.0172\n",
      "Epoch: 0137 train_loss= 0.4658 train_acc= 0.9714 val_loss= 0.8170 val_acc= 0.8300 time= 0.0170\n",
      "Epoch: 0138 train_loss= 0.4626 train_acc= 0.9714 val_loss= 0.8123 val_acc= 0.8300 time= 0.0185\n",
      "Epoch: 0139 train_loss= 0.4601 train_acc= 0.9714 val_loss= 0.8079 val_acc= 0.8167 time= 0.0170\n",
      "Epoch: 0140 train_loss= 0.4577 train_acc= 0.9714 val_loss= 0.8043 val_acc= 0.8267 time= 0.0171\n",
      "Epoch: 0141 train_loss= 0.4549 train_acc= 0.9714 val_loss= 0.8017 val_acc= 0.8267 time= 0.0174\n",
      "Epoch: 0142 train_loss= 0.4514 train_acc= 0.9714 val_loss= 0.7999 val_acc= 0.8300 time= 0.0169\n",
      "Epoch: 0143 train_loss= 0.4482 train_acc= 0.9714 val_loss= 0.7989 val_acc= 0.8367 time= 0.0172\n",
      "Epoch: 0144 train_loss= 0.4453 train_acc= 0.9714 val_loss= 0.7990 val_acc= 0.8333 time= 0.0173\n",
      "Epoch: 0145 train_loss= 0.4430 train_acc= 0.9714 val_loss= 0.8000 val_acc= 0.8333 time= 0.0189\n",
      "Epoch: 0146 train_loss= 0.4413 train_acc= 0.9714 val_loss= 0.8023 val_acc= 0.8300 time= 0.0177\n",
      "Epoch: 0147 train_loss= 0.4394 train_acc= 0.9714 val_loss= 0.8035 val_acc= 0.8300 time= 0.0185\n",
      "Epoch: 0148 train_loss= 0.4360 train_acc= 0.9714 val_loss= 0.8000 val_acc= 0.8300 time= 0.0172\n",
      "Epoch: 0149 train_loss= 0.4323 train_acc= 0.9714 val_loss= 0.7952 val_acc= 0.8300 time= 0.0172\n",
      "Epoch: 0150 train_loss= 0.4293 train_acc= 0.9714 val_loss= 0.7904 val_acc= 0.8300 time= 0.0182\n",
      "Epoch: 0151 train_loss= 0.4274 train_acc= 0.9714 val_loss= 0.7860 val_acc= 0.8267 time= 0.0169\n",
      "Epoch: 0152 train_loss= 0.4262 train_acc= 0.9714 val_loss= 0.7836 val_acc= 0.8233 time= 0.0171\n",
      "Epoch: 0153 train_loss= 0.4246 train_acc= 0.9714 val_loss= 0.7826 val_acc= 0.8300 time= 0.0169\n",
      "Epoch: 0154 train_loss= 0.4226 train_acc= 0.9714 val_loss= 0.7819 val_acc= 0.8300 time= 0.0172\n",
      "Epoch: 0155 train_loss= 0.4196 train_acc= 0.9714 val_loss= 0.7810 val_acc= 0.8300 time= 0.0172\n",
      "Epoch: 0156 train_loss= 0.4155 train_acc= 0.9714 val_loss= 0.7810 val_acc= 0.8267 time= 0.0182\n",
      "Epoch: 0157 train_loss= 0.4126 train_acc= 0.9714 val_loss= 0.7827 val_acc= 0.8267 time= 0.0174\n",
      "Epoch: 0158 train_loss= 0.4105 train_acc= 0.9714 val_loss= 0.7841 val_acc= 0.8267 time= 0.0170\n",
      "Epoch: 0159 train_loss= 0.4086 train_acc= 0.9714 val_loss= 0.7844 val_acc= 0.8300 time= 0.0171\n",
      "Epoch: 0160 train_loss= 0.4066 train_acc= 0.9714 val_loss= 0.7849 val_acc= 0.8233 time= 0.0167\n",
      "Epoch: 0161 train_loss= 0.4043 train_acc= 0.9714 val_loss= 0.7838 val_acc= 0.8200 time= 0.0164\n",
      "Epoch: 0162 train_loss= 0.4014 train_acc= 0.9714 val_loss= 0.7801 val_acc= 0.8167 time= 0.0168\n",
      "Epoch: 0163 train_loss= 0.3986 train_acc= 0.9714 val_loss= 0.7752 val_acc= 0.8200 time= 0.0171\n",
      "Epoch: 0164 train_loss= 0.3962 train_acc= 0.9714 val_loss= 0.7711 val_acc= 0.8167 time= 0.0197\n",
      "Epoch: 0165 train_loss= 0.3941 train_acc= 0.9714 val_loss= 0.7680 val_acc= 0.8200 time= 0.0185\n",
      "Epoch: 0166 train_loss= 0.3916 train_acc= 0.9714 val_loss= 0.7660 val_acc= 0.8233 time= 0.0171\n",
      "Epoch: 0167 train_loss= 0.3887 train_acc= 0.9714 val_loss= 0.7654 val_acc= 0.8233 time= 0.0168\n",
      "Epoch: 0168 train_loss= 0.3867 train_acc= 0.9714 val_loss= 0.7647 val_acc= 0.8267 time= 0.0174\n",
      "Epoch: 0169 train_loss= 0.3850 train_acc= 0.9714 val_loss= 0.7657 val_acc= 0.8300 time= 0.0183\n",
      "Epoch: 0170 train_loss= 0.3832 train_acc= 0.9714 val_loss= 0.7650 val_acc= 0.8300 time= 0.0171\n",
      "Epoch: 0171 train_loss= 0.3810 train_acc= 0.9714 val_loss= 0.7633 val_acc= 0.8300 time= 0.0171\n",
      "Epoch: 0172 train_loss= 0.3789 train_acc= 0.9714 val_loss= 0.7620 val_acc= 0.8267 time= 0.0174\n",
      "Epoch: 0173 train_loss= 0.3772 train_acc= 0.9714 val_loss= 0.7613 val_acc= 0.8267 time= 0.0172\n",
      "Epoch: 0174 train_loss= 0.3755 train_acc= 0.9714 val_loss= 0.7610 val_acc= 0.8267 time= 0.0165\n",
      "Epoch: 0175 train_loss= 0.3733 train_acc= 0.9714 val_loss= 0.7571 val_acc= 0.8267 time= 0.0166\n",
      "Epoch: 0176 train_loss= 0.3713 train_acc= 0.9714 val_loss= 0.7530 val_acc= 0.8200 time= 0.0160\n",
      "Epoch: 0177 train_loss= 0.3701 train_acc= 0.9714 val_loss= 0.7488 val_acc= 0.8133 time= 0.0160\n",
      "Epoch: 0178 train_loss= 0.3689 train_acc= 0.9714 val_loss= 0.7467 val_acc= 0.8133 time= 0.0163\n",
      "Epoch: 0179 train_loss= 0.3675 train_acc= 0.9714 val_loss= 0.7457 val_acc= 0.8167 time= 0.0162\n",
      "Epoch: 0180 train_loss= 0.3658 train_acc= 0.9714 val_loss= 0.7452 val_acc= 0.8200 time= 0.0179\n",
      "Epoch: 0181 train_loss= 0.3636 train_acc= 0.9714 val_loss= 0.7445 val_acc= 0.8233 time= 0.0177\n",
      "Epoch: 0182 train_loss= 0.3614 train_acc= 0.9714 val_loss= 0.7455 val_acc= 0.8200 time= 0.0170\n",
      "Epoch: 0183 train_loss= 0.3592 train_acc= 0.9714 val_loss= 0.7469 val_acc= 0.8233 time= 0.0184\n",
      "Epoch: 0184 train_loss= 0.3573 train_acc= 0.9714 val_loss= 0.7484 val_acc= 0.8233 time= 0.0178\n",
      "Epoch: 0185 train_loss= 0.3556 train_acc= 0.9786 val_loss= 0.7501 val_acc= 0.8300 time= 0.0172\n",
      "Epoch: 0186 train_loss= 0.3546 train_acc= 0.9786 val_loss= 0.7499 val_acc= 0.8233 time= 0.0170\n",
      "Epoch: 0187 train_loss= 0.3546 train_acc= 0.9786 val_loss= 0.7506 val_acc= 0.8233 time= 0.0171\n",
      "Epoch: 0188 train_loss= 0.3534 train_acc= 0.9786 val_loss= 0.7502 val_acc= 0.8267 time= 0.0170\n",
      "Epoch: 0189 train_loss= 0.3502 train_acc= 0.9786 val_loss= 0.7473 val_acc= 0.8267 time= 0.0186\n",
      "Epoch: 0190 train_loss= 0.3472 train_acc= 0.9786 val_loss= 0.7446 val_acc= 0.8200 time= 0.0193\n",
      "Epoch: 0191 train_loss= 0.3454 train_acc= 0.9786 val_loss= 0.7425 val_acc= 0.8200 time= 0.0187\n",
      "Epoch: 0192 train_loss= 0.3457 train_acc= 0.9714 val_loss= 0.7413 val_acc= 0.8233 time= 0.0204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0193 train_loss= 0.3468 train_acc= 0.9714 val_loss= 0.7433 val_acc= 0.8200 time= 0.0183\n",
      "Epoch: 0194 train_loss= 0.3469 train_acc= 0.9714 val_loss= 0.7439 val_acc= 0.8100 time= 0.0189\n",
      "Epoch: 0195 train_loss= 0.3456 train_acc= 0.9714 val_loss= 0.7419 val_acc= 0.8067 time= 0.0182\n",
      "Epoch: 0196 train_loss= 0.3419 train_acc= 0.9714 val_loss= 0.7373 val_acc= 0.8100 time= 0.0172\n",
      "Epoch: 0197 train_loss= 0.3384 train_acc= 0.9786 val_loss= 0.7336 val_acc= 0.8100 time= 0.0167\n",
      "Epoch: 0198 train_loss= 0.3373 train_acc= 0.9786 val_loss= 0.7318 val_acc= 0.8233 time= 0.0179\n",
      "Epoch: 0199 train_loss= 0.3380 train_acc= 0.9786 val_loss= 0.7321 val_acc= 0.8233 time= 0.0201\n",
      "Epoch: 0200 train_loss= 0.3378 train_acc= 0.9786 val_loss= 0.7342 val_acc= 0.8300 time= 0.0179\n"
     ]
    }
   ],
   "source": [
    "NB_EPOCH = 200\n",
    "PATIENCE = 10  # early stopping patience\n",
    "# Helper variables for main training loop\n",
    "wait = 0\n",
    "preds = None\n",
    "best_val_loss = 99999\n",
    "\n",
    "# Fit\n",
    "for epoch in range(1, NB_EPOCH+1):\n",
    "\n",
    "    # Log wall-clock time\n",
    "    t = time.time()\n",
    "\n",
    "    # Single training iteration (we mask nodes without labels for loss calculation)\n",
    "    model.fit(graph, y_train, sample_weight=train_mask,\n",
    "              batch_size=A.shape[0], epochs=1, shuffle=False, verbose=0)\n",
    "\n",
    "    # Predict on full dataset\n",
    "    preds = model.predict(graph, batch_size=A.shape[0])\n",
    "\n",
    "    # Train / validation scores\n",
    "    train_val_loss, train_val_acc = evaluate_preds(preds, [y_train, y_val],\n",
    "                                                   [idx_train, idx_val])\n",
    "    print(\"Epoch: {:04d}\".format(epoch),\n",
    "          \"train_loss= {:.4f}\".format(train_val_loss[0]),\n",
    "          \"train_acc= {:.4f}\".format(train_val_acc[0]),\n",
    "          \"val_loss= {:.4f}\".format(train_val_loss[1]),\n",
    "          \"val_acc= {:.4f}\".format(train_val_acc[1]),\n",
    "          \"time= {:.4f}\".format(time.time() - t))\n",
    "\n",
    "    # Early stopping\n",
    "    if train_val_loss[1] < best_val_loss:\n",
    "        best_val_loss = train_val_loss[1]\n",
    "        wait = 0\n",
    "    else:\n",
    "        if wait >= PATIENCE:\n",
    "            print('Epoch {}: early stopping'.format(epoch))\n",
    "            break\n",
    "        wait += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.7619\n",
      "accuracy = 0.8280\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate_preds(preds, [y_test], [idx_test])\n",
    "print(f\"\"\"\n",
    "loss = {test_loss[0]:.4f}\n",
    "accuracy = {test_acc[0]:.4f}\n",
    "\"\"\".strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Graph Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 16:46:22.247374 140571322152768 deprecation.py:323] From /home/bitcommander/Desktop/robust-graph-convolutional-networks-against-adversarial-attacks-implementation/ggcn/ggcl.py:163: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "W1110 16:46:22.248535 140571322152768 deprecation.py:323] From /home/bitcommander/.local/lib/python3.7/site-packages/tensorflow/python/ops/distributions/normal.py:160: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    }
   ],
   "source": [
    "H = Dropout(0.5)(X_in)\n",
    "H = GaussianGraphConvolution(16, A.shape[0], \n",
    "    is_first=True, \n",
    "    activation='relu', \n",
    "    kernel_regularizer=l2(5e-4)\n",
    ")([H]+G)\n",
    "Y = GaussianGraphConvolution(y.shape[1], A.shape[0], \n",
    "    is_last=True,\n",
    "    activation='softmax'\n",
    ")(H+G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[X_in]+G, outputs=Y)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 1433)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1433)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_graph_convolution_1 (G [(None, 16), (None,  45856       dropout_3[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_graph_convolution_2 (G (None, 7)            224         gaussian_graph_convolution_1[0][0\n",
      "                                                                 gaussian_graph_convolution_1[0][1\n",
      "                                                                 input_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 46,080\n",
      "Trainable params: 46,080\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.9384 train_acc= 0.4214 val_loss= 1.9405 val_acc= 0.3700 time= 0.3997\n",
      "Epoch: 0002 train_loss= 1.9316 train_acc= 0.4857 val_loss= 1.9346 val_acc= 0.4733 time= 0.0164\n",
      "Epoch: 0003 train_loss= 1.9212 train_acc= 0.6000 val_loss= 1.9283 val_acc= 0.5133 time= 0.0165\n",
      "Epoch: 0004 train_loss= 1.9109 train_acc= 0.6643 val_loss= 1.9207 val_acc= 0.5767 time= 0.0161\n",
      "Epoch: 0005 train_loss= 1.9014 train_acc= 0.7143 val_loss= 1.9134 val_acc= 0.5700 time= 0.0170\n",
      "Epoch: 0006 train_loss= 1.8883 train_acc= 0.6929 val_loss= 1.9039 val_acc= 0.6000 time= 0.0169\n",
      "Epoch: 0007 train_loss= 1.8762 train_acc= 0.7071 val_loss= 1.8952 val_acc= 0.5633 time= 0.0172\n",
      "Epoch: 0008 train_loss= 1.8626 train_acc= 0.7071 val_loss= 1.8867 val_acc= 0.5500 time= 0.0170\n",
      "Epoch: 0009 train_loss= 1.8475 train_acc= 0.7143 val_loss= 1.8758 val_acc= 0.5667 time= 0.0170\n",
      "Epoch: 0010 train_loss= 1.8327 train_acc= 0.7429 val_loss= 1.8630 val_acc= 0.5633 time= 0.0171\n",
      "Epoch: 0011 train_loss= 1.8178 train_acc= 0.7286 val_loss= 1.8537 val_acc= 0.6000 time= 0.0176\n",
      "Epoch: 0012 train_loss= 1.8017 train_acc= 0.7429 val_loss= 1.8401 val_acc= 0.6067 time= 0.0173\n",
      "Epoch: 0013 train_loss= 1.7827 train_acc= 0.7643 val_loss= 1.8281 val_acc= 0.6067 time= 0.0188\n",
      "Epoch: 0014 train_loss= 1.7681 train_acc= 0.7571 val_loss= 1.8146 val_acc= 0.6200 time= 0.0192\n",
      "Epoch: 0015 train_loss= 1.7474 train_acc= 0.7429 val_loss= 1.8007 val_acc= 0.6167 time= 0.0183\n",
      "Epoch: 0016 train_loss= 1.7279 train_acc= 0.7571 val_loss= 1.7884 val_acc= 0.6067 time= 0.0173\n",
      "Epoch: 0017 train_loss= 1.7075 train_acc= 0.7571 val_loss= 1.7747 val_acc= 0.6033 time= 0.0168\n",
      "Epoch: 0018 train_loss= 1.6869 train_acc= 0.7500 val_loss= 1.7577 val_acc= 0.6000 time= 0.0172\n",
      "Epoch: 0019 train_loss= 1.6646 train_acc= 0.7643 val_loss= 1.7420 val_acc= 0.6133 time= 0.0174\n",
      "Epoch: 0020 train_loss= 1.6442 train_acc= 0.7429 val_loss= 1.7284 val_acc= 0.6067 time= 0.0170\n",
      "Epoch: 0021 train_loss= 1.6202 train_acc= 0.7571 val_loss= 1.7129 val_acc= 0.6267 time= 0.0175\n",
      "Epoch: 0022 train_loss= 1.5972 train_acc= 0.7429 val_loss= 1.6945 val_acc= 0.6100 time= 0.0179\n",
      "Epoch: 0023 train_loss= 1.5767 train_acc= 0.7429 val_loss= 1.6761 val_acc= 0.6167 time= 0.0178\n",
      "Epoch: 0024 train_loss= 1.5482 train_acc= 0.7429 val_loss= 1.6595 val_acc= 0.6067 time= 0.0181\n",
      "Epoch: 0025 train_loss= 1.5268 train_acc= 0.7500 val_loss= 1.6412 val_acc= 0.6167 time= 0.0205\n",
      "Epoch: 0026 train_loss= 1.5007 train_acc= 0.7429 val_loss= 1.6226 val_acc= 0.6167 time= 0.0181\n",
      "Epoch: 0027 train_loss= 1.4724 train_acc= 0.7429 val_loss= 1.6042 val_acc= 0.6167 time= 0.0171\n",
      "Epoch: 0028 train_loss= 1.4500 train_acc= 0.7500 val_loss= 1.5858 val_acc= 0.6267 time= 0.0169\n",
      "Epoch: 0029 train_loss= 1.4252 train_acc= 0.7571 val_loss= 1.5680 val_acc= 0.6233 time= 0.0169\n",
      "Epoch: 0030 train_loss= 1.3970 train_acc= 0.7714 val_loss= 1.5491 val_acc= 0.6167 time= 0.0170\n",
      "Epoch: 0031 train_loss= 1.3717 train_acc= 0.7714 val_loss= 1.5281 val_acc= 0.6233 time= 0.0173\n",
      "Epoch: 0032 train_loss= 1.3448 train_acc= 0.7786 val_loss= 1.5092 val_acc= 0.6267 time= 0.0173\n",
      "Epoch: 0033 train_loss= 1.3190 train_acc= 0.7786 val_loss= 1.4900 val_acc= 0.6300 time= 0.0169\n",
      "Epoch: 0034 train_loss= 1.2916 train_acc= 0.8000 val_loss= 1.4702 val_acc= 0.6433 time= 0.0169\n",
      "Epoch: 0035 train_loss= 1.2651 train_acc= 0.8000 val_loss= 1.4504 val_acc= 0.6500 time= 0.0172\n",
      "Epoch: 0036 train_loss= 1.2391 train_acc= 0.8214 val_loss= 1.4305 val_acc= 0.6700 time= 0.0169\n",
      "Epoch: 0037 train_loss= 1.2123 train_acc= 0.8214 val_loss= 1.4114 val_acc= 0.6767 time= 0.0191\n",
      "Epoch: 0038 train_loss= 1.1865 train_acc= 0.8571 val_loss= 1.3914 val_acc= 0.6867 time= 0.0177\n",
      "Epoch: 0039 train_loss= 1.1597 train_acc= 0.8571 val_loss= 1.3714 val_acc= 0.6867 time= 0.0174\n",
      "Epoch: 0040 train_loss= 1.1328 train_acc= 0.8643 val_loss= 1.3524 val_acc= 0.7000 time= 0.0172\n",
      "Epoch: 0041 train_loss= 1.1074 train_acc= 0.8714 val_loss= 1.3330 val_acc= 0.7033 time= 0.0168\n",
      "Epoch: 0042 train_loss= 1.0804 train_acc= 0.8714 val_loss= 1.3132 val_acc= 0.7067 time= 0.0169\n",
      "Epoch: 0043 train_loss= 1.0551 train_acc= 0.8714 val_loss= 1.2935 val_acc= 0.7167 time= 0.0169\n",
      "Epoch: 0044 train_loss= 1.0300 train_acc= 0.8714 val_loss= 1.2745 val_acc= 0.7200 time= 0.0174\n",
      "Epoch: 0045 train_loss= 1.0044 train_acc= 0.8857 val_loss= 1.2548 val_acc= 0.7200 time= 0.0172\n",
      "Epoch: 0046 train_loss= 0.9793 train_acc= 0.8929 val_loss= 1.2359 val_acc= 0.7267 time= 0.0170\n",
      "Epoch: 0047 train_loss= 0.9566 train_acc= 0.9000 val_loss= 1.2169 val_acc= 0.7400 time= 0.0171\n",
      "Epoch: 0048 train_loss= 0.9315 train_acc= 0.9000 val_loss= 1.1984 val_acc= 0.7467 time= 0.0168\n",
      "Epoch: 0049 train_loss= 0.9067 train_acc= 0.9071 val_loss= 1.1794 val_acc= 0.7467 time= 0.0180\n",
      "Epoch: 0050 train_loss= 0.8823 train_acc= 0.9071 val_loss= 1.1608 val_acc= 0.7500 time= 0.0178\n",
      "Epoch: 0051 train_loss= 0.8589 train_acc= 0.9071 val_loss= 1.1434 val_acc= 0.7500 time= 0.0177\n",
      "Epoch: 0052 train_loss= 0.8345 train_acc= 0.9071 val_loss= 1.1255 val_acc= 0.7500 time= 0.0170\n",
      "Epoch: 0053 train_loss= 0.8116 train_acc= 0.9143 val_loss= 1.1078 val_acc= 0.7500 time= 0.0173\n",
      "Epoch: 0054 train_loss= 0.7910 train_acc= 0.9143 val_loss= 1.0891 val_acc= 0.7533 time= 0.0169\n",
      "Epoch: 0055 train_loss= 0.7693 train_acc= 0.9143 val_loss= 1.0719 val_acc= 0.7600 time= 0.0172\n",
      "Epoch: 0056 train_loss= 0.7500 train_acc= 0.9071 val_loss= 1.0550 val_acc= 0.7600 time= 0.0171\n",
      "Epoch: 0057 train_loss= 0.7279 train_acc= 0.9143 val_loss= 1.0386 val_acc= 0.7633 time= 0.0173\n",
      "Epoch: 0058 train_loss= 0.7070 train_acc= 0.9143 val_loss= 1.0232 val_acc= 0.7667 time= 0.0172\n",
      "Epoch: 0059 train_loss= 0.6871 train_acc= 0.9143 val_loss= 1.0087 val_acc= 0.7733 time= 0.0172\n",
      "Epoch: 0060 train_loss= 0.6657 train_acc= 0.9357 val_loss= 0.9919 val_acc= 0.7767 time= 0.0174\n",
      "Epoch: 0061 train_loss= 0.6463 train_acc= 0.9429 val_loss= 0.9772 val_acc= 0.7800 time= 0.0209\n",
      "Epoch: 0062 train_loss= 0.6279 train_acc= 0.9357 val_loss= 0.9619 val_acc= 0.7900 time= 0.0192\n",
      "Epoch: 0063 train_loss= 0.6080 train_acc= 0.9571 val_loss= 0.9481 val_acc= 0.7933 time= 0.0170\n",
      "Epoch: 0064 train_loss= 0.5921 train_acc= 0.9571 val_loss= 0.9340 val_acc= 0.7933 time= 0.0167\n",
      "Epoch: 0065 train_loss= 0.5746 train_acc= 0.9571 val_loss= 0.9207 val_acc= 0.7900 time= 0.0168\n",
      "Epoch: 0066 train_loss= 0.5577 train_acc= 0.9643 val_loss= 0.9065 val_acc= 0.7967 time= 0.0170\n",
      "Epoch: 0067 train_loss= 0.5404 train_acc= 0.9643 val_loss= 0.8930 val_acc= 0.8000 time= 0.0164\n",
      "Epoch: 0068 train_loss= 0.5243 train_acc= 0.9643 val_loss= 0.8820 val_acc= 0.8100 time= 0.0170\n",
      "Epoch: 0069 train_loss= 0.5096 train_acc= 0.9643 val_loss= 0.8703 val_acc= 0.8167 time= 0.0176\n",
      "Epoch: 0070 train_loss= 0.4945 train_acc= 0.9714 val_loss= 0.8583 val_acc= 0.8133 time= 0.0173\n",
      "Epoch: 0071 train_loss= 0.4801 train_acc= 0.9714 val_loss= 0.8461 val_acc= 0.8167 time= 0.0167\n",
      "Epoch: 0072 train_loss= 0.4648 train_acc= 0.9714 val_loss= 0.8358 val_acc= 0.8167 time= 0.0164\n",
      "Epoch: 0073 train_loss= 0.4521 train_acc= 0.9714 val_loss= 0.8254 val_acc= 0.8200 time= 0.0166\n",
      "Epoch: 0074 train_loss= 0.4379 train_acc= 0.9714 val_loss= 0.8142 val_acc= 0.8133 time= 0.0182\n",
      "Epoch: 0075 train_loss= 0.4250 train_acc= 0.9714 val_loss= 0.8055 val_acc= 0.8167 time= 0.0166\n",
      "Epoch: 0076 train_loss= 0.4116 train_acc= 0.9714 val_loss= 0.7966 val_acc= 0.8167 time= 0.0163\n",
      "Epoch: 0077 train_loss= 0.3997 train_acc= 0.9714 val_loss= 0.7866 val_acc= 0.8167 time= 0.0164\n",
      "Epoch: 0078 train_loss= 0.3895 train_acc= 0.9714 val_loss= 0.7781 val_acc= 0.8233 time= 0.0163\n",
      "Epoch: 0079 train_loss= 0.3777 train_acc= 0.9714 val_loss= 0.7703 val_acc= 0.8233 time= 0.0167\n",
      "Epoch: 0080 train_loss= 0.3671 train_acc= 0.9714 val_loss= 0.7627 val_acc= 0.8300 time= 0.0165\n",
      "Epoch: 0081 train_loss= 0.3555 train_acc= 0.9714 val_loss= 0.7551 val_acc= 0.8300 time= 0.0164\n",
      "Epoch: 0082 train_loss= 0.3457 train_acc= 0.9786 val_loss= 0.7473 val_acc= 0.8300 time= 0.0165\n",
      "Epoch: 0083 train_loss= 0.3343 train_acc= 0.9786 val_loss= 0.7414 val_acc= 0.8300 time= 0.0162\n",
      "Epoch: 0084 train_loss= 0.3255 train_acc= 0.9786 val_loss= 0.7337 val_acc= 0.8300 time= 0.0163\n",
      "Epoch: 0085 train_loss= 0.3156 train_acc= 0.9786 val_loss= 0.7276 val_acc= 0.8300 time= 0.0164\n",
      "Epoch: 0086 train_loss= 0.3077 train_acc= 0.9786 val_loss= 0.7217 val_acc= 0.8300 time= 0.0201\n",
      "Epoch: 0087 train_loss= 0.2976 train_acc= 0.9786 val_loss= 0.7146 val_acc= 0.8300 time= 0.0171\n",
      "Epoch: 0088 train_loss= 0.2904 train_acc= 0.9786 val_loss= 0.7095 val_acc= 0.8267 time= 0.0164\n",
      "Epoch: 0089 train_loss= 0.2816 train_acc= 0.9786 val_loss= 0.7039 val_acc= 0.8267 time= 0.0164\n",
      "Epoch: 0090 train_loss= 0.2733 train_acc= 0.9786 val_loss= 0.6979 val_acc= 0.8267 time= 0.0162\n",
      "Epoch: 0091 train_loss= 0.2650 train_acc= 0.9786 val_loss= 0.6938 val_acc= 0.8267 time= 0.0164\n",
      "Epoch: 0092 train_loss= 0.2579 train_acc= 0.9857 val_loss= 0.6887 val_acc= 0.8233 time= 0.0167\n",
      "Epoch: 0093 train_loss= 0.2508 train_acc= 0.9857 val_loss= 0.6834 val_acc= 0.8233 time= 0.0162\n",
      "Epoch: 0094 train_loss= 0.2431 train_acc= 0.9857 val_loss= 0.6795 val_acc= 0.8233 time= 0.0164\n",
      "Epoch: 0095 train_loss= 0.2364 train_acc= 0.9857 val_loss= 0.6743 val_acc= 0.8267 time= 0.0166\n",
      "Epoch: 0096 train_loss= 0.2295 train_acc= 0.9857 val_loss= 0.6708 val_acc= 0.8233 time= 0.0167\n",
      "Epoch: 0097 train_loss= 0.2240 train_acc= 0.9857 val_loss= 0.6651 val_acc= 0.8200 time= 0.0163\n",
      "Epoch: 0098 train_loss= 0.2176 train_acc= 0.9857 val_loss= 0.6628 val_acc= 0.8233 time= 0.0168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0099 train_loss= 0.2121 train_acc= 0.9857 val_loss= 0.6586 val_acc= 0.8200 time= 0.0165\n",
      "Epoch: 0100 train_loss= 0.2063 train_acc= 0.9857 val_loss= 0.6538 val_acc= 0.8200 time= 0.0178\n",
      "Epoch: 0101 train_loss= 0.2013 train_acc= 0.9857 val_loss= 0.6523 val_acc= 0.8200 time= 0.0163\n",
      "Epoch: 0102 train_loss= 0.1948 train_acc= 0.9857 val_loss= 0.6479 val_acc= 0.8200 time= 0.0161\n",
      "Epoch: 0103 train_loss= 0.1906 train_acc= 0.9857 val_loss= 0.6446 val_acc= 0.8200 time= 0.0160\n",
      "Epoch: 0104 train_loss= 0.1852 train_acc= 0.9857 val_loss= 0.6421 val_acc= 0.8233 time= 0.0178\n",
      "Epoch: 0105 train_loss= 0.1804 train_acc= 0.9857 val_loss= 0.6400 val_acc= 0.8200 time= 0.0168\n",
      "Epoch: 0106 train_loss= 0.1758 train_acc= 0.9857 val_loss= 0.6366 val_acc= 0.8167 time= 0.0171\n",
      "Epoch: 0107 train_loss= 0.1711 train_acc= 0.9857 val_loss= 0.6351 val_acc= 0.8200 time= 0.0183\n",
      "Epoch: 0108 train_loss= 0.1676 train_acc= 0.9857 val_loss= 0.6328 val_acc= 0.8167 time= 0.0172\n",
      "Epoch: 0109 train_loss= 0.1625 train_acc= 0.9857 val_loss= 0.6307 val_acc= 0.8200 time= 0.0166\n",
      "Epoch: 0110 train_loss= 0.1587 train_acc= 0.9857 val_loss= 0.6281 val_acc= 0.8133 time= 0.0169\n",
      "Epoch: 0111 train_loss= 0.1552 train_acc= 0.9857 val_loss= 0.6272 val_acc= 0.8167 time= 0.0183\n",
      "Epoch: 0112 train_loss= 0.1512 train_acc= 0.9857 val_loss= 0.6258 val_acc= 0.8200 time= 0.0194\n",
      "Epoch: 0113 train_loss= 0.1482 train_acc= 0.9857 val_loss= 0.6234 val_acc= 0.8133 time= 0.0185\n",
      "Epoch: 0114 train_loss= 0.1441 train_acc= 0.9857 val_loss= 0.6228 val_acc= 0.8233 time= 0.0169\n",
      "Epoch: 0115 train_loss= 0.1405 train_acc= 0.9857 val_loss= 0.6207 val_acc= 0.8167 time= 0.0172\n",
      "Epoch: 0116 train_loss= 0.1370 train_acc= 0.9857 val_loss= 0.6194 val_acc= 0.8167 time= 0.0171\n",
      "Epoch: 0117 train_loss= 0.1338 train_acc= 0.9857 val_loss= 0.6177 val_acc= 0.8200 time= 0.0168\n",
      "Epoch: 0118 train_loss= 0.1302 train_acc= 0.9857 val_loss= 0.6166 val_acc= 0.8167 time= 0.0161\n",
      "Epoch: 0119 train_loss= 0.1275 train_acc= 0.9857 val_loss= 0.6150 val_acc= 0.8167 time= 0.0164\n",
      "Epoch: 0120 train_loss= 0.1248 train_acc= 0.9857 val_loss= 0.6139 val_acc= 0.8233 time= 0.0164\n",
      "Epoch: 0121 train_loss= 0.1217 train_acc= 0.9857 val_loss= 0.6132 val_acc= 0.8167 time= 0.0164\n",
      "Epoch: 0122 train_loss= 0.1186 train_acc= 0.9929 val_loss= 0.6120 val_acc= 0.8167 time= 0.0166\n",
      "Epoch: 0123 train_loss= 0.1164 train_acc= 0.9929 val_loss= 0.6098 val_acc= 0.8133 time= 0.0167\n",
      "Epoch: 0124 train_loss= 0.1142 train_acc= 0.9929 val_loss= 0.6116 val_acc= 0.8167 time= 0.0167\n",
      "Epoch: 0125 train_loss= 0.1117 train_acc= 0.9929 val_loss= 0.6092 val_acc= 0.8133 time= 0.0167\n",
      "Epoch: 0126 train_loss= 0.1087 train_acc= 0.9929 val_loss= 0.6079 val_acc= 0.8167 time= 0.0164\n",
      "Epoch: 0127 train_loss= 0.1058 train_acc= 0.9929 val_loss= 0.6087 val_acc= 0.8167 time= 0.0165\n",
      "Epoch: 0128 train_loss= 0.1032 train_acc= 0.9929 val_loss= 0.6082 val_acc= 0.8167 time= 0.0170\n",
      "Epoch: 0129 train_loss= 0.1019 train_acc= 0.9929 val_loss= 0.6095 val_acc= 0.8167 time= 0.0170\n",
      "Epoch: 0130 train_loss= 0.0990 train_acc= 0.9929 val_loss= 0.6078 val_acc= 0.8167 time= 0.0165\n",
      "Epoch: 0131 train_loss= 0.0977 train_acc= 0.9929 val_loss= 0.6094 val_acc= 0.8133 time= 0.0167\n",
      "Epoch: 0132 train_loss= 0.0958 train_acc= 0.9929 val_loss= 0.6103 val_acc= 0.8133 time= 0.0165\n",
      "Epoch: 0133 train_loss= 0.0946 train_acc= 0.9929 val_loss= 0.6082 val_acc= 0.8200 time= 0.0164\n",
      "Epoch: 0134 train_loss= 0.0914 train_acc= 0.9929 val_loss= 0.6082 val_acc= 0.8100 time= 0.0163\n",
      "Epoch: 0135 train_loss= 0.0898 train_acc= 1.0000 val_loss= 0.6075 val_acc= 0.8133 time= 0.0163\n",
      "Epoch: 0136 train_loss= 0.0882 train_acc= 0.9929 val_loss= 0.6088 val_acc= 0.8133 time= 0.0167\n",
      "Epoch: 0137 train_loss= 0.0859 train_acc= 1.0000 val_loss= 0.6086 val_acc= 0.8200 time= 0.0164\n",
      "Epoch: 0138 train_loss= 0.0842 train_acc= 1.0000 val_loss= 0.6084 val_acc= 0.8167 time= 0.0164\n",
      "Epoch: 0139 train_loss= 0.0828 train_acc= 1.0000 val_loss= 0.6074 val_acc= 0.8200 time= 0.0165\n",
      "Epoch: 0140 train_loss= 0.0812 train_acc= 1.0000 val_loss= 0.6073 val_acc= 0.8200 time= 0.0164\n",
      "Epoch: 0141 train_loss= 0.0801 train_acc= 1.0000 val_loss= 0.6066 val_acc= 0.8167 time= 0.0165\n",
      "Epoch: 0142 train_loss= 0.0789 train_acc= 1.0000 val_loss= 0.6081 val_acc= 0.8200 time= 0.0163\n",
      "Epoch: 0143 train_loss= 0.0763 train_acc= 1.0000 val_loss= 0.6075 val_acc= 0.8200 time= 0.0164\n",
      "Epoch: 0144 train_loss= 0.0756 train_acc= 1.0000 val_loss= 0.6062 val_acc= 0.8233 time= 0.0164\n",
      "Epoch: 0145 train_loss= 0.0748 train_acc= 1.0000 val_loss= 0.6065 val_acc= 0.8200 time= 0.0164\n",
      "Epoch: 0146 train_loss= 0.0725 train_acc= 1.0000 val_loss= 0.6064 val_acc= 0.8233 time= 0.0164\n",
      "Epoch: 0147 train_loss= 0.0716 train_acc= 1.0000 val_loss= 0.6064 val_acc= 0.8233 time= 0.0164\n",
      "Epoch: 0148 train_loss= 0.0709 train_acc= 1.0000 val_loss= 0.6055 val_acc= 0.8233 time= 0.0168\n",
      "Epoch: 0149 train_loss= 0.0687 train_acc= 1.0000 val_loss= 0.6040 val_acc= 0.8267 time= 0.0176\n",
      "Epoch: 0150 train_loss= 0.0670 train_acc= 1.0000 val_loss= 0.6040 val_acc= 0.8267 time= 0.0167\n",
      "Epoch: 0151 train_loss= 0.0665 train_acc= 1.0000 val_loss= 0.6040 val_acc= 0.8233 time= 0.0165\n",
      "Epoch: 0152 train_loss= 0.0652 train_acc= 1.0000 val_loss= 0.6044 val_acc= 0.8233 time= 0.0167\n",
      "Epoch: 0153 train_loss= 0.0641 train_acc= 1.0000 val_loss= 0.6066 val_acc= 0.8200 time= 0.0167\n",
      "Epoch: 0154 train_loss= 0.0626 train_acc= 1.0000 val_loss= 0.6062 val_acc= 0.8233 time= 0.0169\n",
      "Epoch: 0155 train_loss= 0.0615 train_acc= 1.0000 val_loss= 0.6043 val_acc= 0.8267 time= 0.0174\n",
      "Epoch: 0156 train_loss= 0.0607 train_acc= 1.0000 val_loss= 0.6052 val_acc= 0.8233 time= 0.0163\n",
      "Epoch: 0157 train_loss= 0.0597 train_acc= 1.0000 val_loss= 0.6078 val_acc= 0.8233 time= 0.0167\n",
      "Epoch: 0158 train_loss= 0.0588 train_acc= 1.0000 val_loss= 0.6093 val_acc= 0.8267 time= 0.0166\n",
      "Epoch: 0159 train_loss= 0.0578 train_acc= 1.0000 val_loss= 0.6117 val_acc= 0.8233 time= 0.0164\n",
      "Epoch: 0160 train_loss= 0.0567 train_acc= 1.0000 val_loss= 0.6094 val_acc= 0.8267 time= 0.0162\n",
      "Epoch: 0161 train_loss= 0.0555 train_acc= 1.0000 val_loss= 0.6097 val_acc= 0.8233 time= 0.0161\n",
      "Epoch 161: early stopping\n"
     ]
    }
   ],
   "source": [
    "NB_EPOCH = 200\n",
    "PATIENCE = 10  # early stopping patience\n",
    "# Helper variables for main training loop\n",
    "wait = 0\n",
    "preds = None\n",
    "best_val_loss = 99999\n",
    "\n",
    "# Fit\n",
    "for epoch in range(1, NB_EPOCH+1):\n",
    "\n",
    "    # Log wall-clock time\n",
    "    t = time.time()\n",
    "\n",
    "    # Single training iteration (we mask nodes without labels for loss calculation)\n",
    "    model.fit(graph, y_train, sample_weight=train_mask,\n",
    "              batch_size=A.shape[0], epochs=1, shuffle=False, verbose=0)\n",
    "\n",
    "    # Predict on full dataset\n",
    "    preds = model.predict(graph, batch_size=A.shape[0])\n",
    "\n",
    "    # Train / validation scores\n",
    "    train_val_loss, train_val_acc = evaluate_preds(preds, [y_train, y_val],\n",
    "                                                   [idx_train, idx_val])\n",
    "    print(\"Epoch: {:04d}\".format(epoch),\n",
    "          \"train_loss= {:.4f}\".format(train_val_loss[0]),\n",
    "          \"train_acc= {:.4f}\".format(train_val_acc[0]),\n",
    "          \"val_loss= {:.4f}\".format(train_val_loss[1]),\n",
    "          \"val_acc= {:.4f}\".format(train_val_acc[1]),\n",
    "          \"time= {:.4f}\".format(time.time() - t))\n",
    "\n",
    "    # Early stopping\n",
    "    if train_val_loss[1] < best_val_loss:\n",
    "        best_val_loss = train_val_loss[1]\n",
    "        wait = 0\n",
    "    else:\n",
    "        if wait >= PATIENCE:\n",
    "            print('Epoch {}: early stopping'.format(epoch))\n",
    "            break\n",
    "        wait += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.6309\n",
      "accuracy = 0.8090\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate_preds(preds, [y_test], [idx_test])\n",
    "print(f\"\"\"\n",
    "loss = {test_loss[0]:.4f}\n",
    "accuracy = {test_acc[0]:.4f}\n",
    "\"\"\".strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
