{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the path of execution\n",
    "EXE_PATH = os.path.abspath(os.path.curdir)\n",
    "# the path of the vendor files\n",
    "VENDOR_PATH = os.path.join(EXE_PATH, 'vendor')\n",
    "# the vendors to include in the system path\n",
    "VENDORS = ['keras-gcn']\n",
    "# create the absolute paths for all vendors\n",
    "VENDORS = list(map(lambda x: os.path.join(VENDOR_PATH, x), VENDORS))\n",
    "# update the Python path to include necessary vendor module\n",
    "sys.path += VENDORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kegra.layers.graph import GraphConvolution\n",
    "from kegra.utils import load_data, preprocess_adj, get_splits, evaluate_preds\n",
    "from ggcn import GaussianGraphConvolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "Dataset has 2708 nodes, 5429 edges, 1433 features.\n"
     ]
    }
   ],
   "source": [
    "X, A, y = load_data('data/cora/', dataset='cora')\n",
    "X /= X.sum(1).reshape(-1, 1)\n",
    "A = preprocess_adj(A)\n",
    "y_train, y_val, y_test, idx_train, idx_val, idx_test, train_mask = get_splits(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2708x2708 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 13264 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1110 17:24:54.845210 140065343878976 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1110 17:24:54.852625 140065343878976 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:515: The name tf.sparse_placeholder is deprecated. Please use tf.compat.v1.sparse_placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "support = 1\n",
    "graph = [X, A]\n",
    "G = [Input(shape=(None, None), batch_shape=(None, None), sparse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 17:24:54.858213 140065343878976 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_in = Input(shape=(X.shape[1],))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 17:24:54.863711 140065343878976 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1110 17:24:54.868380 140065343878976 deprecation.py:506] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1110 17:24:54.876400 140065343878976 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define model architecture\n",
    "# NOTE: We pass arguments for graph convolutional layers as a list of tensors.\n",
    "# This is somewhat hacky, more elegant options would require rewriting the Layer base class.\n",
    "H = Dropout(0.5)(X_in)\n",
    "H = GraphConvolution(32, support, activation='relu', kernel_regularizer=l2(5e-4))([H]+G)\n",
    "H = Dropout(0.5)(H)\n",
    "Y = GraphConvolution(y.shape[1], support, activation='softmax')([H]+G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 17:24:54.917336 140065343878976 deprecation_wrapper.py:119] From /home/bitcommander/.local/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[X_in]+G, outputs=Y)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 1433)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1433)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_1 (GraphConvo (None, 32)           45888       dropout_1[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32)           0           graph_convolution_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_2 (GraphConvo (None, 7)            231         dropout_2[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 46,119\n",
      "Trainable params: 46,119\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 17:24:54.991344 140065343878976 deprecation.py:323] From /home/bitcommander/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.9261 train_acc= 0.3000 val_loss= 1.9281 val_acc= 0.3500 time= 1.1037\n",
      "Epoch: 0002 train_loss= 1.9055 train_acc= 0.2929 val_loss= 1.9096 val_acc= 0.3500 time= 0.0172\n",
      "Epoch: 0003 train_loss= 1.8836 train_acc= 0.2929 val_loss= 1.8904 val_acc= 0.3500 time= 0.0187\n",
      "Epoch: 0004 train_loss= 1.8608 train_acc= 0.2929 val_loss= 1.8707 val_acc= 0.3500 time= 0.0224\n",
      "Epoch: 0005 train_loss= 1.8378 train_acc= 0.3000 val_loss= 1.8513 val_acc= 0.3500 time= 0.0187\n",
      "Epoch: 0006 train_loss= 1.8157 train_acc= 0.3000 val_loss= 1.8329 val_acc= 0.3500 time= 0.0186\n",
      "Epoch: 0007 train_loss= 1.7948 train_acc= 0.3000 val_loss= 1.8156 val_acc= 0.3500 time= 0.0218\n",
      "Epoch: 0008 train_loss= 1.7755 train_acc= 0.3000 val_loss= 1.7999 val_acc= 0.3500 time= 0.0202\n",
      "Epoch: 0009 train_loss= 1.7580 train_acc= 0.3000 val_loss= 1.7860 val_acc= 0.3500 time= 0.0190\n",
      "Epoch: 0010 train_loss= 1.7419 train_acc= 0.3000 val_loss= 1.7736 val_acc= 0.3500 time= 0.0237\n",
      "Epoch: 0011 train_loss= 1.7277 train_acc= 0.3000 val_loss= 1.7629 val_acc= 0.3500 time= 0.0183\n",
      "Epoch: 0012 train_loss= 1.7146 train_acc= 0.3143 val_loss= 1.7536 val_acc= 0.3500 time= 0.0178\n",
      "Epoch: 0013 train_loss= 1.7020 train_acc= 0.3357 val_loss= 1.7451 val_acc= 0.3567 time= 0.0237\n",
      "Epoch: 0014 train_loss= 1.6892 train_acc= 0.3429 val_loss= 1.7368 val_acc= 0.3633 time= 0.0194\n",
      "Epoch: 0015 train_loss= 1.6760 train_acc= 0.3714 val_loss= 1.7285 val_acc= 0.3633 time= 0.0193\n",
      "Epoch: 0016 train_loss= 1.6621 train_acc= 0.4000 val_loss= 1.7197 val_acc= 0.3667 time= 0.0194\n",
      "Epoch: 0017 train_loss= 1.6475 train_acc= 0.4071 val_loss= 1.7103 val_acc= 0.3800 time= 0.0198\n",
      "Epoch: 0018 train_loss= 1.6326 train_acc= 0.4214 val_loss= 1.7007 val_acc= 0.3900 time= 0.0187\n",
      "Epoch: 0019 train_loss= 1.6174 train_acc= 0.4357 val_loss= 1.6908 val_acc= 0.4033 time= 0.0193\n",
      "Epoch: 0020 train_loss= 1.6021 train_acc= 0.4500 val_loss= 1.6809 val_acc= 0.4100 time= 0.0200\n",
      "Epoch: 0021 train_loss= 1.5867 train_acc= 0.4571 val_loss= 1.6707 val_acc= 0.4200 time= 0.0194\n",
      "Epoch: 0022 train_loss= 1.5714 train_acc= 0.4571 val_loss= 1.6604 val_acc= 0.4233 time= 0.0187\n",
      "Epoch: 0023 train_loss= 1.5565 train_acc= 0.4571 val_loss= 1.6501 val_acc= 0.4300 time= 0.0179\n",
      "Epoch: 0024 train_loss= 1.5421 train_acc= 0.4571 val_loss= 1.6401 val_acc= 0.4367 time= 0.0199\n",
      "Epoch: 0025 train_loss= 1.5276 train_acc= 0.4643 val_loss= 1.6299 val_acc= 0.4367 time= 0.0231\n",
      "Epoch: 0026 train_loss= 1.5128 train_acc= 0.4643 val_loss= 1.6195 val_acc= 0.4400 time= 0.0193\n",
      "Epoch: 0027 train_loss= 1.4976 train_acc= 0.4643 val_loss= 1.6084 val_acc= 0.4400 time= 0.0199\n",
      "Epoch: 0028 train_loss= 1.4822 train_acc= 0.4643 val_loss= 1.5972 val_acc= 0.4400 time= 0.0188\n",
      "Epoch: 0029 train_loss= 1.4666 train_acc= 0.4643 val_loss= 1.5857 val_acc= 0.4367 time= 0.0186\n",
      "Epoch: 0030 train_loss= 1.4512 train_acc= 0.4714 val_loss= 1.5742 val_acc= 0.4400 time= 0.0206\n",
      "Epoch: 0031 train_loss= 1.4359 train_acc= 0.4714 val_loss= 1.5629 val_acc= 0.4333 time= 0.0250\n",
      "Epoch: 0032 train_loss= 1.4208 train_acc= 0.4714 val_loss= 1.5518 val_acc= 0.4433 time= 0.0240\n",
      "Epoch: 0033 train_loss= 1.4060 train_acc= 0.4714 val_loss= 1.5410 val_acc= 0.4433 time= 0.0230\n",
      "Epoch: 0034 train_loss= 1.3914 train_acc= 0.4857 val_loss= 1.5305 val_acc= 0.4467 time= 0.0206\n",
      "Epoch: 0035 train_loss= 1.3768 train_acc= 0.4929 val_loss= 1.5201 val_acc= 0.4467 time= 0.0189\n",
      "Epoch: 0036 train_loss= 1.3621 train_acc= 0.5000 val_loss= 1.5097 val_acc= 0.4533 time= 0.0191\n",
      "Epoch: 0037 train_loss= 1.3474 train_acc= 0.5071 val_loss= 1.4995 val_acc= 0.4600 time= 0.0248\n",
      "Epoch: 0038 train_loss= 1.3326 train_acc= 0.5143 val_loss= 1.4893 val_acc= 0.4733 time= 0.0193\n",
      "Epoch: 0039 train_loss= 1.3179 train_acc= 0.5286 val_loss= 1.4793 val_acc= 0.4967 time= 0.0242\n",
      "Epoch: 0040 train_loss= 1.3034 train_acc= 0.5429 val_loss= 1.4692 val_acc= 0.5067 time= 0.0188\n",
      "Epoch: 0041 train_loss= 1.2892 train_acc= 0.5643 val_loss= 1.4595 val_acc= 0.5167 time= 0.0240\n",
      "Epoch: 0042 train_loss= 1.2752 train_acc= 0.5786 val_loss= 1.4497 val_acc= 0.5200 time= 0.0187\n",
      "Epoch: 0043 train_loss= 1.2612 train_acc= 0.5929 val_loss= 1.4397 val_acc= 0.5367 time= 0.0198\n",
      "Epoch: 0044 train_loss= 1.2473 train_acc= 0.6000 val_loss= 1.4295 val_acc= 0.5500 time= 0.0196\n",
      "Epoch: 0045 train_loss= 1.2334 train_acc= 0.6214 val_loss= 1.4192 val_acc= 0.5533 time= 0.0193\n",
      "Epoch: 0046 train_loss= 1.2196 train_acc= 0.6429 val_loss= 1.4089 val_acc= 0.5567 time= 0.0228\n",
      "Epoch: 0047 train_loss= 1.2057 train_acc= 0.6643 val_loss= 1.3981 val_acc= 0.5567 time= 0.0232\n",
      "Epoch: 0048 train_loss= 1.1920 train_acc= 0.6643 val_loss= 1.3874 val_acc= 0.5567 time= 0.0243\n",
      "Epoch: 0049 train_loss= 1.1784 train_acc= 0.6643 val_loss= 1.3768 val_acc= 0.5600 time= 0.0195\n",
      "Epoch: 0050 train_loss= 1.1650 train_acc= 0.7000 val_loss= 1.3664 val_acc= 0.5733 time= 0.0239\n",
      "Epoch: 0051 train_loss= 1.1516 train_acc= 0.7071 val_loss= 1.3560 val_acc= 0.5800 time= 0.0237\n",
      "Epoch: 0052 train_loss= 1.1382 train_acc= 0.7214 val_loss= 1.3458 val_acc= 0.5867 time= 0.0181\n",
      "Epoch: 0053 train_loss= 1.1251 train_acc= 0.7286 val_loss= 1.3358 val_acc= 0.6067 time= 0.0192\n",
      "Epoch: 0054 train_loss= 1.1124 train_acc= 0.7214 val_loss= 1.3257 val_acc= 0.6100 time= 0.0283\n",
      "Epoch: 0055 train_loss= 1.1002 train_acc= 0.7214 val_loss= 1.3156 val_acc= 0.6133 time= 0.0183\n",
      "Epoch: 0056 train_loss= 1.0879 train_acc= 0.7429 val_loss= 1.3059 val_acc= 0.6200 time= 0.0168\n",
      "Epoch: 0057 train_loss= 1.0756 train_acc= 0.7500 val_loss= 1.2963 val_acc= 0.6233 time= 0.0194\n",
      "Epoch: 0058 train_loss= 1.0632 train_acc= 0.7929 val_loss= 1.2868 val_acc= 0.6533 time= 0.0240\n",
      "Epoch: 0059 train_loss= 1.0509 train_acc= 0.8071 val_loss= 1.2776 val_acc= 0.6667 time= 0.0235\n",
      "Epoch: 0060 train_loss= 1.0392 train_acc= 0.8286 val_loss= 1.2690 val_acc= 0.6867 time= 0.0235\n",
      "Epoch: 0061 train_loss= 1.0277 train_acc= 0.8500 val_loss= 1.2606 val_acc= 0.7133 time= 0.0234\n",
      "Epoch: 0062 train_loss= 1.0157 train_acc= 0.8714 val_loss= 1.2517 val_acc= 0.7233 time= 0.0191\n",
      "Epoch: 0063 train_loss= 1.0034 train_acc= 0.8786 val_loss= 1.2423 val_acc= 0.7233 time= 0.0191\n",
      "Epoch: 0064 train_loss= 0.9911 train_acc= 0.8786 val_loss= 1.2326 val_acc= 0.7167 time= 0.0192\n",
      "Epoch: 0065 train_loss= 0.9788 train_acc= 0.8857 val_loss= 1.2225 val_acc= 0.7167 time= 0.0199\n",
      "Epoch: 0066 train_loss= 0.9667 train_acc= 0.8857 val_loss= 1.2129 val_acc= 0.7200 time= 0.0195\n",
      "Epoch: 0067 train_loss= 0.9551 train_acc= 0.8857 val_loss= 1.2034 val_acc= 0.7133 time= 0.0260\n",
      "Epoch: 0068 train_loss= 0.9441 train_acc= 0.8857 val_loss= 1.1943 val_acc= 0.7200 time= 0.0244\n",
      "Epoch: 0069 train_loss= 0.9328 train_acc= 0.8857 val_loss= 1.1853 val_acc= 0.7167 time= 0.0294\n",
      "Epoch: 0070 train_loss= 0.9214 train_acc= 0.8857 val_loss= 1.1762 val_acc= 0.7233 time= 0.0198\n",
      "Epoch: 0071 train_loss= 0.9098 train_acc= 0.8857 val_loss= 1.1669 val_acc= 0.7267 time= 0.0292\n",
      "Epoch: 0072 train_loss= 0.8979 train_acc= 0.8857 val_loss= 1.1579 val_acc= 0.7300 time= 0.0308\n",
      "Epoch: 0073 train_loss= 0.8865 train_acc= 0.8857 val_loss= 1.1493 val_acc= 0.7333 time= 0.0183\n",
      "Epoch: 0074 train_loss= 0.8755 train_acc= 0.8857 val_loss= 1.1411 val_acc= 0.7467 time= 0.0181\n",
      "Epoch: 0075 train_loss= 0.8646 train_acc= 0.8857 val_loss= 1.1329 val_acc= 0.7567 time= 0.0181\n",
      "Epoch: 0076 train_loss= 0.8537 train_acc= 0.8929 val_loss= 1.1243 val_acc= 0.7633 time= 0.0183\n",
      "Epoch: 0077 train_loss= 0.8427 train_acc= 0.9071 val_loss= 1.1159 val_acc= 0.7667 time= 0.0190\n",
      "Epoch: 0078 train_loss= 0.8321 train_acc= 0.9071 val_loss= 1.1073 val_acc= 0.7667 time= 0.0191\n",
      "Epoch: 0079 train_loss= 0.8215 train_acc= 0.9071 val_loss= 1.0976 val_acc= 0.7667 time= 0.0196\n",
      "Epoch: 0080 train_loss= 0.8112 train_acc= 0.9071 val_loss= 1.0882 val_acc= 0.7667 time= 0.0189\n",
      "Epoch: 0081 train_loss= 0.8012 train_acc= 0.9071 val_loss= 1.0794 val_acc= 0.7700 time= 0.0178\n",
      "Epoch: 0082 train_loss= 0.7915 train_acc= 0.9143 val_loss= 1.0712 val_acc= 0.7700 time= 0.0231\n",
      "Epoch: 0083 train_loss= 0.7821 train_acc= 0.9143 val_loss= 1.0629 val_acc= 0.7700 time= 0.0253\n",
      "Epoch: 0084 train_loss= 0.7728 train_acc= 0.9143 val_loss= 1.0553 val_acc= 0.7667 time= 0.0272\n",
      "Epoch: 0085 train_loss= 0.7633 train_acc= 0.9143 val_loss= 1.0473 val_acc= 0.7733 time= 0.0200\n",
      "Epoch: 0086 train_loss= 0.7540 train_acc= 0.9214 val_loss= 1.0401 val_acc= 0.7733 time= 0.0195\n",
      "Epoch: 0087 train_loss= 0.7448 train_acc= 0.9214 val_loss= 1.0329 val_acc= 0.7733 time= 0.0202\n",
      "Epoch: 0088 train_loss= 0.7362 train_acc= 0.9214 val_loss= 1.0267 val_acc= 0.7733 time= 0.0192\n",
      "Epoch: 0089 train_loss= 0.7281 train_acc= 0.9286 val_loss= 1.0215 val_acc= 0.7833 time= 0.0200\n",
      "Epoch: 0090 train_loss= 0.7202 train_acc= 0.9286 val_loss= 1.0165 val_acc= 0.7867 time= 0.0254\n",
      "Epoch: 0091 train_loss= 0.7124 train_acc= 0.9286 val_loss= 1.0109 val_acc= 0.7900 time= 0.0245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0092 train_loss= 0.7048 train_acc= 0.9286 val_loss= 1.0047 val_acc= 0.7900 time= 0.0245\n",
      "Epoch: 0093 train_loss= 0.6973 train_acc= 0.9214 val_loss= 0.9977 val_acc= 0.7867 time= 0.0248\n",
      "Epoch: 0094 train_loss= 0.6905 train_acc= 0.9214 val_loss= 0.9916 val_acc= 0.7800 time= 0.0189\n",
      "Epoch: 0095 train_loss= 0.6841 train_acc= 0.9214 val_loss= 0.9861 val_acc= 0.7767 time= 0.0192\n",
      "Epoch: 0096 train_loss= 0.6779 train_acc= 0.9214 val_loss= 0.9804 val_acc= 0.7767 time= 0.0185\n",
      "Epoch: 0097 train_loss= 0.6712 train_acc= 0.9286 val_loss= 0.9741 val_acc= 0.7767 time= 0.0240\n",
      "Epoch: 0098 train_loss= 0.6638 train_acc= 0.9286 val_loss= 0.9678 val_acc= 0.7800 time= 0.0181\n",
      "Epoch: 0099 train_loss= 0.6563 train_acc= 0.9286 val_loss= 0.9619 val_acc= 0.7800 time= 0.0231\n",
      "Epoch: 0100 train_loss= 0.6497 train_acc= 0.9286 val_loss= 0.9572 val_acc= 0.7767 time= 0.0224\n",
      "Epoch: 0101 train_loss= 0.6442 train_acc= 0.9357 val_loss= 0.9539 val_acc= 0.7833 time= 0.0246\n",
      "Epoch: 0102 train_loss= 0.6387 train_acc= 0.9357 val_loss= 0.9506 val_acc= 0.7867 time= 0.0196\n",
      "Epoch: 0103 train_loss= 0.6331 train_acc= 0.9357 val_loss= 0.9473 val_acc= 0.7867 time= 0.0243\n",
      "Epoch: 0104 train_loss= 0.6270 train_acc= 0.9357 val_loss= 0.9430 val_acc= 0.7867 time= 0.0194\n",
      "Epoch: 0105 train_loss= 0.6207 train_acc= 0.9357 val_loss= 0.9383 val_acc= 0.7833 time= 0.0193\n",
      "Epoch: 0106 train_loss= 0.6142 train_acc= 0.9357 val_loss= 0.9326 val_acc= 0.7833 time= 0.0218\n",
      "Epoch: 0107 train_loss= 0.6087 train_acc= 0.9357 val_loss= 0.9274 val_acc= 0.7767 time= 0.0295\n",
      "Epoch: 0108 train_loss= 0.6033 train_acc= 0.9357 val_loss= 0.9236 val_acc= 0.7800 time= 0.0206\n",
      "Epoch: 0109 train_loss= 0.5980 train_acc= 0.9357 val_loss= 0.9202 val_acc= 0.7800 time= 0.0177\n",
      "Epoch: 0110 train_loss= 0.5927 train_acc= 0.9429 val_loss= 0.9160 val_acc= 0.7800 time= 0.0243\n",
      "Epoch: 0111 train_loss= 0.5874 train_acc= 0.9429 val_loss= 0.9116 val_acc= 0.7800 time= 0.0245\n",
      "Epoch: 0112 train_loss= 0.5820 train_acc= 0.9500 val_loss= 0.9076 val_acc= 0.7833 time= 0.0182\n",
      "Epoch: 0113 train_loss= 0.5769 train_acc= 0.9500 val_loss= 0.9039 val_acc= 0.7900 time= 0.0205\n",
      "Epoch: 0114 train_loss= 0.5723 train_acc= 0.9500 val_loss= 0.8993 val_acc= 0.7900 time= 0.0240\n",
      "Epoch: 0115 train_loss= 0.5684 train_acc= 0.9429 val_loss= 0.8952 val_acc= 0.7933 time= 0.0202\n",
      "Epoch: 0116 train_loss= 0.5644 train_acc= 0.9429 val_loss= 0.8919 val_acc= 0.7933 time= 0.0173\n",
      "Epoch: 0117 train_loss= 0.5600 train_acc= 0.9429 val_loss= 0.8885 val_acc= 0.7933 time= 0.0249\n",
      "Epoch: 0118 train_loss= 0.5553 train_acc= 0.9429 val_loss= 0.8863 val_acc= 0.8000 time= 0.0197\n",
      "Epoch: 0119 train_loss= 0.5517 train_acc= 0.9429 val_loss= 0.8843 val_acc= 0.7967 time= 0.0251\n",
      "Epoch: 0120 train_loss= 0.5484 train_acc= 0.9429 val_loss= 0.8822 val_acc= 0.7933 time= 0.0234\n",
      "Epoch: 0121 train_loss= 0.5443 train_acc= 0.9429 val_loss= 0.8782 val_acc= 0.7967 time= 0.0289\n",
      "Epoch: 0122 train_loss= 0.5402 train_acc= 0.9429 val_loss= 0.8729 val_acc= 0.7967 time= 0.0188\n",
      "Epoch: 0123 train_loss= 0.5369 train_acc= 0.9429 val_loss= 0.8690 val_acc= 0.7833 time= 0.0186\n",
      "Epoch: 0124 train_loss= 0.5339 train_acc= 0.9429 val_loss= 0.8655 val_acc= 0.7833 time= 0.0180\n",
      "Epoch: 0125 train_loss= 0.5306 train_acc= 0.9500 val_loss= 0.8627 val_acc= 0.7900 time= 0.0179\n",
      "Epoch: 0126 train_loss= 0.5266 train_acc= 0.9500 val_loss= 0.8602 val_acc= 0.7867 time= 0.0243\n",
      "Epoch: 0127 train_loss= 0.5221 train_acc= 0.9500 val_loss= 0.8579 val_acc= 0.7833 time= 0.0195\n",
      "Epoch: 0128 train_loss= 0.5178 train_acc= 0.9500 val_loss= 0.8559 val_acc= 0.7867 time= 0.0241\n",
      "Epoch: 0129 train_loss= 0.5140 train_acc= 0.9500 val_loss= 0.8537 val_acc= 0.7867 time= 0.0293\n",
      "Epoch: 0130 train_loss= 0.5106 train_acc= 0.9500 val_loss= 0.8516 val_acc= 0.7867 time= 0.0211\n",
      "Epoch: 0131 train_loss= 0.5078 train_acc= 0.9500 val_loss= 0.8504 val_acc= 0.7900 time= 0.0200\n",
      "Epoch: 0132 train_loss= 0.5053 train_acc= 0.9429 val_loss= 0.8487 val_acc= 0.8000 time= 0.0180\n",
      "Epoch: 0133 train_loss= 0.5027 train_acc= 0.9500 val_loss= 0.8479 val_acc= 0.8033 time= 0.0187\n",
      "Epoch: 0134 train_loss= 0.4994 train_acc= 0.9571 val_loss= 0.8451 val_acc= 0.7967 time= 0.0193\n",
      "Epoch: 0135 train_loss= 0.4963 train_acc= 0.9571 val_loss= 0.8421 val_acc= 0.8000 time= 0.0201\n",
      "Epoch: 0136 train_loss= 0.4934 train_acc= 0.9571 val_loss= 0.8398 val_acc= 0.8033 time= 0.0198\n",
      "Epoch: 0137 train_loss= 0.4903 train_acc= 0.9571 val_loss= 0.8367 val_acc= 0.8033 time= 0.0309\n",
      "Epoch: 0138 train_loss= 0.4874 train_acc= 0.9571 val_loss= 0.8330 val_acc= 0.8000 time= 0.0194\n",
      "Epoch: 0139 train_loss= 0.4842 train_acc= 0.9571 val_loss= 0.8307 val_acc= 0.8000 time= 0.0192\n",
      "Epoch: 0140 train_loss= 0.4807 train_acc= 0.9643 val_loss= 0.8296 val_acc= 0.7967 time= 0.0164\n",
      "Epoch: 0141 train_loss= 0.4777 train_acc= 0.9571 val_loss= 0.8292 val_acc= 0.7933 time= 0.0184\n",
      "Epoch: 0142 train_loss= 0.4754 train_acc= 0.9571 val_loss= 0.8295 val_acc= 0.7967 time= 0.0201\n",
      "Epoch: 0143 train_loss= 0.4726 train_acc= 0.9571 val_loss= 0.8282 val_acc= 0.7933 time= 0.0289\n",
      "Epoch: 0144 train_loss= 0.4696 train_acc= 0.9571 val_loss= 0.8256 val_acc= 0.7967 time= 0.0188\n",
      "Epoch: 0145 train_loss= 0.4664 train_acc= 0.9643 val_loss= 0.8221 val_acc= 0.8000 time= 0.0223\n",
      "Epoch: 0146 train_loss= 0.4642 train_acc= 0.9643 val_loss= 0.8196 val_acc= 0.8100 time= 0.0171\n",
      "Epoch: 0147 train_loss= 0.4616 train_acc= 0.9643 val_loss= 0.8163 val_acc= 0.8200 time= 0.0189\n",
      "Epoch: 0148 train_loss= 0.4587 train_acc= 0.9714 val_loss= 0.8135 val_acc= 0.8200 time= 0.0190\n",
      "Epoch: 0149 train_loss= 0.4563 train_acc= 0.9714 val_loss= 0.8113 val_acc= 0.8267 time= 0.0190\n",
      "Epoch: 0150 train_loss= 0.4527 train_acc= 0.9714 val_loss= 0.8092 val_acc= 0.8100 time= 0.0241\n",
      "Epoch: 0151 train_loss= 0.4488 train_acc= 0.9714 val_loss= 0.8074 val_acc= 0.8033 time= 0.0229\n",
      "Epoch: 0152 train_loss= 0.4452 train_acc= 0.9643 val_loss= 0.8060 val_acc= 0.8133 time= 0.0195\n",
      "Epoch: 0153 train_loss= 0.4419 train_acc= 0.9643 val_loss= 0.8050 val_acc= 0.8100 time= 0.0186\n",
      "Epoch: 0154 train_loss= 0.4393 train_acc= 0.9643 val_loss= 0.8035 val_acc= 0.8067 time= 0.0196\n",
      "Epoch: 0155 train_loss= 0.4371 train_acc= 0.9643 val_loss= 0.8031 val_acc= 0.7967 time= 0.0304\n",
      "Epoch: 0156 train_loss= 0.4353 train_acc= 0.9643 val_loss= 0.8019 val_acc= 0.7967 time= 0.0189\n",
      "Epoch: 0157 train_loss= 0.4325 train_acc= 0.9643 val_loss= 0.7987 val_acc= 0.7967 time= 0.0197\n",
      "Epoch: 0158 train_loss= 0.4292 train_acc= 0.9643 val_loss= 0.7942 val_acc= 0.8033 time= 0.0183\n",
      "Epoch: 0159 train_loss= 0.4266 train_acc= 0.9643 val_loss= 0.7903 val_acc= 0.8100 time= 0.0196\n",
      "Epoch: 0160 train_loss= 0.4256 train_acc= 0.9643 val_loss= 0.7883 val_acc= 0.8167 time= 0.0187\n",
      "Epoch: 0161 train_loss= 0.4258 train_acc= 0.9714 val_loss= 0.7874 val_acc= 0.8200 time= 0.0276\n",
      "Epoch: 0162 train_loss= 0.4255 train_acc= 0.9714 val_loss= 0.7872 val_acc= 0.8300 time= 0.0199\n",
      "Epoch: 0163 train_loss= 0.4229 train_acc= 0.9714 val_loss= 0.7858 val_acc= 0.8333 time= 0.0193\n",
      "Epoch: 0164 train_loss= 0.4169 train_acc= 0.9714 val_loss= 0.7848 val_acc= 0.8200 time= 0.0188\n",
      "Epoch: 0165 train_loss= 0.4140 train_acc= 0.9714 val_loss= 0.7875 val_acc= 0.8133 time= 0.0231\n",
      "Epoch: 0166 train_loss= 0.4135 train_acc= 0.9714 val_loss= 0.7923 val_acc= 0.7967 time= 0.0206\n",
      "Epoch: 0167 train_loss= 0.4126 train_acc= 0.9714 val_loss= 0.7944 val_acc= 0.7900 time= 0.0174\n",
      "Epoch: 0168 train_loss= 0.4098 train_acc= 0.9714 val_loss= 0.7922 val_acc= 0.7933 time= 0.0232\n",
      "Epoch: 0169 train_loss= 0.4056 train_acc= 0.9714 val_loss= 0.7860 val_acc= 0.8033 time= 0.0186\n",
      "Epoch: 0170 train_loss= 0.4030 train_acc= 0.9714 val_loss= 0.7800 val_acc= 0.8133 time= 0.0196\n",
      "Epoch: 0171 train_loss= 0.4026 train_acc= 0.9714 val_loss= 0.7773 val_acc= 0.8267 time= 0.0209\n",
      "Epoch: 0172 train_loss= 0.4021 train_acc= 0.9714 val_loss= 0.7755 val_acc= 0.8300 time= 0.0261\n",
      "Epoch: 0173 train_loss= 0.3987 train_acc= 0.9714 val_loss= 0.7736 val_acc= 0.8367 time= 0.0194\n",
      "Epoch: 0174 train_loss= 0.3953 train_acc= 0.9714 val_loss= 0.7719 val_acc= 0.8367 time= 0.0204\n",
      "Epoch: 0175 train_loss= 0.3923 train_acc= 0.9714 val_loss= 0.7708 val_acc= 0.8333 time= 0.0266\n",
      "Epoch: 0176 train_loss= 0.3910 train_acc= 0.9714 val_loss= 0.7729 val_acc= 0.8200 time= 0.0199\n",
      "Epoch: 0177 train_loss= 0.3924 train_acc= 0.9714 val_loss= 0.7783 val_acc= 0.8067 time= 0.0175\n",
      "Epoch: 0178 train_loss= 0.3929 train_acc= 0.9857 val_loss= 0.7810 val_acc= 0.7933 time= 0.0178\n",
      "Epoch: 0179 train_loss= 0.3901 train_acc= 0.9786 val_loss= 0.7774 val_acc= 0.7967 time= 0.0181\n",
      "Epoch: 0180 train_loss= 0.3857 train_acc= 0.9857 val_loss= 0.7699 val_acc= 0.8000 time= 0.0178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0181 train_loss= 0.3830 train_acc= 0.9786 val_loss= 0.7631 val_acc= 0.8333 time= 0.0170\n",
      "Epoch: 0182 train_loss= 0.3842 train_acc= 0.9786 val_loss= 0.7604 val_acc= 0.8400 time= 0.0175\n",
      "Epoch: 0183 train_loss= 0.3871 train_acc= 0.9714 val_loss= 0.7611 val_acc= 0.8367 time= 0.0205\n",
      "Epoch: 0184 train_loss= 0.3864 train_acc= 0.9643 val_loss= 0.7607 val_acc= 0.8367 time= 0.0177\n",
      "Epoch: 0185 train_loss= 0.3818 train_acc= 0.9786 val_loss= 0.7581 val_acc= 0.8433 time= 0.0178\n",
      "Epoch: 0186 train_loss= 0.3762 train_acc= 0.9786 val_loss= 0.7563 val_acc= 0.8400 time= 0.0231\n",
      "Epoch: 0187 train_loss= 0.3723 train_acc= 0.9857 val_loss= 0.7560 val_acc= 0.8433 time= 0.0189\n",
      "Epoch: 0188 train_loss= 0.3706 train_acc= 0.9857 val_loss= 0.7572 val_acc= 0.8333 time= 0.0250\n",
      "Epoch: 0189 train_loss= 0.3690 train_acc= 0.9857 val_loss= 0.7581 val_acc= 0.8133 time= 0.0303\n",
      "Epoch: 0190 train_loss= 0.3671 train_acc= 0.9857 val_loss= 0.7583 val_acc= 0.8233 time= 0.0192\n",
      "Epoch: 0191 train_loss= 0.3654 train_acc= 0.9786 val_loss= 0.7563 val_acc= 0.8167 time= 0.0235\n",
      "Epoch: 0192 train_loss= 0.3635 train_acc= 0.9786 val_loss= 0.7544 val_acc= 0.8233 time= 0.0245\n",
      "Epoch: 0193 train_loss= 0.3622 train_acc= 0.9786 val_loss= 0.7522 val_acc= 0.8333 time= 0.0175\n",
      "Epoch: 0194 train_loss= 0.3618 train_acc= 0.9786 val_loss= 0.7506 val_acc= 0.8333 time= 0.0190\n",
      "Epoch: 0195 train_loss= 0.3610 train_acc= 0.9786 val_loss= 0.7508 val_acc= 0.8300 time= 0.0209\n",
      "Epoch: 0196 train_loss= 0.3597 train_acc= 0.9786 val_loss= 0.7518 val_acc= 0.8300 time= 0.0297\n",
      "Epoch: 0197 train_loss= 0.3587 train_acc= 0.9857 val_loss= 0.7548 val_acc= 0.8300 time= 0.0186\n",
      "Epoch: 0198 train_loss= 0.3582 train_acc= 0.9857 val_loss= 0.7577 val_acc= 0.8233 time= 0.0195\n",
      "Epoch: 0199 train_loss= 0.3567 train_acc= 0.9786 val_loss= 0.7574 val_acc= 0.8233 time= 0.0179\n",
      "Epoch: 0200 train_loss= 0.3542 train_acc= 0.9857 val_loss= 0.7535 val_acc= 0.8267 time= 0.0192\n"
     ]
    }
   ],
   "source": [
    "NB_EPOCH = 200\n",
    "PATIENCE = 10  # early stopping patience\n",
    "# Helper variables for main training loop\n",
    "wait = 0\n",
    "preds = None\n",
    "best_val_loss = 99999\n",
    "\n",
    "# Fit\n",
    "for epoch in range(1, NB_EPOCH+1):\n",
    "\n",
    "    # Log wall-clock time\n",
    "    t = time.time()\n",
    "\n",
    "    # Single training iteration (we mask nodes without labels for loss calculation)\n",
    "    model.fit(graph, y_train, sample_weight=train_mask,\n",
    "              batch_size=A.shape[0], epochs=1, shuffle=False, verbose=0)\n",
    "\n",
    "    # Predict on full dataset\n",
    "    preds = model.predict(graph, batch_size=A.shape[0])\n",
    "\n",
    "    # Train / validation scores\n",
    "    train_val_loss, train_val_acc = evaluate_preds(preds, [y_train, y_val],\n",
    "                                                   [idx_train, idx_val])\n",
    "    print(\"Epoch: {:04d}\".format(epoch),\n",
    "          \"train_loss= {:.4f}\".format(train_val_loss[0]),\n",
    "          \"train_acc= {:.4f}\".format(train_val_acc[0]),\n",
    "          \"val_loss= {:.4f}\".format(train_val_loss[1]),\n",
    "          \"val_acc= {:.4f}\".format(train_val_acc[1]),\n",
    "          \"time= {:.4f}\".format(time.time() - t))\n",
    "\n",
    "    # Early stopping\n",
    "    if train_val_loss[1] < best_val_loss:\n",
    "        best_val_loss = train_val_loss[1]\n",
    "        wait = 0\n",
    "    else:\n",
    "        if wait >= PATIENCE:\n",
    "            print('Epoch {}: early stopping'.format(epoch))\n",
    "            break\n",
    "        wait += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.8010\n",
      "accuracy = 0.8140\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate_preds(preds, [y_test], [idx_test])\n",
    "print(f\"\"\"\n",
    "loss = {test_loss[0]:.4f}\n",
    "accuracy = {test_acc[0]:.4f}\n",
    "\"\"\".strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Graph Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 17:25:00.305068 140065343878976 deprecation.py:323] From /home/bitcommander/Desktop/robust-graph-convolutional-networks-against-adversarial-attacks-implementation/ggcn/ggcl.py:28: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "W1110 17:25:00.306314 140065343878976 deprecation.py:323] From /home/bitcommander/.local/lib/python3.7/site-packages/tensorflow/python/ops/distributions/normal.py:160: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    }
   ],
   "source": [
    "H = Dropout(0.5)(X_in)\n",
    "H1 = GaussianGraphConvolution(16,\n",
    "    is_first=True,\n",
    "    activation='relu',\n",
    ")([H]+G)\n",
    "Y = GaussianGraphConvolution(y.shape[1],\n",
    "    is_last=True,\n",
    "    activation='softmax',\n",
    ")(H1+G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[X_in]+G, outputs=Y)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01))\n",
    "# model.add_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ggcn.losses import kl_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1110 17:25:00.350755 140065343878976 deprecation.py:323] From /home/bitcommander/Desktop/robust-graph-convolutional-networks-against-adversarial-attacks-implementation/ggcn/losses.py:27: kl_divergence (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    }
   ],
   "source": [
    "model.add_loss(kl_reg(*H1), H1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 1433)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1433)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_graph_convolution_1 (G [(None, 16), (None,  45856       dropout_3[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_graph_convolution_2 (G (None, 7)            224         gaussian_graph_convolution_1[0][0\n",
      "                                                                 gaussian_graph_convolution_1[0][1\n",
      "                                                                 input_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 46,080\n",
      "Trainable params: 46,080\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.9379 train_acc= 0.3214 val_loss= 1.9404 val_acc= 0.2500 time= 0.4586\n",
      "Epoch: 0002 train_loss= 1.9273 train_acc= 0.4214 val_loss= 1.9341 val_acc= 0.2833 time= 0.0250\n",
      "Epoch: 0003 train_loss= 1.9165 train_acc= 0.4786 val_loss= 1.9250 val_acc= 0.3600 time= 0.0237\n",
      "Epoch: 0004 train_loss= 1.9042 train_acc= 0.5071 val_loss= 1.9170 val_acc= 0.3733 time= 0.0195\n",
      "Epoch: 0005 train_loss= 1.8910 train_acc= 0.5214 val_loss= 1.9063 val_acc= 0.4233 time= 0.0178\n",
      "Epoch: 0006 train_loss= 1.8762 train_acc= 0.5071 val_loss= 1.8980 val_acc= 0.4267 time= 0.0188\n",
      "Epoch: 0007 train_loss= 1.8634 train_acc= 0.4857 val_loss= 1.8862 val_acc= 0.4400 time= 0.0182\n",
      "Epoch: 0008 train_loss= 1.8498 train_acc= 0.5571 val_loss= 1.8758 val_acc= 0.4367 time= 0.0192\n",
      "Epoch: 0009 train_loss= 1.8308 train_acc= 0.5500 val_loss= 1.8655 val_acc= 0.4300 time= 0.0194\n",
      "Epoch: 0010 train_loss= 1.8167 train_acc= 0.5429 val_loss= 1.8519 val_acc= 0.4267 time= 0.0183\n",
      "Epoch: 0011 train_loss= 1.8018 train_acc= 0.5500 val_loss= 1.8377 val_acc= 0.4500 time= 0.0224\n",
      "Epoch: 0012 train_loss= 1.7770 train_acc= 0.5643 val_loss= 1.8246 val_acc= 0.4600 time= 0.0206\n",
      "Epoch: 0013 train_loss= 1.7626 train_acc= 0.5500 val_loss= 1.8101 val_acc= 0.4700 time= 0.0189\n",
      "Epoch: 0014 train_loss= 1.7416 train_acc= 0.5429 val_loss= 1.7957 val_acc= 0.4833 time= 0.0184\n",
      "Epoch: 0015 train_loss= 1.7170 train_acc= 0.5643 val_loss= 1.7808 val_acc= 0.4833 time= 0.0181\n",
      "Epoch: 0016 train_loss= 1.6965 train_acc= 0.6000 val_loss= 1.7646 val_acc= 0.4833 time= 0.0174\n",
      "Epoch: 0017 train_loss= 1.6783 train_acc= 0.5643 val_loss= 1.7485 val_acc= 0.4767 time= 0.0166\n",
      "Epoch: 0018 train_loss= 1.6546 train_acc= 0.5500 val_loss= 1.7315 val_acc= 0.4733 time= 0.0165\n",
      "Epoch: 0019 train_loss= 1.6328 train_acc= 0.5857 val_loss= 1.7170 val_acc= 0.4767 time= 0.0173\n",
      "Epoch: 0020 train_loss= 1.6085 train_acc= 0.6000 val_loss= 1.7003 val_acc= 0.4867 time= 0.0226\n",
      "Epoch: 0021 train_loss= 1.5864 train_acc= 0.5786 val_loss= 1.6805 val_acc= 0.4767 time= 0.0189\n",
      "Epoch: 0022 train_loss= 1.5648 train_acc= 0.6143 val_loss= 1.6640 val_acc= 0.4833 time= 0.0182\n",
      "Epoch: 0023 train_loss= 1.5375 train_acc= 0.6214 val_loss= 1.6454 val_acc= 0.4933 time= 0.0199\n",
      "Epoch: 0024 train_loss= 1.5143 train_acc= 0.6286 val_loss= 1.6271 val_acc= 0.4967 time= 0.0240\n",
      "Epoch: 0025 train_loss= 1.4895 train_acc= 0.6143 val_loss= 1.6079 val_acc= 0.5100 time= 0.0227\n",
      "Epoch: 0026 train_loss= 1.4620 train_acc= 0.6357 val_loss= 1.5885 val_acc= 0.5100 time= 0.0235\n",
      "Epoch: 0027 train_loss= 1.4377 train_acc= 0.6429 val_loss= 1.5684 val_acc= 0.5200 time= 0.0182\n",
      "Epoch: 0028 train_loss= 1.4127 train_acc= 0.6571 val_loss= 1.5512 val_acc= 0.5200 time= 0.0194\n",
      "Epoch: 0029 train_loss= 1.3873 train_acc= 0.6714 val_loss= 1.5330 val_acc= 0.5367 time= 0.0226\n",
      "Epoch: 0030 train_loss= 1.3621 train_acc= 0.6857 val_loss= 1.5131 val_acc= 0.5433 time= 0.0187\n",
      "Epoch: 0031 train_loss= 1.3325 train_acc= 0.7071 val_loss= 1.4912 val_acc= 0.5400 time= 0.0228\n",
      "Epoch: 0032 train_loss= 1.3086 train_acc= 0.7214 val_loss= 1.4701 val_acc= 0.5500 time= 0.0188\n",
      "Epoch: 0033 train_loss= 1.2784 train_acc= 0.7357 val_loss= 1.4514 val_acc= 0.5633 time= 0.0247\n",
      "Epoch: 0034 train_loss= 1.2494 train_acc= 0.7500 val_loss= 1.4315 val_acc= 0.5900 time= 0.0190\n",
      "Epoch: 0035 train_loss= 1.2276 train_acc= 0.7500 val_loss= 1.4106 val_acc= 0.6033 time= 0.0186\n",
      "Epoch: 0036 train_loss= 1.1995 train_acc= 0.7500 val_loss= 1.3898 val_acc= 0.6200 time= 0.0189\n",
      "Epoch: 0037 train_loss= 1.1771 train_acc= 0.7714 val_loss= 1.3692 val_acc= 0.6233 time= 0.0263\n",
      "Epoch: 0038 train_loss= 1.1481 train_acc= 0.7714 val_loss= 1.3502 val_acc= 0.6367 time= 0.0246\n",
      "Epoch: 0039 train_loss= 1.1214 train_acc= 0.8071 val_loss= 1.3290 val_acc= 0.6533 time= 0.0199\n",
      "Epoch: 0040 train_loss= 1.0981 train_acc= 0.8071 val_loss= 1.3097 val_acc= 0.6667 time= 0.0201\n",
      "Epoch: 0041 train_loss= 1.0721 train_acc= 0.8143 val_loss= 1.2875 val_acc= 0.6700 time= 0.0196\n",
      "Epoch: 0042 train_loss= 1.0465 train_acc= 0.8214 val_loss= 1.2667 val_acc= 0.6900 time= 0.0190\n",
      "Epoch: 0043 train_loss= 1.0188 train_acc= 0.8429 val_loss= 1.2483 val_acc= 0.7033 time= 0.0194\n",
      "Epoch: 0044 train_loss= 0.9971 train_acc= 0.8571 val_loss= 1.2285 val_acc= 0.7100 time= 0.0193\n",
      "Epoch: 0045 train_loss= 0.9704 train_acc= 0.8857 val_loss= 1.2069 val_acc= 0.7167 time= 0.0194\n",
      "Epoch: 0046 train_loss= 0.9456 train_acc= 0.9000 val_loss= 1.1888 val_acc= 0.7267 time= 0.0190\n",
      "Epoch: 0047 train_loss= 0.9229 train_acc= 0.9000 val_loss= 1.1700 val_acc= 0.7400 time= 0.0193\n",
      "Epoch: 0048 train_loss= 0.8993 train_acc= 0.9000 val_loss= 1.1505 val_acc= 0.7500 time= 0.0190\n",
      "Epoch: 0049 train_loss= 0.8748 train_acc= 0.9143 val_loss= 1.1297 val_acc= 0.7667 time= 0.0192\n",
      "Epoch: 0050 train_loss= 0.8513 train_acc= 0.9143 val_loss= 1.1140 val_acc= 0.7667 time= 0.0190\n",
      "Epoch: 0051 train_loss= 0.8289 train_acc= 0.9143 val_loss= 1.0958 val_acc= 0.7700 time= 0.0241\n",
      "Epoch: 0052 train_loss= 0.8061 train_acc= 0.9143 val_loss= 1.0778 val_acc= 0.7700 time= 0.0237\n",
      "Epoch: 0053 train_loss= 0.7863 train_acc= 0.9286 val_loss= 1.0599 val_acc= 0.7800 time= 0.0245\n",
      "Epoch: 0054 train_loss= 0.7619 train_acc= 0.9357 val_loss= 1.0432 val_acc= 0.7833 time= 0.0179\n",
      "Epoch: 0055 train_loss= 0.7415 train_acc= 0.9357 val_loss= 1.0256 val_acc= 0.7867 time= 0.0232\n",
      "Epoch: 0056 train_loss= 0.7222 train_acc= 0.9286 val_loss= 1.0118 val_acc= 0.7967 time= 0.0186\n",
      "Epoch: 0057 train_loss= 0.7003 train_acc= 0.9429 val_loss= 0.9930 val_acc= 0.8033 time= 0.0283\n",
      "Epoch: 0058 train_loss= 0.6823 train_acc= 0.9357 val_loss= 0.9775 val_acc= 0.8033 time= 0.0189\n",
      "Epoch: 0059 train_loss= 0.6599 train_acc= 0.9429 val_loss= 0.9621 val_acc= 0.8033 time= 0.0190\n",
      "Epoch: 0060 train_loss= 0.6417 train_acc= 0.9500 val_loss= 0.9459 val_acc= 0.8100 time= 0.0181\n",
      "Epoch: 0061 train_loss= 0.6244 train_acc= 0.9500 val_loss= 0.9329 val_acc= 0.8133 time= 0.0186\n",
      "Epoch: 0062 train_loss= 0.6058 train_acc= 0.9500 val_loss= 0.9189 val_acc= 0.8100 time= 0.0189\n",
      "Epoch: 0063 train_loss= 0.5865 train_acc= 0.9571 val_loss= 0.9037 val_acc= 0.8200 time= 0.0209\n",
      "Epoch: 0064 train_loss= 0.5705 train_acc= 0.9571 val_loss= 0.8882 val_acc= 0.8267 time= 0.0225\n",
      "Epoch: 0065 train_loss= 0.5514 train_acc= 0.9571 val_loss= 0.8766 val_acc= 0.8267 time= 0.0165\n",
      "Epoch: 0066 train_loss= 0.5357 train_acc= 0.9571 val_loss= 0.8633 val_acc= 0.8300 time= 0.0184\n",
      "Epoch: 0067 train_loss= 0.5175 train_acc= 0.9643 val_loss= 0.8488 val_acc= 0.8333 time= 0.0193\n",
      "Epoch: 0068 train_loss= 0.5024 train_acc= 0.9643 val_loss= 0.8389 val_acc= 0.8367 time= 0.0240\n",
      "Epoch: 0069 train_loss= 0.4867 train_acc= 0.9714 val_loss= 0.8253 val_acc= 0.8333 time= 0.0241\n",
      "Epoch: 0070 train_loss= 0.4731 train_acc= 0.9643 val_loss= 0.8142 val_acc= 0.8433 time= 0.0180\n",
      "Epoch: 0071 train_loss= 0.4593 train_acc= 0.9714 val_loss= 0.8035 val_acc= 0.8300 time= 0.0168\n",
      "Epoch: 0072 train_loss= 0.4431 train_acc= 0.9714 val_loss= 0.7940 val_acc= 0.8367 time= 0.0234\n",
      "Epoch: 0073 train_loss= 0.4313 train_acc= 0.9714 val_loss= 0.7817 val_acc= 0.8300 time= 0.0214\n",
      "Epoch: 0074 train_loss= 0.4175 train_acc= 0.9714 val_loss= 0.7740 val_acc= 0.8333 time= 0.0199\n",
      "Epoch: 0075 train_loss= 0.4031 train_acc= 0.9714 val_loss= 0.7637 val_acc= 0.8333 time= 0.0232\n",
      "Epoch: 0076 train_loss= 0.3912 train_acc= 0.9714 val_loss= 0.7547 val_acc= 0.8367 time= 0.0188\n",
      "Epoch: 0077 train_loss= 0.3789 train_acc= 0.9714 val_loss= 0.7461 val_acc= 0.8333 time= 0.0188\n",
      "Epoch: 0078 train_loss= 0.3678 train_acc= 0.9714 val_loss= 0.7376 val_acc= 0.8333 time= 0.0196\n",
      "Epoch: 0079 train_loss= 0.3567 train_acc= 0.9714 val_loss= 0.7285 val_acc= 0.8333 time= 0.0200\n",
      "Epoch: 0080 train_loss= 0.3453 train_acc= 0.9714 val_loss= 0.7222 val_acc= 0.8333 time= 0.0186\n",
      "Epoch: 0081 train_loss= 0.3359 train_acc= 0.9714 val_loss= 0.7143 val_acc= 0.8367 time= 0.0194\n",
      "Epoch: 0082 train_loss= 0.3241 train_acc= 0.9786 val_loss= 0.7066 val_acc= 0.8333 time= 0.0249\n",
      "Epoch: 0083 train_loss= 0.3156 train_acc= 0.9786 val_loss= 0.6984 val_acc= 0.8367 time= 0.0247\n",
      "Epoch: 0084 train_loss= 0.3047 train_acc= 0.9786 val_loss= 0.6933 val_acc= 0.8367 time= 0.0232\n",
      "Epoch: 0085 train_loss= 0.2958 train_acc= 0.9786 val_loss= 0.6853 val_acc= 0.8367 time= 0.0189\n",
      "Epoch: 0086 train_loss= 0.2875 train_acc= 0.9786 val_loss= 0.6803 val_acc= 0.8367 time= 0.0186\n",
      "Epoch: 0087 train_loss= 0.2783 train_acc= 0.9786 val_loss= 0.6731 val_acc= 0.8367 time= 0.0194\n",
      "Epoch: 0088 train_loss= 0.2698 train_acc= 0.9786 val_loss= 0.6683 val_acc= 0.8333 time= 0.0171\n",
      "Epoch: 0089 train_loss= 0.2632 train_acc= 0.9786 val_loss= 0.6631 val_acc= 0.8400 time= 0.0200\n",
      "Epoch: 0090 train_loss= 0.2557 train_acc= 0.9786 val_loss= 0.6581 val_acc= 0.8367 time= 0.0284\n",
      "Epoch: 0091 train_loss= 0.2483 train_acc= 0.9786 val_loss= 0.6528 val_acc= 0.8400 time= 0.0284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0092 train_loss= 0.2411 train_acc= 0.9786 val_loss= 0.6485 val_acc= 0.8367 time= 0.0193\n",
      "Epoch: 0093 train_loss= 0.2347 train_acc= 0.9786 val_loss= 0.6447 val_acc= 0.8333 time= 0.0245\n",
      "Epoch: 0094 train_loss= 0.2278 train_acc= 0.9786 val_loss= 0.6408 val_acc= 0.8400 time= 0.0185\n",
      "Epoch: 0095 train_loss= 0.2220 train_acc= 0.9786 val_loss= 0.6357 val_acc= 0.8400 time= 0.0240\n",
      "Epoch: 0096 train_loss= 0.2133 train_acc= 0.9786 val_loss= 0.6333 val_acc= 0.8400 time= 0.0281\n",
      "Epoch: 0097 train_loss= 0.2086 train_acc= 0.9786 val_loss= 0.6293 val_acc= 0.8400 time= 0.0282\n",
      "Epoch: 0098 train_loss= 0.2028 train_acc= 0.9786 val_loss= 0.6263 val_acc= 0.8367 time= 0.0287\n",
      "Epoch: 0099 train_loss= 0.1965 train_acc= 0.9786 val_loss= 0.6223 val_acc= 0.8367 time= 0.0283\n",
      "Epoch: 0100 train_loss= 0.1910 train_acc= 0.9786 val_loss= 0.6200 val_acc= 0.8367 time= 0.0285\n",
      "Epoch: 0101 train_loss= 0.1866 train_acc= 0.9786 val_loss= 0.6169 val_acc= 0.8367 time= 0.0286\n",
      "Epoch: 0102 train_loss= 0.1817 train_acc= 0.9786 val_loss= 0.6149 val_acc= 0.8367 time= 0.0184\n",
      "Epoch: 0103 train_loss= 0.1764 train_acc= 0.9857 val_loss= 0.6110 val_acc= 0.8367 time= 0.0191\n",
      "Epoch: 0104 train_loss= 0.1722 train_acc= 0.9857 val_loss= 0.6086 val_acc= 0.8300 time= 0.0191\n",
      "Epoch: 0105 train_loss= 0.1675 train_acc= 0.9857 val_loss= 0.6078 val_acc= 0.8300 time= 0.0195\n",
      "Epoch: 0106 train_loss= 0.1635 train_acc= 0.9857 val_loss= 0.6040 val_acc= 0.8333 time= 0.0183\n",
      "Epoch: 0107 train_loss= 0.1586 train_acc= 0.9857 val_loss= 0.6040 val_acc= 0.8300 time= 0.0190\n",
      "Epoch: 0108 train_loss= 0.1552 train_acc= 0.9857 val_loss= 0.6002 val_acc= 0.8300 time= 0.0233\n",
      "Epoch: 0109 train_loss= 0.1521 train_acc= 0.9857 val_loss= 0.5990 val_acc= 0.8333 time= 0.0194\n",
      "Epoch: 0110 train_loss= 0.1481 train_acc= 0.9857 val_loss= 0.5979 val_acc= 0.8333 time= 0.0282\n",
      "Epoch: 0111 train_loss= 0.1444 train_acc= 0.9857 val_loss= 0.5962 val_acc= 0.8333 time= 0.0189\n",
      "Epoch: 0112 train_loss= 0.1404 train_acc= 0.9857 val_loss= 0.5929 val_acc= 0.8333 time= 0.0196\n",
      "Epoch: 0113 train_loss= 0.1366 train_acc= 0.9857 val_loss= 0.5932 val_acc= 0.8333 time= 0.0233\n",
      "Epoch: 0114 train_loss= 0.1335 train_acc= 0.9857 val_loss= 0.5911 val_acc= 0.8333 time= 0.0295\n",
      "Epoch: 0115 train_loss= 0.1309 train_acc= 0.9857 val_loss= 0.5892 val_acc= 0.8333 time= 0.0240\n",
      "Epoch: 0116 train_loss= 0.1280 train_acc= 0.9857 val_loss= 0.5892 val_acc= 0.8333 time= 0.0189\n",
      "Epoch: 0117 train_loss= 0.1245 train_acc= 0.9857 val_loss= 0.5884 val_acc= 0.8333 time= 0.0189\n",
      "Epoch: 0118 train_loss= 0.1212 train_acc= 0.9857 val_loss= 0.5862 val_acc= 0.8333 time= 0.0199\n",
      "Epoch: 0119 train_loss= 0.1193 train_acc= 0.9857 val_loss= 0.5859 val_acc= 0.8333 time= 0.0233\n",
      "Epoch: 0120 train_loss= 0.1160 train_acc= 0.9857 val_loss= 0.5852 val_acc= 0.8333 time= 0.0207\n",
      "Epoch: 0121 train_loss= 0.1138 train_acc= 0.9857 val_loss= 0.5840 val_acc= 0.8300 time= 0.0234\n",
      "Epoch: 0122 train_loss= 0.1108 train_acc= 0.9857 val_loss= 0.5817 val_acc= 0.8300 time= 0.0201\n",
      "Epoch: 0123 train_loss= 0.1091 train_acc= 0.9857 val_loss= 0.5822 val_acc= 0.8300 time= 0.0292\n",
      "Epoch: 0124 train_loss= 0.1067 train_acc= 0.9857 val_loss= 0.5805 val_acc= 0.8300 time= 0.0190\n",
      "Epoch: 0125 train_loss= 0.1040 train_acc= 0.9929 val_loss= 0.5794 val_acc= 0.8333 time= 0.0187\n",
      "Epoch: 0126 train_loss= 0.1020 train_acc= 0.9857 val_loss= 0.5801 val_acc= 0.8333 time= 0.0192\n",
      "Epoch: 0127 train_loss= 0.0996 train_acc= 0.9929 val_loss= 0.5789 val_acc= 0.8300 time= 0.0275\n",
      "Epoch: 0128 train_loss= 0.0982 train_acc= 0.9857 val_loss= 0.5783 val_acc= 0.8300 time= 0.0177\n",
      "Epoch: 0129 train_loss= 0.0959 train_acc= 0.9929 val_loss= 0.5767 val_acc= 0.8300 time= 0.0171\n",
      "Epoch: 0130 train_loss= 0.0938 train_acc= 0.9929 val_loss= 0.5768 val_acc= 0.8300 time= 0.0190\n",
      "Epoch: 0131 train_loss= 0.0920 train_acc= 0.9929 val_loss= 0.5775 val_acc= 0.8300 time= 0.0193\n",
      "Epoch: 0132 train_loss= 0.0908 train_acc= 0.9929 val_loss= 0.5760 val_acc= 0.8300 time= 0.0170\n",
      "Epoch: 0133 train_loss= 0.0884 train_acc= 0.9929 val_loss= 0.5769 val_acc= 0.8300 time= 0.0184\n",
      "Epoch: 0134 train_loss= 0.0864 train_acc= 0.9929 val_loss= 0.5755 val_acc= 0.8300 time= 0.0235\n",
      "Epoch: 0135 train_loss= 0.0852 train_acc= 0.9929 val_loss= 0.5760 val_acc= 0.8300 time= 0.0186\n",
      "Epoch: 0136 train_loss= 0.0838 train_acc= 0.9929 val_loss= 0.5747 val_acc= 0.8300 time= 0.0170\n",
      "Epoch: 0137 train_loss= 0.0811 train_acc= 0.9929 val_loss= 0.5754 val_acc= 0.8300 time= 0.0189\n",
      "Epoch: 0138 train_loss= 0.0801 train_acc= 0.9929 val_loss= 0.5742 val_acc= 0.8333 time= 0.0194\n",
      "Epoch: 0139 train_loss= 0.0788 train_acc= 0.9929 val_loss= 0.5749 val_acc= 0.8333 time= 0.0243\n",
      "Epoch: 0140 train_loss= 0.0773 train_acc= 0.9929 val_loss= 0.5743 val_acc= 0.8333 time= 0.0202\n",
      "Epoch: 0141 train_loss= 0.0751 train_acc= 0.9929 val_loss= 0.5736 val_acc= 0.8333 time= 0.0296\n",
      "Epoch: 0142 train_loss= 0.0747 train_acc= 0.9929 val_loss= 0.5737 val_acc= 0.8333 time= 0.0247\n",
      "Epoch: 0143 train_loss= 0.0725 train_acc= 0.9929 val_loss= 0.5731 val_acc= 0.8367 time= 0.0298\n",
      "Epoch: 0144 train_loss= 0.0712 train_acc= 0.9929 val_loss= 0.5731 val_acc= 0.8333 time= 0.0239\n",
      "Epoch: 0145 train_loss= 0.0704 train_acc= 0.9929 val_loss= 0.5730 val_acc= 0.8367 time= 0.0206\n",
      "Epoch: 0146 train_loss= 0.0686 train_acc= 0.9929 val_loss= 0.5726 val_acc= 0.8367 time= 0.0279\n",
      "Epoch: 0147 train_loss= 0.0679 train_acc= 0.9929 val_loss= 0.5721 val_acc= 0.8367 time= 0.0249\n",
      "Epoch: 0148 train_loss= 0.0665 train_acc= 0.9929 val_loss= 0.5722 val_acc= 0.8367 time= 0.0231\n",
      "Epoch: 0149 train_loss= 0.0652 train_acc= 0.9929 val_loss= 0.5715 val_acc= 0.8333 time= 0.0176\n",
      "Epoch: 0150 train_loss= 0.0642 train_acc= 1.0000 val_loss= 0.5716 val_acc= 0.8333 time= 0.0196\n",
      "Epoch: 0151 train_loss= 0.0629 train_acc= 1.0000 val_loss= 0.5722 val_acc= 0.8333 time= 0.0236\n",
      "Epoch: 0152 train_loss= 0.0618 train_acc= 1.0000 val_loss= 0.5724 val_acc= 0.8333 time= 0.0180\n",
      "Epoch: 0153 train_loss= 0.0612 train_acc= 1.0000 val_loss= 0.5727 val_acc= 0.8333 time= 0.0248\n",
      "Epoch: 0154 train_loss= 0.0603 train_acc= 1.0000 val_loss= 0.5714 val_acc= 0.8333 time= 0.0231\n",
      "Epoch: 0155 train_loss= 0.0589 train_acc= 1.0000 val_loss= 0.5713 val_acc= 0.8333 time= 0.0184\n",
      "Epoch: 0156 train_loss= 0.0576 train_acc= 1.0000 val_loss= 0.5720 val_acc= 0.8333 time= 0.0183\n",
      "Epoch: 0157 train_loss= 0.0565 train_acc= 1.0000 val_loss= 0.5713 val_acc= 0.8333 time= 0.0234\n",
      "Epoch: 0158 train_loss= 0.0559 train_acc= 1.0000 val_loss= 0.5721 val_acc= 0.8333 time= 0.0203\n",
      "Epoch: 0159 train_loss= 0.0552 train_acc= 1.0000 val_loss= 0.5719 val_acc= 0.8333 time= 0.0242\n",
      "Epoch: 0160 train_loss= 0.0547 train_acc= 1.0000 val_loss= 0.5730 val_acc= 0.8333 time= 0.0193\n",
      "Epoch: 0161 train_loss= 0.0532 train_acc= 1.0000 val_loss= 0.5733 val_acc= 0.8333 time= 0.0196\n",
      "Epoch: 0162 train_loss= 0.0524 train_acc= 1.0000 val_loss= 0.5735 val_acc= 0.8333 time= 0.0191\n",
      "Epoch: 0163 train_loss= 0.0513 train_acc= 1.0000 val_loss= 0.5746 val_acc= 0.8333 time= 0.0200\n",
      "Epoch: 0164 train_loss= 0.0507 train_acc= 1.0000 val_loss= 0.5737 val_acc= 0.8333 time= 0.0232\n",
      "Epoch: 0165 train_loss= 0.0503 train_acc= 1.0000 val_loss= 0.5756 val_acc= 0.8333 time= 0.0200\n",
      "Epoch: 0166 train_loss= 0.0493 train_acc= 1.0000 val_loss= 0.5754 val_acc= 0.8300 time= 0.0234\n",
      "Epoch: 0167 train_loss= 0.0481 train_acc= 1.0000 val_loss= 0.5770 val_acc= 0.8333 time= 0.0246\n",
      "Epoch: 0168 train_loss= 0.0479 train_acc= 1.0000 val_loss= 0.5772 val_acc= 0.8333 time= 0.0233\n",
      "Epoch 168: early stopping\n"
     ]
    }
   ],
   "source": [
    "NB_EPOCH = 200\n",
    "PATIENCE = 10  # early stopping patience\n",
    "# Helper variables for main training loop\n",
    "wait = 0\n",
    "preds = None\n",
    "best_val_loss = 99999\n",
    "\n",
    "# Fit\n",
    "for epoch in range(1, NB_EPOCH+1):\n",
    "\n",
    "    # Log wall-clock time\n",
    "    t = time.time()\n",
    "\n",
    "    # Single training iteration (we mask nodes without labels for loss calculation)\n",
    "    model.fit(graph, y_train, sample_weight=train_mask,\n",
    "              batch_size=A.shape[0], epochs=1, shuffle=False, verbose=0)\n",
    "\n",
    "    # Predict on full dataset\n",
    "    preds = model.predict(graph, batch_size=A.shape[0])\n",
    "\n",
    "    # Train / validation scores\n",
    "    train_val_loss, train_val_acc = evaluate_preds(preds, [y_train, y_val],\n",
    "                                                   [idx_train, idx_val])\n",
    "    print(\"Epoch: {:04d}\".format(epoch),\n",
    "          \"train_loss= {:.4f}\".format(train_val_loss[0]),\n",
    "          \"train_acc= {:.4f}\".format(train_val_acc[0]),\n",
    "          \"val_loss= {:.4f}\".format(train_val_loss[1]),\n",
    "          \"val_acc= {:.4f}\".format(train_val_acc[1]),\n",
    "          \"time= {:.4f}\".format(time.time() - t))\n",
    "\n",
    "    # Early stopping\n",
    "    if train_val_loss[1] < best_val_loss:\n",
    "        best_val_loss = train_val_loss[1]\n",
    "        wait = 0\n",
    "    else:\n",
    "        if wait >= PATIENCE:\n",
    "            print('Epoch {}: early stopping'.format(epoch))\n",
    "            break\n",
    "        wait += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.5910\n",
      "accuracy = 0.8100\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate_preds(preds, [y_test], [idx_test])\n",
    "print(f\"\"\"\n",
    "loss = {test_loss[0]:.4f}\n",
    "accuracy = {test_acc[0]:.4f}\n",
    "\"\"\".strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
